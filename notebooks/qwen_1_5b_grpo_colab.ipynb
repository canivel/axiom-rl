{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen 1.5B GRPO Training on Colab (Memory-Optimized)\n",
    "\n",
    "This notebook runs GRPO on **Qwen2.5-Coder-1.5B-Instruct** to fix the Trapping Rain Water problem.\n",
    "\n",
    "## Memory Optimization\n",
    "\n",
    "GRPO requires both a **policy model** and a **reference model**, which is tight on T4 (16GB).\n",
    "\n",
    "This notebook uses several optimizations:\n",
    "1. **Gradient checkpointing** - trades compute for memory\n",
    "2. **Sequential processing** - process one completion at a time\n",
    "3. **Aggressive cache clearing** - free memory between steps\n",
    "4. **Shorter sequences** - limit max tokens\n",
    "\n",
    "## Background\n",
    "\n",
    "From our experiments:\n",
    "- Qwen 1.5B solves 9/10 hard problems natively (90%)\n",
    "- Only failure: **Trapping Rain Water** (4/5 = 80%)\n",
    "- Goal: Use GRPO to fix this single failure\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check Runtime\n",
    "\n",
    "**IMPORTANT**: Use GPU runtime (T4 minimum, A100 preferred for comfort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Aggressively clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    if gpu_memory < 14:\n",
    "        print(\"\\nERROR: Need at least T4 (16GB) for this notebook.\")\n",
    "        print(\"Go to Runtime -> Change runtime type -> T4 GPU\")\n",
    "    elif gpu_memory < 20:\n",
    "        print(\"\\nT4 detected - using memory-optimized settings.\")\n",
    "        USE_LOW_MEMORY = True\n",
    "    else:\n",
    "        print(\"\\nA100/V100 detected - can use standard settings.\")\n",
    "        USE_LOW_MEMORY = False\n",
    "else:\n",
    "    raise RuntimeError(\"GPU required!\")\n",
    "\n",
    "print(f\"\\nLow memory mode: {USE_LOW_MEMORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install -q transformers accelerate\n",
    "print(\"\\nDependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration based on GPU memory\n",
    "if USE_LOW_MEMORY:\n",
    "    CONFIG = {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "        \"num_steps\": 10,\n",
    "        \"num_generations\": 2,      # Reduced from 4\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"beta\": 0.04,\n",
    "        \"max_new_tokens\": 384,     # Reduced from 512\n",
    "        \"difficulty\": 5,\n",
    "        \"num_test_cases\": 5,\n",
    "        \"seed\": 42,\n",
    "        \"gradient_checkpointing\": True,\n",
    "    }\n",
    "else:\n",
    "    CONFIG = {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "        \"num_steps\": 10,\n",
    "        \"num_generations\": 4,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"beta\": 0.04,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"difficulty\": 5,\n",
    "        \"num_test_cases\": 5,\n",
    "        \"seed\": 42,\n",
    "        \"gradient_checkpointing\": False,\n",
    "    }\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k:25} = {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Problem Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Any\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    input_args: List[Any]\n",
    "    expected_output: Any\n",
    "\n",
    "@dataclass \n",
    "class Problem:\n",
    "    problem_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    function_signature: str\n",
    "    function_name: str\n",
    "    test_cases: List[TestCase]\n",
    "    \n",
    "    def to_prompt(self) -> str:\n",
    "        examples = \"\\n\".join(\n",
    "            f\"  {self.function_name}({repr(tc.input_args[0])}) -> {repr(tc.expected_output)}\"\n",
    "            for tc in self.test_cases[:3]\n",
    "        )\n",
    "        return f\"\"\"## {self.title}\n",
    "\n",
    "{self.description}\n",
    "\n",
    "### Function Signature\n",
    "```python\n",
    "{self.function_signature}\n",
    "```\n",
    "\n",
    "### Examples\n",
    "```python\n",
    "{examples}\n",
    "```\n",
    "\n",
    "Write ONLY the function implementation.\"\"\"\n",
    "\n",
    "\n",
    "class TrappingRainWaterGenerator:\n",
    "    def __init__(self, seed=None):\n",
    "        self.rng = random.Random(seed)\n",
    "        self._counter = 0\n",
    "    \n",
    "    def _solve(self, height):\n",
    "        if not height: return 0\n",
    "        left, right = 0, len(height) - 1\n",
    "        left_max = right_max = water = 0\n",
    "        while left < right:\n",
    "            if height[left] < height[right]:\n",
    "                if height[left] >= left_max: left_max = height[left]\n",
    "                else: water += left_max - height[left]\n",
    "                left += 1\n",
    "            else:\n",
    "                if height[right] >= right_max: right_max = height[right]\n",
    "                else: water += right_max - height[right]\n",
    "                right -= 1\n",
    "        return water\n",
    "    \n",
    "    def generate(self, difficulty=5, num_test_cases=5):\n",
    "        self._counter += 1\n",
    "        test_cases = []\n",
    "        for _ in range(num_test_cases):\n",
    "            length = 5 + difficulty * 2\n",
    "            heights = [self.rng.randint(0, 3 + difficulty) for _ in range(length)]\n",
    "            test_cases.append(TestCase([heights], self._solve(heights)))\n",
    "        \n",
    "        return Problem(\n",
    "            problem_id=f\"trap_{self._counter}\",\n",
    "            title=\"Trapping Rain Water\",\n",
    "            description=\"Given n non-negative integers representing an elevation map, compute how much water it can trap after raining.\",\n",
    "            function_signature=\"def trap(height: list) -> int:\",\n",
    "            function_name=\"trap\",\n",
    "            test_cases=test_cases,\n",
    "        )\n",
    "\n",
    "# Test\n",
    "gen = TrappingRainWaterGenerator(seed=42)\n",
    "p = gen.generate()\n",
    "print(f\"Generated problem with {len(p.test_cases)} test cases\")\n",
    "print(f\"Example: trap({p.test_cases[0].input_args[0][:6]}...) -> {p.test_cases[0].expected_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Model with Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "if CONFIG[\"gradient_checkpointing\"]:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"Gradient checkpointing: ENABLED\")\n",
    "\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B params\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_code(response):\n",
    "    for pattern in [r\"```python\\n(.*?)```\", r\"```\\n(.*?)```\"]:\n",
    "        matches = re.findall(pattern, response, re.DOTALL)\n",
    "        if matches: return matches[-1].strip()\n",
    "    if \"def \" in response:\n",
    "        lines, code, in_func = response.split(\"\\n\"), [], False\n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"def \"): in_func = True\n",
    "            if in_func: code.append(line)\n",
    "        if code: return \"\\n\".join(code).strip()\n",
    "    return response\n",
    "\n",
    "def verify(code, problem):\n",
    "    namespace = {}\n",
    "    try:\n",
    "        exec(code, namespace)\n",
    "    except: return False, 0.0\n",
    "    \n",
    "    func = namespace.get(problem.function_name)\n",
    "    if not func:\n",
    "        funcs = [v for k, v in namespace.items() if callable(v) and not k.startswith(\"_\")]\n",
    "        func = funcs[0] if funcs else None\n",
    "    if not func: return False, 0.0\n",
    "    \n",
    "    passed = sum(1 for tc in problem.test_cases \n",
    "                 if _safe_call(func, tc.input_args) == tc.expected_output)\n",
    "    return passed == len(problem.test_cases), passed / len(problem.test_cases)\n",
    "\n",
    "def _safe_call(func, args):\n",
    "    try: return func(*args)\n",
    "    except: return None\n",
    "\n",
    "def generate(model, tokenizer, problem, temp=0.2):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n",
    "        {\"role\": \"user\", \"content\": problem.to_prompt()},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
    "                            do_sample=True, temperature=temp, top_p=0.9,\n",
    "                            pad_token_id=tokenizer.pad_token_id)\n",
    "    return tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Baseline\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gen = TrappingRainWaterGenerator(seed=CONFIG[\"seed\"])\n",
    "baseline_passed = 0\n",
    "for i in range(5):\n",
    "    p = gen.generate(CONFIG[\"difficulty\"], CONFIG[\"num_test_cases\"])\n",
    "    code = extract_code(generate(model, tokenizer, p))\n",
    "    success, partial = verify(code, p)\n",
    "    print(f\"  [{i+1}] {'PASS' if success else 'FAIL'} ({partial*100:.0f}%)\")\n",
    "    baseline_passed += success\n",
    "    clear_memory()\n",
    "\n",
    "baseline_acc = baseline_passed / 5\n",
    "print(f\"\\nBaseline: {baseline_passed}/5 ({baseline_acc*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: GRPO Training (Memory-Optimized)\n",
    "\n",
    "Key optimizations:\n",
    "1. Process ONE completion at a time (not batched)\n",
    "2. Immediately delete tensors after use\n",
    "3. Clear cache after each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CREATING REFERENCE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "clear_memory()\n",
    "print(f\"Before ref model: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Create reference model\n",
    "ref_model = copy.deepcopy(model)\n",
    "ref_model.eval()\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(f\"After ref model: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "print(\"Optimizer created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_step_memory_efficient(model, ref_model, optimizer, problem):\n",
    "    \"\"\"\n",
    "    Memory-efficient GRPO step:\n",
    "    1. Generate all completions first (no grad)\n",
    "    2. Compute rewards\n",
    "    3. Process ONE completion at a time for gradient\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n",
    "        {\"role\": \"user\", \"content\": problem.to_prompt()},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_len = inputs.input_ids.shape[1]\n",
    "    \n",
    "    # Phase 1: Generate completions (no grad)\n",
    "    completions = []\n",
    "    completion_texts = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(CONFIG[\"num_generations\"]):\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            # Store only the completion tokens (not full sequence)\n",
    "            comp_ids = out[0][prompt_len:].cpu()  # Move to CPU!\n",
    "            completions.append(comp_ids)\n",
    "            completion_texts.append(tokenizer.decode(comp_ids, skip_special_tokens=True))\n",
    "            del out\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    # Phase 2: Compute rewards\n",
    "    rewards = []\n",
    "    for text in completion_texts:\n",
    "        code = extract_code(text)\n",
    "        success, partial = verify(code, problem)\n",
    "        rewards.append(1.0 if success else partial * 0.5)\n",
    "    \n",
    "    rewards = torch.tensor(rewards)\n",
    "    advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "    \n",
    "    # Phase 3: Gradient update (one at a time)\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for comp_ids, adv in zip(completions, advantages):\n",
    "        # Reconstruct full sequence\n",
    "        full_ids = torch.cat([inputs.input_ids[0].cpu(), comp_ids]).unsqueeze(0).to(model.device)\n",
    "        attn_mask = torch.ones_like(full_ids)\n",
    "        \n",
    "        # Forward pass (policy)\n",
    "        outputs = model(input_ids=full_ids, attention_mask=attn_mask)\n",
    "        logits = outputs.logits[:, prompt_len-1:-1, :]  # Only completion tokens\n",
    "        target = full_ids[:, prompt_len:]\n",
    "        \n",
    "        # Log probs\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        token_log_probs = torch.gather(log_probs, -1, target.unsqueeze(-1)).squeeze(-1)\n",
    "        seq_log_prob = token_log_probs.mean()\n",
    "        \n",
    "        # Reference log prob\n",
    "        with torch.no_grad():\n",
    "            ref_out = ref_model(input_ids=full_ids, attention_mask=attn_mask)\n",
    "            ref_logits = ref_out.logits[:, prompt_len-1:-1, :]\n",
    "            ref_log_probs = torch.nn.functional.log_softmax(ref_logits, dim=-1)\n",
    "            ref_token_log_probs = torch.gather(ref_log_probs, -1, target.unsqueeze(-1)).squeeze(-1)\n",
    "            ref_seq_log_prob = ref_token_log_probs.mean()\n",
    "        \n",
    "        # Loss\n",
    "        kl = seq_log_prob - ref_seq_log_prob\n",
    "        loss = -adv.to(model.device) * seq_log_prob + CONFIG[\"beta\"] * kl\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Immediately free memory\n",
    "        del full_ids, attn_mask, outputs, logits, log_probs, token_log_probs\n",
    "        del ref_out, ref_logits, ref_log_probs, ref_token_log_probs\n",
    "        clear_memory()\n",
    "    \n",
    "    # Update\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    del inputs\n",
    "    clear_memory()\n",
    "    \n",
    "    return {\n",
    "        \"loss\": total_loss / CONFIG[\"num_generations\"],\n",
    "        \"mean_reward\": rewards.mean().item(),\n",
    "        \"max_reward\": rewards.max().item(),\n",
    "    }\n",
    "\n",
    "print(\"GRPO function defined (memory-optimized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"=\" * 60)\n",
    "print(\"GRPO TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Steps: {CONFIG['num_steps']}, Generations: {CONFIG['num_generations']}\")\n",
    "print()\n",
    "\n",
    "gen = TrappingRainWaterGenerator(seed=CONFIG[\"seed\"] + 100)\n",
    "metrics = []\n",
    "\n",
    "for step in range(CONFIG[\"num_steps\"]):\n",
    "    problem = gen.generate(CONFIG[\"difficulty\"], CONFIG[\"num_test_cases\"])\n",
    "    \n",
    "    try:\n",
    "        m = grpo_step_memory_efficient(model, ref_model, optimizer, problem)\n",
    "        metrics.append(m)\n",
    "        print(f\"Step {step+1:2d}: Loss={m['loss']:.4f}, Avg={m['mean_reward']:.3f}, Max={m['max_reward']:.3f}\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"Step {step+1}: OOM! Clearing memory and continuing...\")\n",
    "            clear_memory()\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Post-Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free reference model\n",
    "del ref_model\n",
    "clear_memory()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "eval_gen = TrappingRainWaterGenerator(seed=CONFIG[\"seed\"] + 999)\n",
    "final_passed = 0\n",
    "model.eval()\n",
    "\n",
    "for i in range(5):\n",
    "    p = eval_gen.generate(CONFIG[\"difficulty\"], CONFIG[\"num_test_cases\"])\n",
    "    code = extract_code(generate(model, tokenizer, p, temp=0.2))\n",
    "    success, partial = verify(code, p)\n",
    "    print(f\"  [{i+1}] {'PASS' if success else 'FAIL'} ({partial*100:.0f}%)\")\n",
    "    final_passed += success\n",
    "    clear_memory()\n",
    "\n",
    "final_acc = final_passed / 5\n",
    "print(f\"\\nFinal: {final_passed}/5 ({final_acc*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nBaseline: {baseline_acc*100:.0f}%\")\n",
    "print(f\"Final:    {final_acc*100:.0f}%\")\n",
    "print(f\"Change:   {(final_acc - baseline_acc)*100:+.0f}%\")\n",
    "\n",
    "if metrics:\n",
    "    print(f\"\\nTraining curve:\")\n",
    "    for i, m in enumerate(metrics):\n",
    "        print(f\"  Step {i+1}: reward={m['mean_reward']:.3f}\")\n",
    "\n",
    "if final_acc > baseline_acc:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUCCESS! Model improved!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save to Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# save_path = \"/content/drive/MyDrive/axiom-rl/qwen-1.5b-grpo\"\n",
    "# model.save_pretrained(save_path)\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "# print(f\"Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Still getting OOM?**\n",
    "1. Reduce `num_generations` to 1\n",
    "2. Reduce `max_new_tokens` to 256\n",
    "3. Use A100 runtime (Runtime -> Change runtime type)\n",
    "\n",
    "**Training not improving?**\n",
    "1. Increase `num_steps` to 20-30\n",
    "2. Try lower learning rate (1e-5)\n",
    "3. Consider teacher distillation instead"
   ]
  }
 ]
}
