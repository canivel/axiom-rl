{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen 1.5B GRPO Training on Colab\n",
    "\n",
    "This notebook runs GRPO (Group Relative Policy Optimization) on **Qwen2.5-Coder-1.5B-Instruct** to fix the Trapping Rain Water problem.\n",
    "\n",
    "## Why Colab?\n",
    "\n",
    "GRPO requires both a **policy model** and a **reference model**:\n",
    "- Local RTX 3080 (10GB): Cannot fit 1.5B + 1.5B + optimizer\n",
    "- Colab T4 (16GB): Should work with some optimization\n",
    "- Colab V100/A100: Comfortable fit\n",
    "\n",
    "## Background\n",
    "\n",
    "From our experiments:\n",
    "- Qwen 1.5B solves 9/10 hard problems natively (90%)\n",
    "- Only failure: **Trapping Rain Water** (4/5 = 80%)\n",
    "- Goal: Use GRPO to fix this single failure\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check Runtime\n",
    "\n",
    "**IMPORTANT**: Make sure you're using a GPU runtime!\n",
    "- Go to Runtime -> Change runtime type\n",
    "- Select **GPU** (T4 is fine, V100/A100 is better)\n",
    "- Click Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and memory\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    if gpu_memory < 15:\n",
    "        print(\"\\nWARNING: Less than 15GB VRAM. Training may be tight.\")\n",
    "        print(\"Consider using T4 (16GB) or better.\")\n",
    "    else:\n",
    "        print(\"\\nGood! Sufficient memory for GRPO on 1.5B model.\")\n",
    "else:\n",
    "    print(\"ERROR: No GPU found!\")\n",
    "    print(\"Go to Runtime -> Change runtime type -> GPU\")\n",
    "    raise RuntimeError(\"GPU required for this notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "\n",
    "Install the required packages. This takes 2-3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Install dependencies\n",
    "!pip install -q torch transformers accelerate peft bitsandbytes datasets trl\n",
    "print(\"\\nDependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration\n",
    "\n",
    "Set up the experiment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"model_name\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "    \n",
    "    # Problem to fix\n",
    "    \"target_problem\": \"trapping_rain_water\",\n",
    "    \n",
    "    # GRPO settings\n",
    "    \"num_steps\": 10,           # Training steps\n",
    "    \"num_generations\": 4,      # Generations per prompt (G)\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"beta\": 0.04,              # KL penalty\n",
    "    \"max_seq_length\": 768,\n",
    "    \n",
    "    # Problem settings\n",
    "    \"difficulty\": 5,           # 1-10 scale\n",
    "    \"num_test_cases\": 5,\n",
    "    \n",
    "    # Seed\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key:20} = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Problem Generator\n",
    "\n",
    "Create the Trapping Rain Water problem generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Any\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"A single test case.\"\"\"\n",
    "    input_args: List[Any]\n",
    "    expected_output: Any\n",
    "\n",
    "@dataclass \n",
    "class AlgorithmicProblem:\n",
    "    \"\"\"A problem with test cases.\"\"\"\n",
    "    problem_type: str\n",
    "    problem_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    function_signature: str\n",
    "    function_name: str\n",
    "    test_cases: List[TestCase]\n",
    "    difficulty: int = 5\n",
    "    \n",
    "    def to_prompt(self) -> str:\n",
    "        \"\"\"Convert to prompt for model.\"\"\"\n",
    "        examples = \"\\n\".join(\n",
    "            f\"  {self.function_name}({repr(tc.input_args[0])}) -> {repr(tc.expected_output)}\"\n",
    "            for tc in self.test_cases[:3]\n",
    "        )\n",
    "        return f\"\"\"## {self.title}\n",
    "\n",
    "{self.description}\n",
    "\n",
    "### Function Signature\n",
    "```python\n",
    "{self.function_signature}\n",
    "```\n",
    "\n",
    "### Examples\n",
    "```python\n",
    "{examples}\n",
    "```\n",
    "\n",
    "Implement the function. Your solution must pass ALL test cases.\"\"\"\n",
    "\n",
    "\n",
    "class TrappingRainWaterGenerator:\n",
    "    \"\"\"\n",
    "    Generates Trapping Rain Water problems.\n",
    "    \n",
    "    Given an elevation map, calculate how much water can be trapped.\n",
    "    Classic two-pointer or DP problem.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed: int = None):\n",
    "        self.rng = random.Random(seed)\n",
    "        self._counter = 0\n",
    "    \n",
    "    @property\n",
    "    def problem_type(self) -> str:\n",
    "        return \"trapping_rain_water\"\n",
    "    \n",
    "    @property\n",
    "    def title(self) -> str:\n",
    "        return \"Trapping Rain Water\"\n",
    "    \n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return \"\"\"Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it can trap after raining.\n",
    "\n",
    "The elevation map is represented as a list of integers where each integer represents the height of a bar.\n",
    "\n",
    "Example: For heights [0,1,0,2,1,0,1,3,2,1,2,1], the answer is 6.\n",
    "The water fills the valleys between the bars.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def function_signature(self) -> str:\n",
    "        return \"def trap(height: list) -> int:\"\n",
    "    \n",
    "    @property\n",
    "    def function_name(self) -> str:\n",
    "        return \"trap\"\n",
    "    \n",
    "    def _solve(self, height: List[int]) -> int:\n",
    "        \"\"\"Reference solution using two pointers.\"\"\"\n",
    "        if not height:\n",
    "            return 0\n",
    "        \n",
    "        left, right = 0, len(height) - 1\n",
    "        left_max = right_max = 0\n",
    "        water = 0\n",
    "        \n",
    "        while left < right:\n",
    "            if height[left] < height[right]:\n",
    "                if height[left] >= left_max:\n",
    "                    left_max = height[left]\n",
    "                else:\n",
    "                    water += left_max - height[left]\n",
    "                left += 1\n",
    "            else:\n",
    "                if height[right] >= right_max:\n",
    "                    right_max = height[right]\n",
    "                else:\n",
    "                    water += right_max - height[right]\n",
    "                right -= 1\n",
    "        \n",
    "        return water\n",
    "    \n",
    "    def _generate_heights(self, difficulty: int) -> List[int]:\n",
    "        \"\"\"Generate random height array.\"\"\"\n",
    "        # Length based on difficulty\n",
    "        length = 5 + difficulty * 2\n",
    "        max_height = 3 + difficulty\n",
    "        \n",
    "        # Generate heights with some structure to ensure water can be trapped\n",
    "        heights = []\n",
    "        for i in range(length):\n",
    "            h = self.rng.randint(0, max_height)\n",
    "            heights.append(h)\n",
    "        \n",
    "        return heights\n",
    "    \n",
    "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> AlgorithmicProblem:\n",
    "        \"\"\"Generate a problem instance.\"\"\"\n",
    "        self._counter += 1\n",
    "        \n",
    "        test_cases = []\n",
    "        for _ in range(num_test_cases):\n",
    "            heights = self._generate_heights(difficulty)\n",
    "            expected = self._solve(heights)\n",
    "            test_cases.append(TestCase(\n",
    "                input_args=[heights],\n",
    "                expected_output=expected\n",
    "            ))\n",
    "        \n",
    "        return AlgorithmicProblem(\n",
    "            problem_type=self.problem_type,\n",
    "            problem_id=f\"{self.problem_type}_{self._counter}\",\n",
    "            title=self.title,\n",
    "            description=self.description,\n",
    "            function_signature=self.function_signature,\n",
    "            function_name=self.function_name,\n",
    "            test_cases=test_cases,\n",
    "            difficulty=difficulty,\n",
    "        )\n",
    "\n",
    "\n",
    "# Test the generator\n",
    "print(\"=\" * 60)\n",
    "print(\"PROBLEM GENERATOR TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gen = TrappingRainWaterGenerator(seed=42)\n",
    "test_problem = gen.generate(difficulty=5, num_test_cases=5)\n",
    "\n",
    "print(f\"\\nGenerated problem: {test_problem.problem_id}\")\n",
    "print(f\"Test cases: {len(test_problem.test_cases)}\")\n",
    "print(\"\\nSample test cases:\")\n",
    "for i, tc in enumerate(test_problem.test_cases[:3], 1):\n",
    "    print(f\"  {i}. trap({tc.input_args[0][:8]}...) -> {tc.expected_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load the Model\n",
    "\n",
    "Load Qwen 1.5B with float16 for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel: {CONFIG['model_name']}\")\n",
    "print(\"Loading... (this takes 1-2 minutes)\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Tokenizer loaded!\")\n",
    "\n",
    "# Load model in float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "\n",
    "# Model info\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Parameters: {num_params / 1e9:.2f}B\")\n",
    "\n",
    "# Check memory usage\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"\\nGPU Memory:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Evaluation\n",
    "\n",
    "Test the model on Trapping Rain Water before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_code(response: str) -> str:\n",
    "    \"\"\"Extract Python code from model response.\"\"\"\n",
    "    patterns = [r\"```python\\n(.*?)```\", r\"```\\n(.*?)```\"]\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, response, re.DOTALL)\n",
    "        if matches:\n",
    "            return matches[-1].strip()\n",
    "    \n",
    "    # Fallback: look for function definition\n",
    "    if \"def \" in response:\n",
    "        lines = response.split(\"\\n\")\n",
    "        code_lines = []\n",
    "        in_function = False\n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"def \"):\n",
    "                in_function = True\n",
    "            if in_function:\n",
    "                code_lines.append(line)\n",
    "        if code_lines:\n",
    "            return \"\\n\".join(code_lines).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def verify_solution(code: str, problem: AlgorithmicProblem) -> tuple:\n",
    "    \"\"\"Verify solution against all test cases.\"\"\"\n",
    "    func_name = problem.function_name\n",
    "    namespace = {}\n",
    "    \n",
    "    try:\n",
    "        exec(code, namespace)\n",
    "    except Exception as e:\n",
    "        return False, 0.0, str(e)\n",
    "    \n",
    "    if func_name not in namespace:\n",
    "        # Try to find any function\n",
    "        funcs = [k for k, v in namespace.items() if callable(v) and not k.startswith(\"_\")]\n",
    "        if funcs:\n",
    "            func_name = funcs[0]\n",
    "        else:\n",
    "            return False, 0.0, \"No function found\"\n",
    "    \n",
    "    func = namespace[func_name]\n",
    "    passed = 0\n",
    "    total = len(problem.test_cases)\n",
    "    \n",
    "    for tc in problem.test_cases:\n",
    "        try:\n",
    "            result = func(*tc.input_args)\n",
    "            if result == tc.expected_output:\n",
    "                passed += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    success = passed == total\n",
    "    partial = passed / total if total > 0 else 0.0\n",
    "    return success, partial, None\n",
    "\n",
    "\n",
    "def generate_solution(model, tokenizer, problem, temperature=0.2):\n",
    "    \"\"\"Generate a solution for a problem.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Python programmer. Write ONLY the function implementation.\"},\n",
    "        {\"role\": \"user\", \"content\": problem.to_prompt()},\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Baseline evaluation\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "generator = TrappingRainWaterGenerator(seed=CONFIG[\"seed\"])\n",
    "num_eval = 5\n",
    "passed_count = 0\n",
    "\n",
    "print(f\"\\nTesting {num_eval} problems...\")\n",
    "for i in range(num_eval):\n",
    "    problem = generator.generate(difficulty=CONFIG[\"difficulty\"], num_test_cases=CONFIG[\"num_test_cases\"])\n",
    "    response = generate_solution(model, tokenizer, problem)\n",
    "    code = extract_code(response)\n",
    "    success, partial, error = verify_solution(code, problem)\n",
    "    \n",
    "    status = \"PASS\" if success else \"FAIL\"\n",
    "    print(f\"  [{i+1}] {status} ({partial*100:.0f}%)\")\n",
    "    \n",
    "    if success:\n",
    "        passed_count += 1\n",
    "\n",
    "baseline_accuracy = passed_count / num_eval\n",
    "print(f\"\\nBaseline Accuracy: {passed_count}/{num_eval} ({baseline_accuracy*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: GRPO Training\n",
    "\n",
    "Now we implement and run GRPO to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from torch.optim import AdamW\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GRPO TRAINING SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create reference model (frozen copy)\n",
    "print(\"\\nCreating reference model...\")\n",
    "ref_model = copy.deepcopy(model)\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Reference model created!\")\n",
    "\n",
    "# Check memory after creating reference\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"\\nGPU Memory after ref model: {allocated:.2f} GB\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "print(f\"Optimizer: AdamW (lr={CONFIG['learning_rate']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reward_function(problem):\n",
    "    \"\"\"Create reward function for a problem.\"\"\"\n",
    "    def reward_fn(completions: list) -> torch.Tensor:\n",
    "        rewards = []\n",
    "        for completion in completions:\n",
    "            code = extract_code(completion)\n",
    "            success, partial, _ = verify_solution(code, problem)\n",
    "            if success:\n",
    "                reward = 1.0\n",
    "            else:\n",
    "                reward = partial * 0.5  # Partial credit\n",
    "            rewards.append(reward)\n",
    "        return torch.tensor(rewards, dtype=torch.float32)\n",
    "    return reward_fn\n",
    "\n",
    "\n",
    "def compute_log_probs(model, input_ids, attention_mask, labels):\n",
    "    \"\"\"Compute log probabilities for a sequence.\"\"\"\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Shift for next token prediction\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    # Compute log probs\n",
    "    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "    token_log_probs = torch.gather(log_probs, dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # Mask padding\n",
    "    mask = (shift_labels != tokenizer.pad_token_id).float()\n",
    "    sequence_log_prob = (token_log_probs * mask).sum(dim=-1) / mask.sum(dim=-1).clamp(min=1)\n",
    "    \n",
    "    return sequence_log_prob\n",
    "\n",
    "\n",
    "def grpo_step(model, ref_model, optimizer, prompt, problem, num_generations=4):\n",
    "    \"\"\"\n",
    "    Single GRPO training step.\n",
    "    \n",
    "    1. Generate G completions\n",
    "    2. Compute rewards for each\n",
    "    3. Compute group-relative advantages\n",
    "    4. Update policy to maximize advantage-weighted log probs\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Build prompt\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_length = inputs.input_ids.shape[1]\n",
    "    \n",
    "    # Generate completions\n",
    "    completions = []\n",
    "    completion_ids = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_generations):\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            completion = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n",
    "            completions.append(completion)\n",
    "            completion_ids.append(outputs[0])\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Compute rewards\n",
    "    reward_fn = create_reward_function(problem)\n",
    "    rewards = reward_fn(completions)\n",
    "    \n",
    "    # Group-relative advantages\n",
    "    mean_reward = rewards.mean()\n",
    "    std_reward = rewards.std() + 1e-8\n",
    "    advantages = (rewards - mean_reward) / std_reward\n",
    "    \n",
    "    # Compute loss\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i, (completion_id, advantage) in enumerate(zip(completion_ids, advantages)):\n",
    "        # Prepare inputs\n",
    "        input_ids = completion_id.unsqueeze(0)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Policy log prob\n",
    "        policy_log_prob = compute_log_probs(model, input_ids, attention_mask, input_ids)\n",
    "        \n",
    "        # Reference log prob (for KL penalty)\n",
    "        with torch.no_grad():\n",
    "            ref_log_prob = compute_log_probs(ref_model, input_ids, attention_mask, input_ids)\n",
    "        \n",
    "        # KL penalty\n",
    "        kl_penalty = policy_log_prob - ref_log_prob\n",
    "        \n",
    "        # Loss: -advantage * log_prob + beta * KL\n",
    "        loss = -advantage.to(model.device) * policy_log_prob + CONFIG[\"beta\"] * kl_penalty\n",
    "        total_loss += loss\n",
    "    \n",
    "    total_loss = total_loss / num_generations\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return {\n",
    "        \"loss\": total_loss.item(),\n",
    "        \"mean_reward\": mean_reward.item(),\n",
    "        \"max_reward\": rewards.max().item(),\n",
    "        \"completions\": completions,\n",
    "        \"rewards\": rewards.tolist(),\n",
    "    }\n",
    "\n",
    "print(\"GRPO functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GRPO training\n",
    "print(\"=\" * 60)\n",
    "print(\"GRPO TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSteps: {CONFIG['num_steps']}\")\n",
    "print(f\"Generations per step: {CONFIG['num_generations']}\")\n",
    "print()\n",
    "\n",
    "generator = TrappingRainWaterGenerator(seed=CONFIG[\"seed\"] + 100)  # Different seed from eval\n",
    "training_metrics = []\n",
    "\n",
    "for step in range(CONFIG[\"num_steps\"]):\n",
    "    # Generate new problem\n",
    "    problem = generator.generate(\n",
    "        difficulty=CONFIG[\"difficulty\"],\n",
    "        num_test_cases=CONFIG[\"num_test_cases\"]\n",
    "    )\n",
    "    prompt = problem.to_prompt()\n",
    "    \n",
    "    # GRPO step\n",
    "    metrics = grpo_step(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        optimizer=optimizer,\n",
    "        prompt=prompt,\n",
    "        problem=problem,\n",
    "        num_generations=CONFIG[\"num_generations\"]\n",
    "    )\n",
    "    \n",
    "    training_metrics.append(metrics)\n",
    "    \n",
    "    print(f\"Step {step+1:2d}/{CONFIG['num_steps']}: \"\n",
    "          f\"Loss={metrics['loss']:.4f}, \"\n",
    "          f\"Avg Reward={metrics['mean_reward']:.3f}, \"\n",
    "          f\"Max Reward={metrics['max_reward']:.3f}\")\n",
    "    \n",
    "    # Clear cache periodically\n",
    "    if (step + 1) % 5 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Post-Training Evaluation\n",
    "\n",
    "Evaluate the model after GRPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete reference model to free memory\n",
    "del ref_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use fresh seed for evaluation\n",
    "eval_generator = TrappingRainWaterGenerator(seed=CONFIG[\"seed\"] + 999)\n",
    "num_eval = 5\n",
    "passed_count = 0\n",
    "\n",
    "print(f\"\\nTesting {num_eval} problems...\")\n",
    "model.eval()\n",
    "\n",
    "for i in range(num_eval):\n",
    "    problem = eval_generator.generate(\n",
    "        difficulty=CONFIG[\"difficulty\"],\n",
    "        num_test_cases=CONFIG[\"num_test_cases\"]\n",
    "    )\n",
    "    response = generate_solution(model, tokenizer, problem, temperature=0.2)\n",
    "    code = extract_code(response)\n",
    "    success, partial, error = verify_solution(code, problem)\n",
    "    \n",
    "    status = \"PASS\" if success else \"FAIL\"\n",
    "    print(f\"  [{i+1}] {status} ({partial*100:.0f}%)\")\n",
    "    \n",
    "    if success:\n",
    "        passed_count += 1\n",
    "\n",
    "final_accuracy = passed_count / num_eval\n",
    "print(f\"\\nFinal Accuracy: {passed_count}/{num_eval} ({final_accuracy*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel: {CONFIG['model_name']}\")\n",
    "print(f\"Problem: {CONFIG['target_problem']}\")\n",
    "print(f\"Training Steps: {CONFIG['num_steps']}\")\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Before':>12} {'After':>12} {'Change':>12}\")\n",
    "print(\"-\" * 58)\n",
    "change = (final_accuracy - baseline_accuracy) * 100\n",
    "print(f\"{'Accuracy':<20} {baseline_accuracy*100:>11.0f}% {final_accuracy*100:>11.0f}% {change:>+11.0f}%\")\n",
    "\n",
    "# Training curve\n",
    "print(f\"\\nTraining Progress:\")\n",
    "print(f\"{'Step':>4} {'Avg Reward':>12} {'Max Reward':>12}\")\n",
    "print(\"-\" * 30)\n",
    "for i, m in enumerate(training_metrics):\n",
    "    print(f\"{i+1:>4} {m['mean_reward']:>12.3f} {m['max_reward']:>12.3f}\")\n",
    "\n",
    "if final_accuracy > baseline_accuracy:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUCCESS! GRPO improved performance on Trapping Rain Water!\")\n",
    "    print(f\"{'='*60}\")\n",
    "elif final_accuracy == baseline_accuracy:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"No change. May need more training steps or different hyperparameters.\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Performance decreased. Consider reducing learning rate.\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Model (Optional)\n",
    "\n",
    "Save the trained model to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Save model\n",
    "save_path = \"/content/drive/MyDrive/axiom-rl/models/qwen-1.5b-grpo-trapping\"\n",
    "print(f\"Saving model to: {save_path}\")\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **If successful**: Download the model and test locally on all hard problems\n",
    "2. **If unsuccessful**: Try:\n",
    "   - More training steps (20-50)\n",
    "   - Lower learning rate (1e-5)\n",
    "   - More generations per step (8)\n",
    "   - Teacher distillation instead of GRPO\n",
    "\n",
    "## Memory Tips\n",
    "\n",
    "If you get OOM errors:\n",
    "1. Reduce `num_generations` to 2\n",
    "2. Reduce `max_seq_length` to 512\n",
    "3. Use gradient checkpointing\n",
    "4. Upgrade to A100 runtime"
   ]
  }
 ]
}
