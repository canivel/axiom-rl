{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen 1.5B GRPO Training on Colab TPU\n",
    "\n",
    "This notebook runs GRPO on **Qwen2.5-Coder-1.5B-Instruct** using **TPU v2-8**.\n",
    "\n",
    "## TPU vs GPU\n",
    "\n",
    "TPUs have more memory (8GB per core x 8 cores = 64GB total) but require special setup:\n",
    "- Use `torch_xla` for TPU support\n",
    "- Models need to be moved to TPU device explicitly\n",
    "- Some operations work differently\n",
    "\n",
    "## Goal\n",
    "\n",
    "Fix Trapping Rain Water (only Qwen 1.5B failure: 4/5 = 80%)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check Runtime Type\n",
    "\n",
    "Make sure you selected **TPU** runtime:\n",
    "- Runtime -> Change runtime type -> TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\n\nprint(\"=\" * 60)\nprint(\"STEP 1: INSTALL DEPENDENCIES\")\nprint(\"=\" * 60)\n\n# Check if we might be on TPU (install torch_xla)\nmight_be_tpu = 'COLAB_TPU_ADDR' in os.environ or 'TPU_NAME' in os.environ\n\nif might_be_tpu:\n    print(\"TPU environment variable detected, installing torch_xla...\")\n    os.system(\"pip install -q torch~=2.1.0 torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html\")\nelse:\n    print(\"No TPU env var found, skipping torch_xla\")\n\nos.system(\"pip install -q transformers accelerate sentencepiece\")\nprint(\"Core dependencies installed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"STEP 2: DETECT RUNTIME\")\nprint(\"=\" * 60)\n\nUSE_TPU = False\n\n# Method 1: Check COLAB_TPU_ADDR (TPU v2/v3)\nif 'COLAB_TPU_ADDR' in os.environ:\n    print(f\"TPU Address: {os.environ['COLAB_TPU_ADDR']}\")\n    USE_TPU = True\n\n# Method 2: Check for TPU_NAME (TPU v4/newer)\nelif 'TPU_NAME' in os.environ:\n    print(f\"TPU Name: {os.environ['TPU_NAME']}\")\n    USE_TPU = True\n\n# Method 3: Try to detect TPU via torch_xla (if installed)\nelse:\n    try:\n        import torch_xla\n        import torch_xla.core.xla_model as xm\n        device = xm.xla_device()\n        print(f\"TPU detected via torch_xla: {device}\")\n        USE_TPU = True\n    except ImportError:\n        print(\"torch_xla not installed (not on TPU)\")\n    except Exception as e:\n        print(f\"torch_xla error: {e}\")\n\nif USE_TPU:\n    print(\"\\n✅ TPU detected! Using TPU runtime.\")\nelse:\n    # Check for GPU as fallback\n    import torch\n    if torch.cuda.is_available():\n        print(f\"\\n✅ GPU detected: {torch.cuda.get_device_name(0)}\")\n        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n        print(f\"   GPU Memory: {gpu_mem:.1f} GB\")\n    else:\n        print(\"\\n⚠️  No TPU or GPU found!\")\n        print(\"   Go to Runtime -> Change runtime type -> TPU or GPU\")\n        print(\"   Continuing anyway (will use CPU, very slow)...\")\n\nprint(f\"\\nUSE_TPU = {USE_TPU}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup TPU Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "if USE_TPU:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    \n",
    "    # Get TPU device\n",
    "    device = xm.xla_device()\n",
    "    print(f\"TPU Device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if USE_TPU:\n",
    "        xm.mark_step()  # Sync TPU\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nDevice ready: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"model_name\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "    \"num_steps\": 10,\n",
    "    \"num_generations\": 4,      # TPU has more memory\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"beta\": 0.04,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"difficulty\": 5,\n",
    "    \"num_test_cases\": 5,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k:20} = {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Problem Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Any\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    input_args: List[Any]\n",
    "    expected_output: Any\n",
    "\n",
    "@dataclass \n",
    "class Problem:\n",
    "    problem_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    function_signature: str\n",
    "    function_name: str\n",
    "    test_cases: List[TestCase]\n",
    "    \n",
    "    def to_prompt(self) -> str:\n",
    "        examples = \"\\n\".join(\n",
    "            f\"  {self.function_name}({repr(tc.input_args[0])}) -> {repr(tc.expected_output)}\"\n",
    "            for tc in self.test_cases[:3]\n",
    "        )\n",
    "        return f\"\"\"## {self.title}\n",
    "\n",
    "{self.description}\n",
    "\n",
    "### Function Signature\n",
    "```python\n",
    "{self.function_signature}\n",
    "```\n",
    "\n",
    "### Examples\n",
    "```python\n",
    "{examples}\n",
    "```\n",
    "\n",
    "Write ONLY the function implementation.\"\"\"\n",
    "\n",
    "\n",
    "class TrappingRainWaterGenerator:\n",
    "    def __init__(self, seed=None):\n",
    "        self.rng = random.Random(seed)\n",
    "        self._counter = 0\n",
    "    \n",
    "    def _solve(self, height):\n",
    "        if not height: return 0\n",
    "        left, right = 0, len(height) - 1\n",
    "        left_max = right_max = water = 0\n",
    "        while left < right:\n",
    "            if height[left] < height[right]:\n",
    "                if height[left] >= left_max: left_max = height[left]\n",
    "                else: water += left_max - height[left]\n",
    "                left += 1\n",
    "            else:\n",
    "                if height[right] >= right_max: right_max = height[right]\n",
    "                else: water += right_max - height[right]\n",
    "                right -= 1\n",
    "        return water\n",
    "    \n",
    "    def generate(self, difficulty=5, num_test_cases=5):\n",
    "        self._counter += 1\n",
    "        test_cases = []\n",
    "        for _ in range(num_test_cases):\n",
    "            length = 5 + difficulty * 2\n",
    "            heights = [self.rng.randint(0, 3 + difficulty) for _ in range(length)]\n",
    "            test_cases.append(TestCase([heights], self._solve(heights)))\n",
    "        \n",
    "        return Problem(\n",
    "            problem_id=f\"trap_{self._counter}\",\n",
    "            title=\"Trapping Rain Water\",\n",
    "            description=\"Given n non-negative integers representing an elevation map, compute how much water it can trap after raining.\",\n",
    "            function_signature=\"def trap(height: list) -> int:\",\n",
    "            function_name=\"trap\",\n",
    "            test_cases=test_cases,\n",
    "        )\n",
    "\n",
    "# Test\n",
    "gen = TrappingRainWaterGenerator(seed=42)\n",
    "p = gen.generate()\n",
    "print(f\"Generated problem with {len(p.test_cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(\"Loading... (this takes 2-3 minutes on TPU)\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Tokenizer loaded!\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=torch.float32 if USE_TPU else torch.float16,  # TPU prefers float32\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on: {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_code(response):\n",
    "    for pattern in [r\"```python\\n(.*?)```\", r\"```\\n(.*?)```\"]:\n",
    "        matches = re.findall(pattern, response, re.DOTALL)\n",
    "        if matches: return matches[-1].strip()\n",
    "    if \"def \" in response:\n",
    "        lines, code, in_func = response.split(\"\\n\"), [], False\n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"def \"): in_func = True\n",
    "            if in_func: code.append(line)\n",
    "        if code: return \"\\n\".join(code).strip()\n",
    "    return response\n",
    "\n",
    "def verify(code, problem):\n",
    "    namespace = {}\n",
    "    try:\n",
    "        exec(code, namespace)\n",
    "    except: return False, 0.0\n",
    "    \n",
    "    func = namespace.get(problem.function_name)\n",
    "    if not func:\n",
    "        funcs = [v for k, v in namespace.items() if callable(v) and not k.startswith(\"_\")]\n",
    "        func = funcs[0] if funcs else None\n",
    "    if not func: return False, 0.0\n",
    "    \n",
    "    passed = sum(1 for tc in problem.test_cases \n",
    "                 if _safe_call(func, tc.input_args) == tc.expected_output)\n",
    "    return passed == len(problem.test_cases), passed / len(problem.test_cases)\n",
    "\n",
    "def _safe_call(func, args):\n",
    "    try: return func(*args)\n",
    "    except: return None\n",
    "\n",
    "def generate_solution(model, tokenizer, problem, temp=0.2):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n",
    "        {\"role\": \"user\", \"content\": problem.to_prompt()},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
    "            do_sample=True, \n",
    "            temperature=temp, \n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    if USE_TPU:\n",
    "        xm.mark_step()  # Sync TPU\n",
    "    \n",
    "    return tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BASELINE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gen = TrappingRainWaterGenerator(seed=CONFIG[\"seed\"])\n",
    "baseline_passed = 0\n",
    "\n",
    "for i in range(5):\n",
    "    p = gen.generate(CONFIG[\"difficulty\"], CONFIG[\"num_test_cases\"])\n",
    "    response = generate_solution(model, tokenizer, p)\n",
    "    code = extract_code(response)\n",
    "    success, partial = verify(code, p)\n",
    "    print(f\"  [{i+1}] {'PASS' if success else 'FAIL'} ({partial*100:.0f}%)\")\n",
    "    baseline_passed += success\n",
    "    clear_memory()\n",
    "\n",
    "baseline_acc = baseline_passed / 5\n",
    "print(f\"\\nBaseline: {baseline_passed}/5 ({baseline_acc*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create Reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CREATING REFERENCE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create reference model (frozen copy)\n",
    "ref_model = copy.deepcopy(model)\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Reference model created!\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "print(f\"Optimizer: AdamW (lr={CONFIG['learning_rate']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_step(model, ref_model, optimizer, problem):\n",
    "    \"\"\"Single GRPO training step.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n",
    "        {\"role\": \"user\", \"content\": problem.to_prompt()},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    prompt_len = inputs.input_ids.shape[1]\n",
    "    \n",
    "    # Generate completions\n",
    "    completions = []\n",
    "    completion_ids = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(CONFIG[\"num_generations\"]):\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            comp_text = tokenizer.decode(out[0][prompt_len:], skip_special_tokens=True)\n",
    "            completions.append(comp_text)\n",
    "            completion_ids.append(out[0])\n",
    "            if USE_TPU:\n",
    "                xm.mark_step()\n",
    "    \n",
    "    # Compute rewards\n",
    "    rewards = []\n",
    "    for comp in completions:\n",
    "        code = extract_code(comp)\n",
    "        success, partial = verify(code, problem)\n",
    "        rewards.append(1.0 if success else partial * 0.5)\n",
    "    \n",
    "    rewards = torch.tensor(rewards, device=device)\n",
    "    advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for comp_ids, adv in zip(completion_ids, advantages):\n",
    "        full_ids = comp_ids.unsqueeze(0)\n",
    "        attn_mask = torch.ones_like(full_ids)\n",
    "        \n",
    "        # Policy forward\n",
    "        outputs = model(input_ids=full_ids, attention_mask=attn_mask)\n",
    "        logits = outputs.logits[:, prompt_len-1:-1, :]\n",
    "        target = full_ids[:, prompt_len:]\n",
    "        \n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        token_log_probs = torch.gather(log_probs, -1, target.unsqueeze(-1)).squeeze(-1)\n",
    "        seq_log_prob = token_log_probs.mean()\n",
    "        \n",
    "        # Reference forward\n",
    "        with torch.no_grad():\n",
    "            ref_out = ref_model(input_ids=full_ids, attention_mask=attn_mask)\n",
    "            ref_logits = ref_out.logits[:, prompt_len-1:-1, :]\n",
    "            ref_log_probs = torch.nn.functional.log_softmax(ref_logits, dim=-1)\n",
    "            ref_token_log_probs = torch.gather(ref_log_probs, -1, target.unsqueeze(-1)).squeeze(-1)\n",
    "            ref_seq_log_prob = ref_token_log_probs.mean()\n",
    "        \n",
    "        # Loss\n",
    "        kl = seq_log_prob - ref_seq_log_prob\n",
    "        loss = -adv * seq_log_prob + CONFIG[\"beta\"] * kl\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if USE_TPU:\n",
    "            xm.mark_step()\n",
    "    \n",
    "    # Update\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    if USE_TPU:\n",
    "        xm.optimizer_step(optimizer)\n",
    "        xm.mark_step()\n",
    "    else:\n",
    "        optimizer.step()\n",
    "    \n",
    "    return {\n",
    "        \"loss\": total_loss / CONFIG[\"num_generations\"],\n",
    "        \"mean_reward\": rewards.mean().item(),\n",
    "        \"max_reward\": rewards.max().item(),\n",
    "    }\n",
    "\n",
    "print(\"GRPO function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"=\" * 60)\n",
    "print(\"GRPO TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Steps: {CONFIG['num_steps']}, Generations: {CONFIG['num_generations']}\")\n",
    "print()\n",
    "\n",
    "train_gen = TrappingRainWaterGenerator(seed=CONFIG[\"seed\"] + 100)\n",
    "metrics = []\n",
    "\n",
    "for step in range(CONFIG[\"num_steps\"]):\n",
    "    problem = train_gen.generate(CONFIG[\"difficulty\"], CONFIG[\"num_test_cases\"])\n",
    "    \n",
    "    try:\n",
    "        m = grpo_step(model, ref_model, optimizer, problem)\n",
    "        metrics.append(m)\n",
    "        print(f\"Step {step+1:2d}: Loss={m['loss']:.4f}, Avg={m['mean_reward']:.3f}, Max={m['max_reward']:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Step {step+1}: Error - {e}\")\n",
    "    \n",
    "    clear_memory()\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Post-Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free reference model\n",
    "del ref_model\n",
    "clear_memory()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POST-TRAINING EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "eval_gen = TrappingRainWaterGenerator(seed=CONFIG[\"seed\"] + 999)\n",
    "final_passed = 0\n",
    "model.eval()\n",
    "\n",
    "for i in range(5):\n",
    "    p = eval_gen.generate(CONFIG[\"difficulty\"], CONFIG[\"num_test_cases\"])\n",
    "    response = generate_solution(model, tokenizer, p, temp=0.2)\n",
    "    code = extract_code(response)\n",
    "    success, partial = verify(code, p)\n",
    "    print(f\"  [{i+1}] {'PASS' if success else 'FAIL'} ({partial*100:.0f}%)\")\n",
    "    final_passed += success\n",
    "    clear_memory()\n",
    "\n",
    "final_acc = final_passed / 5\n",
    "print(f\"\\nFinal: {final_passed}/5 ({final_acc*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nBaseline: {baseline_acc*100:.0f}%\")\n",
    "print(f\"Final:    {final_acc*100:.0f}%\")\n",
    "print(f\"Change:   {(final_acc - baseline_acc)*100:+.0f}%\")\n",
    "\n",
    "if metrics:\n",
    "    print(f\"\\nTraining curve:\")\n",
    "    for i, m in enumerate(metrics):\n",
    "        print(f\"  Step {i+1}: reward={m['mean_reward']:.3f}, max={m['max_reward']:.3f}\")\n",
    "\n",
    "if final_acc > baseline_acc:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUCCESS! Model improved!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Save Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save to Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# # Move model to CPU for saving\n",
    "# model_cpu = model.to('cpu')\n",
    "# save_path = \"/content/drive/MyDrive/axiom-rl/qwen-1.5b-grpo-tpu\"\n",
    "# model_cpu.save_pretrained(save_path)\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "# print(f\"Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "**TPU-specific considerations:**\n",
    "- Use `xm.mark_step()` to sync TPU operations\n",
    "- Use `xm.optimizer_step()` for optimizer updates\n",
    "- TPU prefers float32 over float16\n",
    "- First compilation is slow, subsequent runs are faster\n",
    "\n",
    "**If training is slow:**\n",
    "- This is normal for the first few steps (JIT compilation)\n",
    "- TPU shines on larger batch sizes\n",
    "- Consider using GPU (T4/A100) for this small-scale experiment"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}