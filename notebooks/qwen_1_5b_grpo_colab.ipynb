{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Qwen 1.5B GRPO Training on Colab\n\nTrain **Qwen2.5-Coder-1.5B-Instruct** to fix Trapping Rain Water (80% â†’ 100%).\n\n## Runtime Selection Guide\n\n| Runtime | VRAM | Approach | Speed |\n|---------|------|----------|-------|\n| **A100** | 40GB | Full GRPO | Fast |\n| **L4** | 24GB | Full GRPO | Fast |\n| **T4** | 16GB | LoRA+REINFORCE | Medium |\n| **TPU** | 16GB | LoRA+REINFORCE | Slow |\n\n**Recommended**: A100 or L4 for best results.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check Runtime Type\n",
    "\n",
    "Make sure you selected **TPU** runtime:\n",
    "- Runtime -> Change runtime type -> TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\n\nprint(\"=\" * 60)\nprint(\"STEP 1: INSTALL DEPENDENCIES\")\nprint(\"=\" * 60)\n\n# Install all dependencies\nos.system(\"pip install -q transformers accelerate sentencepiece peft bitsandbytes\")\nprint(\"Dependencies installed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"STEP 2: DETECT RUNTIME & SELECT APPROACH\")\nprint(\"=\" * 60)\n\nimport torch\n\nUSE_FULL_GRPO = False  # Will be set based on GPU memory\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    \n    print(f\"âœ… GPU: {gpu_name}\")\n    print(f\"   Memory: {gpu_mem:.1f} GB\")\n    \n    # Decide approach based on memory\n    if gpu_mem >= 20:  # A100 (40GB) or L4 (24GB)\n        USE_FULL_GRPO = True\n        print(f\"\\nðŸš€ Using FULL GRPO (enough memory for 2x model copies)\")\n    else:  # T4 (16GB) or smaller\n        USE_FULL_GRPO = False\n        print(f\"\\nâš¡ Using LoRA + REINFORCE (memory-efficient)\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"âš ï¸  No GPU found! Using CPU (very slow)\")\n    USE_FULL_GRPO = False\n\nprint(f\"\\nDevice: {device}\")\nprint(f\"Full GRPO: {USE_FULL_GRPO}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup TPU Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import gc\n\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nprint(\"Memory utilities defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration based on runtime\nif USE_FULL_GRPO:\n    CONFIG = {\n        \"model_name\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n        \"num_steps\": 10,\n        \"num_generations\": 4,\n        \"learning_rate\": 5e-5,\n        \"beta\": 0.04,\n        \"max_new_tokens\": 512,\n        \"difficulty\": 5,\n        \"num_test_cases\": 5,\n        \"seed\": 42,\n    }\nelse:\n    CONFIG = {\n        \"model_name\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n        \"num_steps\": 10,\n        \"num_generations\": 2,      # Reduced for memory\n        \"learning_rate\": 1e-4,     # Higher LR for LoRA\n        \"beta\": 0.04,\n        \"max_new_tokens\": 256,     # Reduced for memory\n        \"difficulty\": 5,\n        \"num_test_cases\": 5,\n        \"seed\": 42,\n        # LoRA config\n        \"lora_r\": 16,\n        \"lora_alpha\": 32,\n        \"lora_dropout\": 0.05,\n    }\n\nprint(\"=\" * 60)\nprint(\"CONFIGURATION\")\nprint(\"=\" * 60)\nfor k, v in CONFIG.items():\n    print(f\"  {k:20} = {v}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Problem Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated problem with 5 test cases\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Any\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    input_args: List[Any]\n",
    "    expected_output: Any\n",
    "\n",
    "@dataclass \n",
    "class Problem:\n",
    "    problem_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    function_signature: str\n",
    "    function_name: str\n",
    "    test_cases: List[TestCase]\n",
    "    \n",
    "    def to_prompt(self) -> str:\n",
    "        examples = \"\\n\".join(\n",
    "            f\"  {self.function_name}({repr(tc.input_args[0])}) -> {repr(tc.expected_output)}\"\n",
    "            for tc in self.test_cases[:3]\n",
    "        )\n",
    "        return f\"\"\"## {self.title}\n",
    "\n",
    "{self.description}\n",
    "\n",
    "### Function Signature\n",
    "```python\n",
    "{self.function_signature}\n",
    "```\n",
    "\n",
    "### Examples\n",
    "```python\n",
    "{examples}\n",
    "```\n",
    "\n",
    "Write ONLY the function implementation.\"\"\"\n",
    "\n",
    "\n",
    "class TrappingRainWaterGenerator:\n",
    "    def __init__(self, seed=None):\n",
    "        self.rng = random.Random(seed)\n",
    "        self._counter = 0\n",
    "    \n",
    "    def _solve(self, height):\n",
    "        if not height: return 0\n",
    "        left, right = 0, len(height) - 1\n",
    "        left_max = right_max = water = 0\n",
    "        while left < right:\n",
    "            if height[left] < height[right]:\n",
    "                if height[left] >= left_max: left_max = height[left]\n",
    "                else: water += left_max - height[left]\n",
    "                left += 1\n",
    "            else:\n",
    "                if height[right] >= right_max: right_max = height[right]\n",
    "                else: water += right_max - height[right]\n",
    "                right -= 1\n",
    "        return water\n",
    "    \n",
    "    def generate(self, difficulty=5, num_test_cases=5):\n",
    "        self._counter += 1\n",
    "        test_cases = []\n",
    "        for _ in range(num_test_cases):\n",
    "            length = 5 + difficulty * 2\n",
    "            heights = [self.rng.randint(0, 3 + difficulty) for _ in range(length)]\n",
    "            test_cases.append(TestCase([heights], self._solve(heights)))\n",
    "        \n",
    "        return Problem(\n",
    "            problem_id=f\"trap_{self._counter}\",\n",
    "            title=\"Trapping Rain Water\",\n",
    "            description=\"Given n non-negative integers representing an elevation map, compute how much water it can trap after raining.\",\n",
    "            function_signature=\"def trap(height: list) -> int:\",\n",
    "            function_name=\"trap\",\n",
    "            test_cases=test_cases,\n",
    "        )\n",
    "\n",
    "# Test\n",
    "gen = TrappingRainWaterGenerator(seed=42)\n",
    "p = gen.generate()\n",
    "print(f\"Generated problem with {len(p.test_cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nprint(\"=\" * 60)\nprint(\"LOADING MODEL\")\nprint(\"=\" * 60)\nprint(f\"Model: {CONFIG['model_name']}\")\nprint(f\"Mode: {'Full GRPO' if USE_FULL_GRPO else 'LoRA + REINFORCE'}\")\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nprint(\"Tokenizer loaded!\")\n\nif USE_FULL_GRPO:\n    # Full precision model for A100/L4\n    model = AutoModelForCausalLM.from_pretrained(\n        CONFIG[\"model_name\"],\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n    print(f\"Full model loaded (float16)\")\nelse:\n    # QLoRA for T4/smaller\n    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n    from transformers import BitsAndBytesConfig\n    \n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n    )\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        CONFIG[\"model_name\"],\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n    )\n    model = prepare_model_for_kbit_training(model)\n    \n    lora_config = LoraConfig(\n        r=CONFIG[\"lora_r\"],\n        lora_alpha=CONFIG[\"lora_alpha\"],\n        lora_dropout=CONFIG[\"lora_dropout\"],\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    model = get_peft_model(model, lora_config)\n    print(f\"QLoRA model loaded (4-bit + LoRA adapters)\")\n\n# Stats\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"\\nTrainable: {trainable/1e6:.1f}M / {total/1e9:.2f}B ({100*trainable/total:.2f}%)\")\n\nif torch.cuda.is_available():\n    mem = torch.cuda.memory_allocated() / 1e9\n    print(f\"GPU Memory used: {mem:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\n\ndef extract_code(response):\n    for pattern in [r\"```python\\n(.*?)```\", r\"```\\n(.*?)```\"]:\n        matches = re.findall(pattern, response, re.DOTALL)\n        if matches: return matches[-1].strip()\n    if \"def \" in response:\n        lines, code, in_func = response.split(\"\\n\"), [], False\n        for line in lines:\n            if line.strip().startswith(\"def \"): in_func = True\n            if in_func: code.append(line)\n        if code: return \"\\n\".join(code).strip()\n    return response\n\ndef verify(code, problem):\n    namespace = {}\n    try:\n        exec(code, namespace)\n    except: return False, 0.0\n    \n    func = namespace.get(problem.function_name)\n    if not func:\n        funcs = [v for k, v in namespace.items() if callable(v) and not k.startswith(\"_\")]\n        func = funcs[0] if funcs else None\n    if not func: return False, 0.0\n    \n    passed = sum(1 for tc in problem.test_cases \n                 if _safe_call(func, tc.input_args) == tc.expected_output)\n    return passed == len(problem.test_cases), passed / len(problem.test_cases)\n\ndef _safe_call(func, args):\n    try: return func(*args)\n    except: return None\n\ndef generate_solution(model, tokenizer, problem, temp=0.2):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n        {\"role\": \"user\", \"content\": problem.to_prompt()},\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    \n    model.eval()\n    with torch.no_grad():\n        out = model.generate(\n            **inputs, \n            max_new_tokens=CONFIG[\"max_new_tokens\"],\n            do_sample=True, \n            temperature=temp, \n            top_p=0.9,\n            pad_token_id=tokenizer.pad_token_id\n        )\n    \n    return tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n\nprint(\"Helper functions defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE EVALUATION\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-948635322.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"difficulty\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_test_cases\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2737678221.py\u001b[0m in \u001b[0;36mgenerate_solution\u001b[0;34m(model, tokenizer, problem, temp)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         out = model.generate(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_new_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2840\u001b[0m                 \u001b[0mstreamer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2842\u001b[0;31m             \u001b[0munfinished_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munfinished_sequences\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mstopping_criteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2843\u001b[0m             \u001b[0mthis_peer_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munfinished_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2844\u001b[0m             \u001b[0mcur_len\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/stopping_criteria.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0mis_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcriteria\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mis_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_done\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/stopping_criteria.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mis_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misin_mps_friendly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36misin_mps_friendly\u001b[0;34m(elements, test_elements)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BASELINE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gen = TrappingRainWaterGenerator(seed=CONFIG[\"seed\"])\n",
    "baseline_passed = 0\n",
    "\n",
    "for i in range(5):\n",
    "    p = gen.generate(CONFIG[\"difficulty\"], CONFIG[\"num_test_cases\"])\n",
    "    response = generate_solution(model, tokenizer, p)\n",
    "    code = extract_code(response)\n",
    "    success, partial = verify(code, p)\n",
    "    print(f\"  [{i+1}] {'PASS' if success else 'FAIL'} ({partial*100:.0f}%)\")\n",
    "    baseline_passed += success\n",
    "    clear_memory()\n",
    "\n",
    "baseline_acc = baseline_passed / 5\n",
    "print(f\"\\nBaseline: {baseline_passed}/5 ({baseline_acc*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 9: Setup Optimizer (No Reference Model Needed!)\n\nWith LoRA, we skip the reference model copy. The base model weights are frozen, so we use simple REINFORCE."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import copy\n\nif USE_FULL_GRPO:\n    print(\"=\" * 60)\n    print(\"CREATING REFERENCE MODEL (Full GRPO)\")\n    print(\"=\" * 60)\n    \n    # Create frozen reference model copy\n    ref_model = copy.deepcopy(model)\n    ref_model.eval()\n    for param in ref_model.parameters():\n        param.requires_grad = False\n    print(\"Reference model created!\")\n    \n    # Optimizer for all parameters\n    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\nelse:\n    print(\"=\" * 60)\n    print(\"SETUP OPTIMIZER (LoRA - No Reference Model)\")\n    print(\"=\" * 60)\n    \n    ref_model = None  # Not needed for REINFORCE\n    print(\"Skipping reference model (using REINFORCE)\")\n    \n    # Optimizer for LoRA params only\n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=CONFIG[\"learning_rate\"]\n    )\n\nprint(f\"Optimizer: AdamW (lr={CONFIG['learning_rate']})\")\n\nif torch.cuda.is_available():\n    mem = torch.cuda.memory_allocated() / 1e9\n    print(f\"GPU Memory: {mem:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def grpo_step(model, ref_model, optimizer, problem):\n    \"\"\"Full GRPO step with reference model (for A100/L4).\"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n        {\"role\": \"user\", \"content\": problem.to_prompt()},\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    prompt_len = inputs.input_ids.shape[1]\n    \n    # Generate completions\n    completions, completion_ids = [], []\n    model.eval()\n    with torch.no_grad():\n        for _ in range(CONFIG[\"num_generations\"]):\n            out = model.generate(\n                **inputs, max_new_tokens=CONFIG[\"max_new_tokens\"],\n                do_sample=True, temperature=0.7, top_p=0.9,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n            completions.append(tokenizer.decode(out[0][prompt_len:], skip_special_tokens=True))\n            completion_ids.append(out[0].detach())\n    \n    # Compute rewards\n    rewards = []\n    for comp in completions:\n        code = extract_code(comp)\n        success, partial = verify(code, problem)\n        rewards.append(1.0 if success else partial * 0.5)\n    \n    rewards = torch.tensor(rewards, device=model.device)\n    advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n    \n    # Training\n    model.train()\n    optimizer.zero_grad()\n    total_loss = 0.0\n    \n    for comp_ids, adv in zip(completion_ids, advantages):\n        full_ids = comp_ids.unsqueeze(0)\n        \n        # Policy log probs\n        outputs = model(input_ids=full_ids)\n        logits = outputs.logits[:, prompt_len-1:-1, :]\n        target = full_ids[:, prompt_len:]\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        seq_log_prob = torch.gather(log_probs, -1, target.unsqueeze(-1)).squeeze(-1).mean()\n        \n        # Reference log probs\n        with torch.no_grad():\n            ref_out = ref_model(input_ids=full_ids)\n            ref_logits = ref_out.logits[:, prompt_len-1:-1, :]\n            ref_log_probs = torch.nn.functional.log_softmax(ref_logits, dim=-1)\n            ref_seq_log_prob = torch.gather(ref_log_probs, -1, target.unsqueeze(-1)).squeeze(-1).mean()\n        \n        # GRPO loss with KL penalty\n        kl = seq_log_prob - ref_seq_log_prob\n        loss = -adv * seq_log_prob + CONFIG[\"beta\"] * kl\n        loss.backward()\n        total_loss += loss.item()\n    \n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    optimizer.step()\n    \n    return {\"loss\": total_loss / CONFIG[\"num_generations\"], \n            \"mean_reward\": rewards.mean().item(), \"max_reward\": rewards.max().item()}\n\n\ndef reinforce_step(model, optimizer, problem):\n    \"\"\"REINFORCE step without reference model (for T4/smaller).\"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n        {\"role\": \"user\", \"content\": problem.to_prompt()},\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    prompt_len = inputs.input_ids.shape[1]\n    \n    # Generate completions\n    completions, completion_ids = [], []\n    model.eval()\n    with torch.no_grad():\n        for _ in range(CONFIG[\"num_generations\"]):\n            out = model.generate(\n                **inputs, max_new_tokens=CONFIG[\"max_new_tokens\"],\n                do_sample=True, temperature=0.7, top_p=0.9,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n            completions.append(tokenizer.decode(out[0][prompt_len:], skip_special_tokens=True))\n            completion_ids.append(out[0].detach())\n    \n    # Compute rewards\n    rewards = []\n    for comp in completions:\n        code = extract_code(comp)\n        success, partial = verify(code, problem)\n        rewards.append(1.0 if success else partial * 0.5)\n    \n    rewards = torch.tensor(rewards, device=model.device)\n    advantages = rewards - rewards.mean()\n    if rewards.std() > 0:\n        advantages = advantages / (rewards.std() + 1e-8)\n    \n    # Training\n    model.train()\n    optimizer.zero_grad()\n    total_loss = 0.0\n    \n    for comp_ids, adv in zip(completion_ids, advantages):\n        if adv.abs() < 1e-6: continue\n        \n        full_ids = comp_ids.unsqueeze(0)\n        outputs = model(input_ids=full_ids)\n        logits = outputs.logits[:, prompt_len-1:-1, :]\n        target = full_ids[:, prompt_len:]\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        seq_log_prob = torch.gather(log_probs, -1, target.unsqueeze(-1)).squeeze(-1).mean()\n        \n        loss = -adv * seq_log_prob\n        loss.backward()\n        total_loss += loss.item()\n    \n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    optimizer.step()\n    \n    return {\"loss\": total_loss / max(1, CONFIG[\"num_generations\"]),\n            \"mean_reward\": rewards.mean().item(), \"max_reward\": rewards.max().item()}\n\n\nprint(\"Training functions defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop\nprint(\"=\" * 60)\nprint(f\"TRAINING ({'Full GRPO' if USE_FULL_GRPO else 'REINFORCE'})\")\nprint(\"=\" * 60)\nprint(f\"Steps: {CONFIG['num_steps']}, Generations: {CONFIG['num_generations']}\")\nprint()\n\ntrain_gen = TrappingRainWaterGenerator(seed=CONFIG[\"seed\"] + 100)\nmetrics = []\n\nfor step in range(CONFIG[\"num_steps\"]):\n    problem = train_gen.generate(CONFIG[\"difficulty\"], CONFIG[\"num_test_cases\"])\n    \n    try:\n        if USE_FULL_GRPO:\n            m = grpo_step(model, ref_model, optimizer, problem)\n        else:\n            m = reinforce_step(model, optimizer, problem)\n        \n        metrics.append(m)\n        print(f\"Step {step+1:2d}: Loss={m['loss']:.4f}, Avg={m['mean_reward']:.3f}, Max={m['max_reward']:.3f}\")\n    except Exception as e:\n        print(f\"Step {step+1}: Error - {e}\")\n        import traceback\n        traceback.print_exc()\n    \n    clear_memory()\n\nprint(\"\\nTraining complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Post-Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"POST-TRAINING EVALUATION\")\nprint(\"=\" * 60)\n\neval_gen = TrappingRainWaterGenerator(seed=CONFIG[\"seed\"] + 999)\nfinal_passed = 0\nmodel.eval()\n\nfor i in range(5):\n    p = eval_gen.generate(CONFIG[\"difficulty\"], CONFIG[\"num_test_cases\"])\n    response = generate_solution(model, tokenizer, p, temp=0.2)\n    code = extract_code(response)\n    success, partial = verify(code, p)\n    print(f\"  [{i+1}] {'PASS' if success else 'FAIL'} ({partial*100:.0f}%)\")\n    final_passed += success\n    clear_memory()\n\nfinal_acc = final_passed / 5\nprint(f\"\\nFinal: {final_passed}/5 ({final_acc*100:.0f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nBaseline: {baseline_acc*100:.0f}%\")\n",
    "print(f\"Final:    {final_acc*100:.0f}%\")\n",
    "print(f\"Change:   {(final_acc - baseline_acc)*100:+.0f}%\")\n",
    "\n",
    "if metrics:\n",
    "    print(f\"\\nTraining curve:\")\n",
    "    for i, m in enumerate(metrics):\n",
    "        print(f\"  Step {i+1}: reward={m['mean_reward']:.3f}, max={m['max_reward']:.3f}\")\n",
    "\n",
    "if final_acc > baseline_acc:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUCCESS! Model improved!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Save Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save to Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# # Move model to CPU for saving\n",
    "# model_cpu = model.to('cpu')\n",
    "# save_path = \"/content/drive/MyDrive/axiom-rl/qwen-1.5b-grpo-tpu\"\n",
    "# model_cpu.save_pretrained(save_path)\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "# print(f\"Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "**TPU-specific considerations:**\n",
    "- Use `xm.mark_step()` to sync TPU operations\n",
    "- Use `xm.optimizer_step()` for optimizer updates\n",
    "- TPU prefers float32 over float16\n",
    "- First compilation is slow, subsequent runs are faster\n",
    "\n",
    "**If training is slow:**\n",
    "- This is normal for the first few steps (JIT compilation)\n",
    "- TPU shines on larger batch sizes\n",
    "- Consider using GPU (T4/A100) for this small-scale experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}