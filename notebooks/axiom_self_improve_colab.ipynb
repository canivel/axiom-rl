{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Axiom-RL: Self-Improving Code Generation\n",
    "\n",
    "This notebook implements a complete self-improvement loop for code generation using Expert Iteration:\n",
    "\n",
    "1. **Generate** - Model attempts to solve problems\n",
    "2. **Verify** - Solutions are tested in a sandbox\n",
    "3. **Train** - Model is fine-tuned on correct solutions using LoRA\n",
    "4. **Repeat** - Iterate to continuously improve\n",
    "\n",
    "All code is self-contained - no external imports required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Research\\axiom-rl\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n",
      "✓ PyTorch version: 2.5.1+cu121\n",
      "✓ CUDA available: True\n",
      "✓ GPU: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"✓ Libraries imported\")\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Problem Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example Problem ===\n",
      "ID: fibonacci_example\n",
      "Type: fibonacci\n",
      "Description: Write a function that returns the nth Fibonacci number. F(0)=0, F(1)=1, F(n)=F(n-1)+F(n-2).\n",
      "Signature: def fibonacci(n: int) -> int:\n",
      "\n",
      "Test Cases (4 total):\n",
      "  1. Input: {'n': 0} → Expected: 0\n",
      "  2. Input: {'n': 1} → Expected: 1\n",
      "  3. Input: {'n': 5} → Expected: 5\n",
      "  4. Input: {'n': 10} → Expected: 55\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# These are the core data structures from axiom/core/data_structures.py\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"A single test case for a problem.\n",
    "    \n",
    "    Think of this as one example that the generated code must pass.\n",
    "    Example: For a fibonacci function, a test case might be:\n",
    "        input: {\"n\": 5}\n",
    "        expected_output: 5\n",
    "    \"\"\"\n",
    "    input: Dict[str, Any]      # The arguments to pass to the function\n",
    "    expected_output: Any        # What the function should return\n",
    "\n",
    "@dataclass  \n",
    "class Problem:\n",
    "    \"\"\"A programming problem that the model must solve.\n",
    "    \n",
    "    This is what we ask the model to generate code for.\n",
    "    \"\"\"\n",
    "    problem_id: str            # Unique identifier (e.g., \"fibonacci_v2_001\")\n",
    "    problem_type: str          # Category (e.g., \"fibonacci\", \"fizzbuzz\")\n",
    "    description: str           # Natural language description\n",
    "    function_signature: str    # The function signature to implement\n",
    "    test_cases: List[TestCase] # List of test cases to verify correctness\n",
    "\n",
    "# Let's create an example problem manually\n",
    "example_problem = Problem(\n",
    "    problem_id=\"fibonacci_example\",\n",
    "    problem_type=\"fibonacci\",\n",
    "    description=\"Write a function that returns the nth Fibonacci number. F(0)=0, F(1)=1, F(n)=F(n-1)+F(n-2).\",\n",
    "    function_signature=\"def fibonacci(n: int) -> int:\",\n",
    "    test_cases=[\n",
    "        TestCase(input={\"n\": 0}, expected_output=0),\n",
    "        TestCase(input={\"n\": 1}, expected_output=1),\n",
    "        TestCase(input={\"n\": 5}, expected_output=5),\n",
    "        TestCase(input={\"n\": 10}, expected_output=55),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== Example Problem ===\")\n",
    "print(f\"ID: {example_problem.problem_id}\")\n",
    "print(f\"Type: {example_problem.problem_type}\")\n",
    "print(f\"Description: {example_problem.description}\")\n",
    "print(f\"Signature: {example_problem.function_signature}\")\n",
    "print(f\"\\nTest Cases ({len(example_problem.test_cases)} total):\")\n",
    "for i, tc in enumerate(example_problem.test_cases):\n",
    "    print(f\"  {i+1}. Input: {tc.input} → Expected: {tc.expected_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Procedural Problem Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PROCEDURAL PROBLEM GENERATORS\n",
    "# ============================================================================\n",
    "\n",
    "class ProblemGenerator(ABC):\n",
    "    \"\"\"Abstract base class for generators.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def generate(self, difficulty: int = 1) -> ProceduralProblem:\n",
    "        pass\n",
    "    \n",
    "    def generate_batch(self, n: int, difficulty: int = 1) -> List[ProceduralProblem]:\n",
    "        return [self.generate(difficulty) for _ in range(n)]\n",
    "\n",
    "\n",
    "class ArithmeticGenerator(ProblemGenerator):\n",
    "    \"\"\"Generates arithmetic expression problems.\"\"\"\n",
    "    \n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        self.rng = random.Random(seed)\n",
    "        self.ops = ['+', '-', '*']\n",
    "        \n",
    "    def generate(self, difficulty: int = 1) -> ProceduralProblem:\n",
    "        expr = self._generate_expression(difficulty)\n",
    "        result = eval(expr)\n",
    "        \n",
    "        description = f\"\"\"Write a Python function `solve()` that returns the result of the following arithmetic expression:\n",
    "\n",
    "{expr}\n",
    "\n",
    "The function should take no arguments and return an integer.\"\"\"\n",
    "        \n",
    "        solution_code = f\"def solve():\\n    return {result}\"\n",
    "        pid = f\"arithmetic_{abs(hash(expr)) % 10**10}\"\n",
    "        \n",
    "        return ProceduralProblem(\n",
    "            id=pid,\n",
    "            title=f\"Arithmetic: {expr}\",\n",
    "            description=description,\n",
    "            difficulty=difficulty,\n",
    "            solution_code=solution_code,\n",
    "            test_cases=[{\"input\": [], \"output\": result}],\n",
    "            function_signature=\"def solve():\",\n",
    "            problem_type=\"arithmetic\",\n",
    "        )\n",
    "\n",
    "    def _generate_expression(self, depth: int) -> str:\n",
    "        if depth == 1:\n",
    "            return f\"{self.rng.randint(1, 20)} {self.rng.choice(self.ops)} {self.rng.randint(1, 20)}\"\n",
    "            \n",
    "        left = self._generate_expression(depth - 1) if self.rng.random() > 0.5 else str(self.rng.randint(1, 20))\n",
    "        right = self._generate_expression(depth - 1) if self.rng.random() > 0.5 else str(self.rng.randint(1, 20))\n",
    "        op = self.rng.choice(self.ops)\n",
    "        \n",
    "        if self.rng.random() > 0.5:\n",
    "            return f\"({left} {op} {right})\"\n",
    "        return f\"{left} {op} {right}\"\n",
    "\n",
    "\n",
    "class RPNGenerator(ProblemGenerator):\n",
    "    \"\"\"Generates Reverse Polish Notation evaluation problems.\"\"\"\n",
    "    \n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        self.rng = random.Random(seed)\n",
    "        self.ops = {\n",
    "            '+': lambda x, y: x + y,\n",
    "            '-': lambda x, y: x - y,\n",
    "            '*': lambda x, y: x * y\n",
    "        }\n",
    "        \n",
    "    def generate(self, difficulty: int = 1) -> ProceduralProblem:\n",
    "        length = 3 + difficulty * 2\n",
    "        expression, result = self._generate_rpn(length)\n",
    "        \n",
    "        description = f\"\"\"Write a Python function `solve()` that evaluates the following Reverse Polish Notation (RPN) expression and returns the result:\n",
    "\n",
    "Expression: \"{expression}\"\n",
    "\n",
    "In RPN, operators come after their operands. For example:\n",
    "- \"3 4 +\" means 3 + 4 = 7\n",
    "- \"3 4 + 5 *\" means (3 + 4) * 5 = 35\n",
    "\n",
    "The function should take no arguments and return the integer result.\"\"\"\n",
    "        \n",
    "        solution_code = f\"def solve():\\n    # RPN: {expression}\\n    return {result}\"\n",
    "        pid = f\"rpn_{abs(hash(expression)) % 10**10}\"\n",
    "        \n",
    "        return ProceduralProblem(\n",
    "            id=pid,\n",
    "            title=f\"RPN: {expression}\",\n",
    "            description=description,\n",
    "            difficulty=difficulty,\n",
    "            solution_code=solution_code,\n",
    "            test_cases=[{\"input\": [], \"output\": result}],\n",
    "            function_signature=\"def solve():\",\n",
    "            problem_type=\"rpn\",\n",
    "        )\n",
    "\n",
    "    def _generate_rpn(self, target_length: int):\n",
    "        stack = []\n",
    "        expression = []\n",
    "        current_depth = 0\n",
    "        \n",
    "        while len(expression) < target_length or current_depth > 1:\n",
    "            if current_depth < 2:\n",
    "                num = self.rng.randint(1, 9)\n",
    "                stack.append(num)\n",
    "                expression.append(str(num))\n",
    "                current_depth += 1\n",
    "            elif len(expression) >= target_length:\n",
    "                op_sym = self.rng.choice(list(self.ops.keys()))\n",
    "                b = stack.pop()\n",
    "                a = stack.pop()\n",
    "                res = self.ops[op_sym](a, b)\n",
    "                stack.append(res)\n",
    "                expression.append(op_sym)\n",
    "                current_depth -= 1\n",
    "            else:\n",
    "                if self.rng.random() > 0.6:\n",
    "                    num = self.rng.randint(1, 9)\n",
    "                    stack.append(num)\n",
    "                    expression.append(str(num))\n",
    "                    current_depth += 1\n",
    "                else:\n",
    "                    op_sym = self.rng.choice(list(self.ops.keys()))\n",
    "                    b = stack.pop()\n",
    "                    a = stack.pop()\n",
    "                    res = self.ops[op_sym](a, b)\n",
    "                    stack.append(res)\n",
    "                    expression.append(op_sym)\n",
    "                    current_depth -= 1\n",
    "                    \n",
    "        return \" \".join(expression), stack[0]\n",
    "\n",
    "\n",
    "class ParenthesesGenerator(ProblemGenerator):\n",
    "    \"\"\"Generates parentheses validation problems.\"\"\"\n",
    "    \n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        self.rng = random.Random(seed)\n",
    "        self.bracket_pairs = [(\"(\", \")\"), (\"[\", \"]\"), (\"{\", \"}\")]\n",
    "        \n",
    "    def generate(self, difficulty: int = 5) -> ProceduralProblem:\n",
    "        # Determine parameters based on difficulty\n",
    "        if difficulty <= 3:\n",
    "            length = self.rng.randint(4, 10)\n",
    "            num_types = 1\n",
    "            valid_prob = 0.6\n",
    "        elif difficulty <= 6:\n",
    "            length = self.rng.randint(8, 18)\n",
    "            num_types = 2\n",
    "            valid_prob = 0.5\n",
    "        else:\n",
    "            length = self.rng.randint(14, 28)\n",
    "            num_types = 3\n",
    "            valid_prob = 0.4\n",
    "        \n",
    "        pairs = self.bracket_pairs[:num_types]\n",
    "        generate_valid = self.rng.random() < valid_prob\n",
    "        \n",
    "        if generate_valid:\n",
    "            s = self._generate_valid(length, pairs)\n",
    "            expected = True\n",
    "        else:\n",
    "            s = self._generate_invalid(length, pairs)\n",
    "            expected = False\n",
    "        \n",
    "        description = f\"\"\"Determine if the following string containing brackets is valid.\n",
    "\n",
    "A string is valid if:\n",
    "1. Open brackets must be closed by the same type of brackets\n",
    "2. Open brackets must be closed in the correct order\n",
    "3. Every close bracket has a corresponding open bracket\n",
    "\n",
    "Input string: \"{s}\"\n",
    "\n",
    "Write a Python function `solve()` that returns True if valid, False otherwise.\"\"\"\n",
    "        \n",
    "        solution_code = f\"def solve():\\n    return {expected}\"\n",
    "        pid = f\"parens_{abs(hash(s)) % 10**10}\"\n",
    "        \n",
    "        return ProceduralProblem(\n",
    "            id=pid,\n",
    "            title=f\"Parentheses: {s[:20]}{'...' if len(s) > 20 else ''}\",\n",
    "            description=description,\n",
    "            difficulty=difficulty,\n",
    "            solution_code=solution_code,\n",
    "            test_cases=[{\"input\": [], \"output\": expected}],\n",
    "            function_signature=\"def solve():\",\n",
    "            problem_type=\"parentheses\",\n",
    "        )\n",
    "\n",
    "    def _generate_valid(self, target_length: int, pairs: List[Tuple[str, str]]) -> str:\n",
    "        result = []\n",
    "        stack = []\n",
    "        \n",
    "        while len(result) < target_length:\n",
    "            remaining = target_length - len(result)\n",
    "            can_open = remaining >= 2\n",
    "            can_close = len(stack) > 0\n",
    "            \n",
    "            if can_open and can_close:\n",
    "                if len(stack) >= remaining // 2:\n",
    "                    action = \"close\"\n",
    "                else:\n",
    "                    action = self.rng.choice([\"open\", \"close\"])\n",
    "            elif can_open:\n",
    "                action = \"open\"\n",
    "            elif can_close:\n",
    "                action = \"close\"\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "            if action == \"open\":\n",
    "                pair = self.rng.choice(pairs)\n",
    "                result.append(pair[0])\n",
    "                stack.append(pair[1])\n",
    "            else:\n",
    "                result.append(stack.pop())\n",
    "        \n",
    "        while stack:\n",
    "            result.append(stack.pop())\n",
    "        \n",
    "        return \"\".join(result)\n",
    "\n",
    "    def _generate_invalid(self, target_length: int, pairs: List[Tuple[str, str]]) -> str:\n",
    "        # Simple strategy: generate valid-ish then corrupt\n",
    "        result = []\n",
    "        for _ in range((target_length - 1) // 2):\n",
    "            pair = self.rng.choice(pairs)\n",
    "            result.append(pair[0])\n",
    "            result.append(pair[1])\n",
    "        \n",
    "        # Add corruption\n",
    "        corruption = self.rng.choice([\"extra_close\", \"unclosed\", \"mismatch\"])\n",
    "        if corruption == \"extra_close\":\n",
    "            result.insert(0, self.rng.choice(pairs)[1])\n",
    "        elif corruption == \"unclosed\":\n",
    "            result.append(self.rng.choice(pairs)[0])\n",
    "        else:  # mismatch\n",
    "            if len(pairs) > 1 and len(result) > 1:\n",
    "                idx = self.rng.randint(0, len(result) - 1)\n",
    "                if result[idx] in [p[1] for p in pairs]:\n",
    "                    other = [p[1] for p in pairs if p[1] != result[idx]]\n",
    "                    if other:\n",
    "                        result[idx] = self.rng.choice(other)\n",
    "        \n",
    "        return \"\".join(result)[:target_length] if len(result) > target_length else \"\".join(result)\n",
    "\n",
    "\n",
    "# Test generators\n",
    "print(\"Testing generators...\")\n",
    "arith = ArithmeticGenerator(seed=42)\n",
    "rpn = RPNGenerator(seed=42)\n",
    "parens = ParenthesesGenerator(seed=42)\n",
    "\n",
    "p1 = arith.generate(difficulty=3)\n",
    "print(f\"\\nArithmetic: {p1.title}\")\n",
    "print(f\"  Expected: {p1.test_cases[0]['output']}\")\n",
    "\n",
    "p2 = rpn.generate(difficulty=3)\n",
    "print(f\"\\nRPN: {p2.title}\")\n",
    "print(f\"  Expected: {p2.test_cases[0]['output']}\")\n",
    "\n",
    "p3 = parens.generate(difficulty=5)\n",
    "print(f\"\\nParentheses: {p3.title}\")\n",
    "print(f\"  Expected: {p3.test_cases[0]['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Code Verifier (Sandbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VERIFICATION SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "class VerificationStatus(Enum):\n",
    "    PASSED = \"passed\"\n",
    "    FAILED = \"failed\"\n",
    "    ERROR = \"error\"\n",
    "    TIMEOUT = \"timeout\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VerificationResult:\n",
    "    status: VerificationStatus\n",
    "    passed_count: int\n",
    "    total_count: int\n",
    "    error_message: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def passed(self) -> bool:\n",
    "        return self.status == VerificationStatus.PASSED\n",
    "\n",
    "\n",
    "class PythonSandbox:\n",
    "    \"\"\"Sandboxed Python execution using subprocess.\"\"\"\n",
    "    \n",
    "    def __init__(self, timeout: float = 5.0):\n",
    "        self.timeout = timeout\n",
    "    \n",
    "    def execute(self, code: str) -> Tuple[str, str, int, bool]:\n",
    "        \"\"\"Execute code and return (stdout, stderr, returncode, timed_out).\"\"\"\n",
    "        fd, temp_path = tempfile.mkstemp(suffix=\".py\", prefix=\"axiom_\")\n",
    "        try:\n",
    "            with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(code)\n",
    "            \n",
    "            result = subprocess.run(\n",
    "                [\"python\", temp_path],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=self.timeout,\n",
    "                cwd=tempfile.gettempdir(),\n",
    "            )\n",
    "            return result.stdout, result.stderr, result.returncode, False\n",
    "        except subprocess.TimeoutExpired:\n",
    "            return \"\", f\"Timeout after {self.timeout}s\", -1, True\n",
    "        except Exception as e:\n",
    "            return \"\", str(e), -1, False\n",
    "        finally:\n",
    "            try:\n",
    "                os.unlink(temp_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "class TestHarness:\n",
    "    \"\"\"Runs generated code against test cases.\"\"\"\n",
    "    \n",
    "    def __init__(self, timeout: float = 5.0):\n",
    "        self.sandbox = PythonSandbox(timeout=timeout)\n",
    "    \n",
    "    def verify(self, solution_code: str, problem: Problem) -> VerificationResult:\n",
    "        \"\"\"Verify a solution against all test cases.\"\"\"\n",
    "        test_script = self._build_test_script(solution_code, problem)\n",
    "        stdout, stderr, returncode, timed_out = self.sandbox.execute(test_script)\n",
    "        \n",
    "        if timed_out:\n",
    "            return VerificationResult(\n",
    "                status=VerificationStatus.TIMEOUT,\n",
    "                passed_count=0,\n",
    "                total_count=len(problem.test_cases),\n",
    "                error_message=stderr,\n",
    "            )\n",
    "        \n",
    "        if returncode != 0 and not stdout.strip():\n",
    "            return VerificationResult(\n",
    "                status=VerificationStatus.ERROR,\n",
    "                passed_count=0,\n",
    "                total_count=len(problem.test_cases),\n",
    "                error_message=stderr or \"Unknown error\",\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(stdout)\n",
    "            if not data.get(\"success\", False) and \"error\" in data:\n",
    "                return VerificationResult(\n",
    "                    status=VerificationStatus.ERROR,\n",
    "                    passed_count=0,\n",
    "                    total_count=len(problem.test_cases),\n",
    "                    error_message=data[\"error\"],\n",
    "                )\n",
    "            \n",
    "            passed_count = sum(1 for r in data[\"results\"] if r[\"passed\"])\n",
    "            total_count = len(data[\"results\"])\n",
    "            \n",
    "            return VerificationResult(\n",
    "                status=VerificationStatus.PASSED if passed_count == total_count else VerificationStatus.FAILED,\n",
    "                passed_count=passed_count,\n",
    "                total_count=total_count,\n",
    "            )\n",
    "        except json.JSONDecodeError as e:\n",
    "            return VerificationResult(\n",
    "                status=VerificationStatus.ERROR,\n",
    "                passed_count=0,\n",
    "                total_count=len(problem.test_cases),\n",
    "                error_message=f\"Parse error: {stderr or stdout or str(e)}\",\n",
    "            )\n",
    "    \n",
    "    def _build_test_script(self, solution_code: str, problem: Problem) -> str:\n",
    "        test_cases_json = json.dumps([\n",
    "            {\"input\": tc.input, \"expected\": tc.expected_output}\n",
    "            for tc in problem.test_cases\n",
    "        ])\n",
    "        func_name = problem.function_name\n",
    "        \n",
    "        return f'''# -*- coding: utf-8 -*-\n",
    "import json\n",
    "from typing import List, Optional, Tuple, Dict, Any, Set\n",
    "\n",
    "# === SOLUTION CODE ===\n",
    "{solution_code}\n",
    "# === END SOLUTION ===\n",
    "\n",
    "def run_tests():\n",
    "    test_cases = json.loads(\\'{test_cases_json}\\')\n",
    "    results = []\n",
    "    \n",
    "    for i, tc in enumerate(test_cases):\n",
    "        inp = tc[\"input\"]\n",
    "        expected = tc[\"expected\"]\n",
    "        \n",
    "        try:\n",
    "            if isinstance(inp, list):\n",
    "                actual = {func_name}(*inp)\n",
    "            else:\n",
    "                actual = {func_name}(inp)\n",
    "            \n",
    "            passed = actual == expected\n",
    "            results.append({{\"index\": i, \"passed\": passed, \"actual\": actual, \"expected\": expected}})\n",
    "        except Exception as e:\n",
    "            results.append({{\"index\": i, \"passed\": False, \"error\": str(e)}})\n",
    "    \n",
    "    print(json.dumps({{\"results\": results, \"success\": True}}))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_tests()\n",
    "    except Exception as e:\n",
    "        print(json.dumps({{\"results\": [], \"success\": False, \"error\": str(e)}}))\n",
    "'''\n",
    "\n",
    "\n",
    "# Test the verifier\n",
    "print(\"Testing verifier...\")\n",
    "harness = TestHarness(timeout=5.0)\n",
    "\n",
    "test_problem = arith.generate(difficulty=2).to_problem()\n",
    "correct_code = \"def solve():\\n    return \" + str(test_problem.test_cases[0].expected_output)\n",
    "wrong_code = \"def solve():\\n    return 999999\"\n",
    "\n",
    "result1 = harness.verify(correct_code, test_problem)\n",
    "result2 = harness.verify(wrong_code, test_problem)\n",
    "\n",
    "print(f\"Correct solution: {result1.status.value}\")\n",
    "print(f\"Wrong solution: {result2.status.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Code Generator (Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CODE GENERATOR\n",
    "# ============================================================================\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert Python programmer. Your task is to solve algorithmic problems by writing clean, efficient, and correct Python code.\n",
    "\n",
    "Rules:\n",
    "1. Write ONLY the function implementation - no explanations, no test code\n",
    "2. The function signature is provided - implement the function body\n",
    "3. Use standard Python libraries only\n",
    "4. Write clear, readable code\n",
    "5. Handle edge cases appropriately\n",
    "6. Return the result as specified\"\"\"\n",
    "\n",
    "\n",
    "def build_user_prompt(problem: Problem) -> str:\n",
    "    return f\"\"\"Solve the following problem by implementing the function.\n",
    "\n",
    "## Problem: {problem.title}\n",
    "\n",
    "{problem.description}\n",
    "\n",
    "## Function Signature\n",
    "```python\n",
    "{problem.function_signature}\n",
    "    # Your implementation here\n",
    "```\n",
    "\n",
    "Write ONLY the complete function implementation. Do not include any explanations, examples, or test code.\"\"\"\n",
    "\n",
    "\n",
    "def build_messages(problem: Problem) -> List[dict]:\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": build_user_prompt(problem)},\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_code_from_response(response: str) -> str:\n",
    "    \"\"\"Extract Python code from model response.\"\"\"\n",
    "    # Try ```python blocks\n",
    "    matches = re.findall(r\"```python\\s*(.*?)```\", response, re.DOTALL)\n",
    "    if matches:\n",
    "        return matches[0].strip()\n",
    "    \n",
    "    # Try generic ``` blocks\n",
    "    matches = re.findall(r\"```\\s*(.*?)```\", response, re.DOTALL)\n",
    "    if matches:\n",
    "        code = matches[0].strip()\n",
    "        lines = code.split(\"\\n\")\n",
    "        if lines and lines[0].strip().lower() in [\"python\", \"py\", \"\"]:\n",
    "            code = \"\\n\".join(lines[1:])\n",
    "        return code.strip()\n",
    "    \n",
    "    # Return as-is\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "class CodeGenerator:\n",
    "    \"\"\"HuggingFace-based code generator.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "        device: str = \"auto\",\n",
    "        temperature: float = 0.7,\n",
    "        max_new_tokens: int = 512,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.temperature = temperature\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._loaded = False\n",
    "    \n",
    "    def load(self):\n",
    "        if self._loaded:\n",
    "            return\n",
    "        \n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=self.device,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        \n",
    "        self._loaded = True\n",
    "        device = next(self.model.parameters()).device\n",
    "        print(f\"Model loaded on: {device}\")\n",
    "    \n",
    "    def generate(self, messages: List[dict], temperature: Optional[float] = None) -> str:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        \n",
    "        temp = temperature if temperature is not None else self.temperature\n",
    "        \n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        \n",
    "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "        input_length = model_inputs.input_ids.shape[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                temperature=temp,\n",
    "                top_p=0.95,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        new_tokens = generated_ids[0][input_length:]\n",
    "        return self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    def generate_solution(self, problem: Problem) -> str:\n",
    "        \"\"\"Generate a solution for the given problem.\"\"\"\n",
    "        messages = build_messages(problem)\n",
    "        response = self.generate(messages)\n",
    "        return extract_code_from_response(response)\n",
    "\n",
    "\n",
    "print(\"CodeGenerator class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training Dataset & LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING DATASET & LORA CONFIG\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TrainingSample:\n",
    "    \"\"\"A single training sample.\"\"\"\n",
    "    problem_id: str\n",
    "    problem_title: str\n",
    "    problem_description: str\n",
    "    function_signature: str\n",
    "    solution_code: str\n",
    "    model_name: str = \"student\"\n",
    "    \n",
    "    def to_prompt_completion(self) -> dict:\n",
    "        user_prompt = f\"\"\"Solve the following problem by implementing the function.\n",
    "\n",
    "## Problem: {self.problem_title}\n",
    "\n",
    "{self.problem_description}\n",
    "\n",
    "## Function Signature\n",
    "```python\n",
    "{self.function_signature}\n",
    "    # Your implementation here\n",
    "```\n",
    "\n",
    "Write ONLY the complete function implementation.\"\"\"\n",
    "        \n",
    "        completion = f\"```python\\n{self.solution_code}\\n```\"\n",
    "        return {\"prompt\": user_prompt, \"completion\": completion}\n",
    "\n",
    "\n",
    "class SFTDataset(Dataset):\n",
    "    \"\"\"Dataset for SFT training.\"\"\"\n",
    "    \n",
    "    def __init__(self, samples: List[TrainingSample], tokenizer, max_length: int = 1024):\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.processed = [self._process(s) for s in samples]\n",
    "    \n",
    "    def _process(self, sample: TrainingSample) -> dict:\n",
    "        pc = sample.to_prompt_completion()\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": pc[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": pc[\"completion\"]},\n",
    "        ]\n",
    "        \n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": encodings[\"input_ids\"].squeeze(0).clone(),\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.processed)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed[idx]\n",
    "\n",
    "\n",
    "def get_lora_config(\n",
    "    r: int = 16,\n",
    "    alpha: int = 32,\n",
    "    dropout: float = 0.05,\n",
    ") -> LoraConfig:\n",
    "    \"\"\"Create LoRA config for training.\"\"\"\n",
    "    return LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Training dataset and LoRA config defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Self-Improvement Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SELF-IMPROVEMENT EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SelfImproveConfig:\n",
    "    \"\"\"Configuration for self-improvement experiment.\"\"\"\n",
    "    # Model\n",
    "    base_model: str = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "    \n",
    "    # Problems\n",
    "    problem_types: List[str] = field(default_factory=lambda: [\"arithmetic\", \"rpn\", \"parentheses\"])\n",
    "    train_size: int = 50\n",
    "    val_size: int = 10\n",
    "    test_size: int = 10\n",
    "    difficulty_min: int = 3\n",
    "    difficulty_max: int = 7\n",
    "    seed: int = 42\n",
    "    \n",
    "    # Training\n",
    "    num_iterations: int = 3\n",
    "    learning_rate: float = 1e-4\n",
    "    epochs_per_iteration: int = 1\n",
    "    batch_size: int = 1\n",
    "    gradient_accumulation: int = 4\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    \n",
    "    # Generation\n",
    "    temperature: float = 0.7\n",
    "    max_new_tokens: int = 512\n",
    "\n",
    "\n",
    "class SelfImproveExperiment:\n",
    "    \"\"\"Self-improvement experiment using Expert Iteration.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: SelfImproveConfig):\n",
    "        self.config = config\n",
    "        self.rng = random.Random(config.seed)\n",
    "        \n",
    "        # Initialize generators\n",
    "        self.generators = {\n",
    "            \"arithmetic\": ArithmeticGenerator(seed=config.seed),\n",
    "            \"rpn\": RPNGenerator(seed=config.seed),\n",
    "            \"parentheses\": ParenthesesGenerator(seed=config.seed),\n",
    "        }\n",
    "        \n",
    "        # Components\n",
    "        self.harness = TestHarness(timeout=5.0)\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.peft_model = None\n",
    "        \n",
    "        # History\n",
    "        self.metrics_history = []\n",
    "        self.historical_solutions = []\n",
    "    \n",
    "    def generate_problems(self, n: int) -> List[Problem]:\n",
    "        \"\"\"Generate n problems with mixed types and difficulties.\"\"\"\n",
    "        problems = []\n",
    "        types = self.config.problem_types\n",
    "        \n",
    "        for _ in range(n):\n",
    "            ptype = self.rng.choice(types)\n",
    "            difficulty = self.rng.randint(self.config.difficulty_min, self.config.difficulty_max)\n",
    "            proc_problem = self.generators[ptype].generate(difficulty=difficulty)\n",
    "            problems.append(proc_problem.to_problem())\n",
    "        \n",
    "        return problems\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load base model and tokenizer.\"\"\"\n",
    "        print(f\"Loading model: {self.config.base_model}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config.base_model,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.base_model,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        \n",
    "        device = next(self.model.parameters()).device\n",
    "        print(f\"Model loaded on: {device}\")\n",
    "    \n",
    "    def generate_solution(self, problem: Problem) -> str:\n",
    "        \"\"\"Generate a solution for the problem.\"\"\"\n",
    "        model = self.peft_model if self.peft_model else self.model\n",
    "        model.eval()\n",
    "        \n",
    "        messages = build_messages(problem)\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        \n",
    "        inputs = self.tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        input_length = inputs.input_ids.shape[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "                temperature=self.config.temperature,\n",
    "                top_p=0.95,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        new_tokens = outputs[0][input_length:]\n",
    "        response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "        return extract_code_from_response(response)\n",
    "    \n",
    "    def evaluate(self, problems: List[Problem], desc: str = \"Evaluating\") -> Tuple[int, int]:\n",
    "        \"\"\"Evaluate model on problems. Returns (correct, total).\"\"\"\n",
    "        correct = 0\n",
    "        total = len(problems)\n",
    "        \n",
    "        for i, problem in enumerate(tqdm(problems, desc=desc)):\n",
    "            code = self.generate_solution(problem)\n",
    "            result = self.harness.verify(code, problem)\n",
    "            if result.passed:\n",
    "                correct += 1\n",
    "        \n",
    "        return correct, total\n",
    "    \n",
    "    def collect_solutions(self, problems: List[Problem]) -> List[dict]:\n",
    "        \"\"\"Generate and verify solutions, return successful ones.\"\"\"\n",
    "        solutions = []\n",
    "        \n",
    "        for problem in tqdm(problems, desc=\"Collecting solutions\"):\n",
    "            code = self.generate_solution(problem)\n",
    "            result = self.harness.verify(code, problem)\n",
    "            \n",
    "            if result.passed:\n",
    "                solutions.append({\n",
    "                    \"problem_id\": problem.id,\n",
    "                    \"problem_title\": problem.title,\n",
    "                    \"problem_description\": problem.description,\n",
    "                    \"function_signature\": problem.function_signature,\n",
    "                    \"solution_code\": code,\n",
    "                    \"model_name\": \"student\",\n",
    "                })\n",
    "        \n",
    "        return solutions\n",
    "    \n",
    "    def train_on_solutions(self, solutions: List[dict], iteration: int):\n",
    "        \"\"\"Train on collected solutions using LoRA.\"\"\"\n",
    "        if not solutions:\n",
    "            print(\"  No solutions to train on, skipping\")\n",
    "            return\n",
    "        \n",
    "        print(f\"  Training on {len(solutions)} solutions...\")\n",
    "        \n",
    "        # Add to historical buffer (for future iterations)\n",
    "        self.historical_solutions.extend(solutions)\n",
    "        \n",
    "        # Mix with historical solutions (replay buffer)\n",
    "        if len(self.historical_solutions) > len(solutions):\n",
    "            n_historical = min(len(solutions), len(self.historical_solutions) - len(solutions))\n",
    "            historical_sample = self.rng.sample(\n",
    "                self.historical_solutions[:-len(solutions)],  # Exclude current\n",
    "                n_historical\n",
    "            )\n",
    "            all_solutions = solutions + historical_sample\n",
    "            print(f\"    Mixed: {len(solutions)} new + {len(historical_sample)} historical\")\n",
    "        else:\n",
    "            all_solutions = solutions\n",
    "        \n",
    "        # Create training samples\n",
    "        samples = [\n",
    "            TrainingSample(\n",
    "                problem_id=s[\"problem_id\"],\n",
    "                problem_title=s[\"problem_title\"],\n",
    "                problem_description=s[\"problem_description\"],\n",
    "                function_signature=s[\"function_signature\"],\n",
    "                solution_code=s[\"solution_code\"],\n",
    "                model_name=s[\"model_name\"],\n",
    "            )\n",
    "            for s in all_solutions\n",
    "        ]\n",
    "        \n",
    "        dataset = SFTDataset(samples, self.tokenizer, max_length=1024)\n",
    "        \n",
    "        # Setup LoRA if first iteration\n",
    "        if self.peft_model is None:\n",
    "            lora_config = get_lora_config(\n",
    "                r=self.config.lora_r,\n",
    "                alpha=self.config.lora_alpha,\n",
    "            )\n",
    "            self.peft_model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        self.peft_model.train()\n",
    "        \n",
    "        # Training args\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./checkpoints/iter_{iteration}\",\n",
    "            num_train_epochs=self.config.epochs_per_iteration,\n",
    "            per_device_train_batch_size=self.config.batch_size,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            warmup_ratio=0.1,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"no\",\n",
    "            report_to=\"none\",\n",
    "            remove_unused_columns=False,\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.peft_model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        # Merge LoRA weights for faster inference\n",
    "        print(\"  Merging LoRA weights...\")\n",
    "        self.peft_model = self.peft_model.merge_and_unload()\n",
    "        \n",
    "        # Re-wrap for next iteration\n",
    "        lora_config = get_lora_config(\n",
    "            r=self.config.lora_r,\n",
    "            alpha=self.config.lora_alpha,\n",
    "        )\n",
    "        self.peft_model = get_peft_model(self.peft_model, lora_config)\n",
    "        \n",
    "        print(\"  Training complete.\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the self-improvement loop.\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"SELF-IMPROVEMENT EXPERIMENT\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Model: {self.config.base_model}\")\n",
    "        print(f\"Problems: {self.config.train_size} train, {self.config.val_size} val, {self.config.test_size} test\")\n",
    "        print(f\"Iterations: {self.config.num_iterations}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Generate fixed problem sets\n",
    "        print(\"\\nGenerating problems...\")\n",
    "        train_problems = self.generate_problems(self.config.train_size)\n",
    "        val_problems = self.generate_problems(self.config.val_size)\n",
    "        test_problems = self.generate_problems(self.config.test_size)\n",
    "        print(f\"  Train: {len(train_problems)}, Val: {len(val_problems)}, Test: {len(test_problems)}\")\n",
    "        \n",
    "        # Load model\n",
    "        self.load_model()\n",
    "        \n",
    "        # Run iterations\n",
    "        for iteration in range(self.config.num_iterations):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ITERATION {iteration}\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Evaluate\n",
    "            train_correct, train_total = self.evaluate(train_problems, \"Train\")\n",
    "            val_correct, val_total = self.evaluate(val_problems, \"Val\")\n",
    "            test_correct, test_total = self.evaluate(test_problems, \"Test\")\n",
    "            \n",
    "            train_acc = train_correct / train_total\n",
    "            val_acc = val_correct / val_total\n",
    "            test_acc = test_correct / test_total\n",
    "            \n",
    "            print(f\"\\n  Results:\")\n",
    "            print(f\"    Train: {train_acc:.1%} ({train_correct}/{train_total})\")\n",
    "            print(f\"    Val:   {val_acc:.1%} ({val_correct}/{val_total})\")\n",
    "            print(f\"    Test:  {test_acc:.1%} ({test_correct}/{test_total})\")\n",
    "            \n",
    "            self.metrics_history.append({\n",
    "                \"iteration\": iteration,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "            })\n",
    "            \n",
    "            # Collect solutions\n",
    "            print(f\"\\n  Collecting solutions...\")\n",
    "            solutions = self.collect_solutions(train_problems)\n",
    "            print(f\"    Collected {len(solutions)} correct solutions\")\n",
    "            \n",
    "            # Train\n",
    "            if solutions:\n",
    "                self.train_on_solutions(solutions, iteration)\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"FINAL SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        for m in self.metrics_history:\n",
    "            print(f\"Iter {m['iteration']}: Train {m['train_acc']:.1%}, Val {m['val_acc']:.1%}, Test {m['test_acc']:.1%}\")\n",
    "        \n",
    "        return self.metrics_history\n",
    "\n",
    "\n",
    "print(\"SelfImproveExperiment class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Run the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "# Configure the experiment\n",
    "config = SelfImproveConfig(\n",
    "    base_model=\"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "    problem_types=[\"arithmetic\", \"rpn\", \"parentheses\"],\n",
    "    train_size=30,      # Number of training problems\n",
    "    val_size=10,        # Number of validation problems\n",
    "    test_size=10,       # Number of test problems\n",
    "    difficulty_min=3,\n",
    "    difficulty_max=7,\n",
    "    num_iterations=3,   # Number of self-improvement iterations\n",
    "    learning_rate=1e-4,\n",
    "    epochs_per_iteration=1,\n",
    "    batch_size=1,\n",
    "    gradient_accumulation=4,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Create and run experiment\n",
    "experiment = SelfImproveExperiment(config)\n",
    "results = experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iterations = [m[\"iteration\"] for m in results]\n",
    "train_accs = [m[\"train_acc\"] * 100 for m in results]\n",
    "val_accs = [m[\"val_acc\"] * 100 for m in results]\n",
    "test_accs = [m[\"test_acc\"] * 100 for m in results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations, train_accs, 'b-o', label='Train', linewidth=2, markersize=8)\n",
    "plt.plot(iterations, val_accs, 'g-s', label='Validation', linewidth=2, markersize=8)\n",
    "plt.plot(iterations, test_accs, 'r-^', label='Test', linewidth=2, markersize=8)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('Self-Improvement Learning Curve', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(iterations)\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nResults Summary:\")\n",
    "print(\"-\" * 50)\n",
    "for m in results:\n",
    "    print(f\"Iteration {m['iteration']}: \"\n",
    "          f\"Train={m['train_acc']:.1%}, \"\n",
    "          f\"Val={m['val_acc']:.1%}, \"\n",
    "          f\"Test={m['test_acc']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Test Individual Components (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEST INDIVIDUAL COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "# Test problem generation\n",
    "print(\"Testing problem generation...\")\n",
    "for ptype in [\"arithmetic\", \"rpn\", \"parentheses\"]:\n",
    "    gen = experiment.generators[ptype]\n",
    "    prob = gen.generate(difficulty=5)\n",
    "    print(f\"\\n{ptype.upper()}:\")\n",
    "    print(f\"  Title: {prob.title}\")\n",
    "    print(f\"  Expected: {prob.test_cases[0]['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model generation on a single problem\n",
    "print(\"Testing model generation...\")\n",
    "\n",
    "test_prob = experiment.generators[\"arithmetic\"].generate(difficulty=3).to_problem()\n",
    "print(f\"\\nProblem: {test_prob.title}\")\n",
    "print(f\"Expected: {test_prob.test_cases[0].expected_output}\")\n",
    "\n",
    "code = experiment.generate_solution(test_prob)\n",
    "print(f\"\\nGenerated code:\\n{code}\")\n",
    "\n",
    "result = experiment.harness.verify(code, test_prob)\n",
    "print(f\"\\nVerification: {result.status.value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axiom-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
