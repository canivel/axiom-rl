{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 15: M-GRPO with Entropy Control\n",
    "\n",
    "**Status:** Ready to Run  \n",
    "**Date:** 2024-12-20  \n",
    "**Runtime:** Google Colab A100 Recommended  \n",
    "\n",
    "This notebook implements M-GRPO (Momentum-Anchored GRPO) with entropy control mechanisms.\n",
    "\n",
    "## Key Innovations from Papers\n",
    "\n",
    "### M-GRPO Paper\n",
    "- Two-model setup: policy (trainable) + momentum (EMA)\n",
    "- Combined sampling: M from policy + N from momentum\n",
    "- IQR-based entropy filtering to prevent mode collapse\n",
    "\n",
    "### Entropy Mechanism Paper  \n",
    "- Performance follows: R = -a * exp(H) + b\n",
    "- 95% of gains in first 1/12 of training\n",
    "- Clip-Cov and KL-Cov for entropy control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers accelerate peft datasets matplotlib seaborn"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# GPU Check\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU Detected: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "    DEVICE = \"cuda:0\"\n",
    "    DTYPE = torch.float16\n",
    "else:\n",
    "    print(\"No GPU! This experiment requires GPU.\")\n",
    "    DEVICE = \"cpu\"\n",
    "    DTYPE = torch.float32\n",
    "\n",
    "print(f\"\\nUsing: {DEVICE}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone axiom-rl repository\n",
    "!git clone https://github.com/YOUR_REPO/axiom-rl.git 2>/dev/null || echo \"Repo exists\"\n",
    "import sys\n",
    "sys.path.insert(0, 'axiom-rl')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"model_name\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "    \n",
    "    # M-GRPO\n",
    "    \"num_policy_samples\": 4,\n",
    "    \"num_momentum_samples\": 4,\n",
    "    \"momentum\": 0.99,\n",
    "    \"beta\": 0.04,\n",
    "    \n",
    "    # Entropy Control\n",
    "    \"use_iqr_filter\": True,\n",
    "    \"iqr_k\": 0.75,\n",
    "    \"use_clip_cov\": False,\n",
    "    \"use_kl_cov\": False,\n",
    "    \n",
    "    # Training\n",
    "    \"num_iterations\": 20,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"temperature\": 0.7,\n",
    "    \n",
    "    # LoRA\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    # Problems\n",
    "    \"problem_types\": [\"rpn\", \"parentheses\", \"fibonacci\", \"binary_search\"],\n",
    "    \"train_per_type\": 10,\n",
    "    \"val_per_type\": 5,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: LOAD MODEL AND SETUP TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import copy\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"], trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Tokenizer loaded\")\n",
    "\n",
    "# Load policy model\n",
    "print(\"\\nLoading policy model...\")\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "policy_model = get_peft_model(policy_model, lora_config)\n",
    "policy_model.print_trainable_parameters()\n",
    "\n",
    "# Create momentum model (EMA copy)\n",
    "print(\"\\nCreating momentum model...\")\n",
    "momentum_model = copy.deepcopy(policy_model)\n",
    "momentum_model.eval()\n",
    "for p in momentum_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Reference model (frozen)\n",
    "print(\"\\nLoading reference model...\")\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "ref_model.eval()\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"\\nAll models loaded!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    policy_model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    ")\n",
    "print(\"Optimizer configured\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: PROBLEM GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from axiom.procedural import (\n",
    "    RPNEvaluatorGenerator,\n",
    "    ParenthesesValidatorGenerator,\n",
    "    FibonacciGenerator,\n",
    "    BinarySearchGenerator,\n",
    ")\n",
    "import random\n",
    "\n",
    "GENERATORS = {\n",
    "    \"rpn\": RPNEvaluatorGenerator,\n",
    "    \"parentheses\": ParenthesesValidatorGenerator,\n",
    "    \"fibonacci\": FibonacciGenerator,\n",
    "    \"binary_search\": BinarySearchGenerator,\n",
    "}\n",
    "\n",
    "def generate_problems(config, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    train_problems, val_problems = [], []\n",
    "    \n",
    "    for prob_type in config[\"problem_types\"]:\n",
    "        if prob_type not in GENERATORS:\n",
    "            continue\n",
    "        gen = GENERATORS[prob_type](seed=rng.randint(0, 1000000))\n",
    "        \n",
    "        for _ in range(config[\"train_per_type\"]):\n",
    "            train_problems.append(gen.generate(difficulty=5, num_test_cases=5))\n",
    "        for _ in range(config[\"val_per_type\"]):\n",
    "            val_problems.append(gen.generate(difficulty=5, num_test_cases=5))\n",
    "    \n",
    "    rng.shuffle(train_problems)\n",
    "    rng.shuffle(val_problems)\n",
    "    return train_problems, val_problems\n",
    "\n",
    "train_problems, val_problems = generate_problems(CONFIG)\n",
    "print(f\"Train problems: {len(train_problems)}\")\n",
    "print(f\"Val problems: {len(val_problems)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: M-GRPO TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from axiom.verifier import TestHarness\n",
    "\n",
    "harness = TestHarness()\n",
    "\n",
    "def update_momentum(policy_model, momentum_model, m=0.99):\n",
    "    \"\"\"EMA update: theta_k <- m * theta_k + (1-m) * theta_q\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for p_k, p_q in zip(momentum_model.parameters(), policy_model.parameters()):\n",
    "            p_k.data.mul_(m).add_(p_q.data, alpha=1 - m)\n",
    "\n",
    "def generate_samples(model, tokenizer, prompt, num_samples=4):\n",
    "    \"\"\"Generate samples from a model.\"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer([prompt] * num_samples, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
    "            do_sample=True,\n",
    "            temperature=CONFIG[\"temperature\"],\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    return tokenizer.batch_decode(outputs[:, input_len:], skip_special_tokens=True)\n",
    "\n",
    "def compute_entropy(logits):\n",
    "    \"\"\"Compute per-token entropy.\"\"\"\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    return -(probs * log_probs).sum(dim=-1)\n",
    "\n",
    "def iqr_filter(entropies, k=0.75):\n",
    "    \"\"\"Filter low-entropy samples using IQR method.\"\"\"\n",
    "    arr = np.array(entropies)\n",
    "    Q1, Q3 = np.percentile(arr, 25), np.percentile(arr, 75)\n",
    "    threshold = Q1 - k * (Q3 - Q1)\n",
    "    return arr >= max(threshold, 0.1)\n",
    "\n",
    "print(\"Training utilities defined\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training metrics tracking\n",
    "metrics_history = {\n",
    "    \"iteration\": [],\n",
    "    \"loss\": [],\n",
    "    \"reward\": [],\n",
    "    \"entropy\": [],\n",
    "    \"filtered_count\": [],\n",
    "    \"val_accuracy\": [],\n",
    "}\n",
    "\n",
    "print(\"Metrics tracking initialized\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"M-GRPO TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Iterations: {CONFIG['num_iterations']}\")\n",
    "print(f\"Momentum: {CONFIG['momentum']}\")\n",
    "print(f\"IQR Filter: {CONFIG['use_iqr_filter']}\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for iteration in range(CONFIG[\"num_iterations\"]):\n",
    "    iter_start = datetime.now()\n",
    "    print(f\"\\n--- Iteration {iteration + 1}/{CONFIG['num_iterations']} ---\")\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_reward = 0\n",
    "    total_entropy = 0\n",
    "    total_filtered = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    # Process each training problem\n",
    "    for problem in train_problems[:10]:  # Limit for demo\n",
    "        prompt = problem.to_prompt()\n",
    "        \n",
    "        # 1. Combined rollout: policy + momentum\n",
    "        policy_gens = generate_samples(policy_model, tokenizer, prompt, CONFIG[\"num_policy_samples\"])\n",
    "        momentum_gens = generate_samples(momentum_model, tokenizer, prompt, CONFIG[\"num_momentum_samples\"])\n",
    "        all_gens = policy_gens + momentum_gens\n",
    "        \n",
    "        # 2. Compute rewards\n",
    "        rewards = []\n",
    "        for gen in all_gens:\n",
    "            try:\n",
    "                result = harness.verify(problem, gen)\n",
    "                rewards.append(1.0 if result.passed else 0.0)\n",
    "            except:\n",
    "                rewards.append(0.0)\n",
    "        \n",
    "        policy_rewards = rewards[:len(policy_gens)]\n",
    "        \n",
    "        # 3. Only train on positive rewards\n",
    "        for gen, rew in zip(policy_gens, policy_rewards):\n",
    "            if rew <= 0:\n",
    "                continue\n",
    "            \n",
    "            # Tokenize and compute loss\n",
    "            full_text = prompt + gen\n",
    "            inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=2048).to(policy_model.device)\n",
    "            prompt_len = len(tokenizer.encode(prompt))\n",
    "            \n",
    "            outputs = policy_model(**inputs)\n",
    "            logits = outputs.logits[:, prompt_len-1:-1, :]\n",
    "            labels = inputs.input_ids[:, prompt_len:]\n",
    "            \n",
    "            # Compute entropy\n",
    "            entropy = compute_entropy(logits).mean().item()\n",
    "            total_entropy += entropy\n",
    "            \n",
    "            # Policy gradient loss\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            token_log_probs = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n",
    "            loss = -token_log_probs.mean()\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_reward += rew\n",
    "            num_samples += 1\n",
    "    \n",
    "    # 4. Update momentum model\n",
    "    update_momentum(policy_model, momentum_model, CONFIG[\"momentum\"])\n",
    "    \n",
    "    # 5. Evaluate on validation set\n",
    "    val_correct = 0\n",
    "    for problem in val_problems[:5]:  # Quick eval\n",
    "        prompt = problem.to_prompt()\n",
    "        gens = generate_samples(policy_model, tokenizer, prompt, 1)\n",
    "        try:\n",
    "            result = harness.verify(problem, gens[0])\n",
    "            if result.passed:\n",
    "                val_correct += 1\n",
    "        except:\n",
    "            pass\n",
    "    val_acc = val_correct / 5\n",
    "    \n",
    "    # 6. Log metrics\n",
    "    avg_loss = total_loss / max(num_samples, 1)\n",
    "    avg_reward = total_reward / max(num_samples, 1)\n",
    "    avg_entropy = total_entropy / max(num_samples, 1)\n",
    "    \n",
    "    metrics_history[\"iteration\"].append(iteration + 1)\n",
    "    metrics_history[\"loss\"].append(avg_loss)\n",
    "    metrics_history[\"reward\"].append(avg_reward)\n",
    "    metrics_history[\"entropy\"].append(avg_entropy)\n",
    "    metrics_history[\"filtered_count\"].append(total_filtered)\n",
    "    metrics_history[\"val_accuracy\"].append(val_acc)\n",
    "    \n",
    "    iter_time = (datetime.now() - iter_start).total_seconds()\n",
    "    print(f\"  Loss: {avg_loss:.4f}, Reward: {avg_reward:.3f}, Entropy: {avg_entropy:.3f}\")\n",
    "    print(f\"  Val Accuracy: {val_acc:.1%}, Time: {iter_time:.1f}s\")\n",
    "    \n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "total_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "print(f\"\\nTraining complete! Total time: {total_time:.1f} minutes\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 5: RESULTS VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(metrics_history[\"iteration\"], metrics_history[\"loss\"], 'b-o')\n",
    "axes[0, 0].set_xlabel(\"Iteration\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].set_title(\"Training Loss\")\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Reward\n",
    "axes[0, 1].plot(metrics_history[\"iteration\"], metrics_history[\"reward\"], 'g-o')\n",
    "axes[0, 1].set_xlabel(\"Iteration\")\n",
    "axes[0, 1].set_ylabel(\"Reward\")\n",
    "axes[0, 1].set_title(\"Average Reward\")\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Entropy\n",
    "axes[1, 0].plot(metrics_history[\"iteration\"], metrics_history[\"entropy\"], 'r-o')\n",
    "axes[1, 0].set_xlabel(\"Iteration\")\n",
    "axes[1, 0].set_ylabel(\"Entropy\")\n",
    "axes[1, 0].set_title(\"Policy Entropy (should stay stable)\")\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Validation Accuracy\n",
    "axes[1, 1].plot(metrics_history[\"iteration\"], [v*100 for v in metrics_history[\"val_accuracy\"]], 'm-o')\n",
    "axes[1, 1].set_xlabel(\"Iteration\")\n",
    "axes[1, 1].set_ylabel(\"Accuracy (%)\")\n",
    "axes[1, 1].set_title(\"Validation Accuracy\")\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"mgrpo_results.png\", dpi=150)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6: SAVE MODEL AND RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# Save model\n",
    "policy_model.save_pretrained(\"mgrpo_model\")\n",
    "tokenizer.save_pretrained(\"mgrpo_model\")\n",
    "print(\"Model saved to mgrpo_model/\")\n",
    "\n",
    "# Save metrics\n",
    "with open(\"mgrpo_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics_history, f, indent=2)\n",
    "print(\"Metrics saved to mgrpo_metrics.json\")\n",
    "\n",
    "# Save config\n",
    "with open(\"mgrpo_config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(\"Config saved to mgrpo_config.json\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 7: RUN BENCHMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install benchmark dependencies\n",
    "!pip install -q datasets"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from axiom.benchmarks import run_all_benchmarks\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING BENCHMARKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run benchmarks on trained model\n",
    "reports = run_all_benchmarks(\n",
    "    model_path=\"mgrpo_model\",\n",
    "    benchmark_names=[\"math500\", \"gpqa_diamond\"],\n",
    "    output_dir=Path(\"benchmark_results\"),\n",
    "    max_samples=50,  # Limit for quick testing\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for name, report in reports.items():\n",
    "    print(f\"  {name}: {report.accuracy:.1%} ({report.correct}/{report.total})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 15 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {CONFIG['model_name']}\")\n",
    "print(f\"Iterations: {CONFIG['num_iterations']}\")\n",
    "print(f\"Momentum: {CONFIG['momentum']}\")\n",
    "print(f\"IQR Filter: {CONFIG['use_iqr_filter']}\")\n",
    "print(f\"\\nFinal Validation Accuracy: {metrics_history['val_accuracy'][-1]:.1%}\")\n",
    "print(f\"Final Entropy: {metrics_history['entropy'][-1]:.3f}\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(\"  - mgrpo_model/ (trained model)\")\n",
    "print(\"  - mgrpo_metrics.json (training metrics)\")\n",
    "print(\"  - mgrpo_config.json (configuration)\")\n",
    "print(\"  - mgrpo_results.png (training curves)\")\n",
    "print(\"  - benchmark_results/ (benchmark scores)\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
