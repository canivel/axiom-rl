{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 16: M-GRPO with Class Wrapper Fix\n",
    "\n",
    "**Standalone Colab Notebook** - All code self-contained, no external imports needed.\n",
    "\n",
    "## Changes from Experiment 15\n",
    "1. **Fixed `extract_code()`** to handle `class Solution` wrappers\n",
    "2. **Improved prompts** to explicitly request standalone functions\n",
    "3. **Added greedy evaluation** during training for early bug detection\n",
    "\n",
    "## Expected Runtime\n",
    "- ~7 minutes per step on T4\n",
    "- 20 steps = ~2.5 hours total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers peft bitsandbytes accelerate\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Problem Generators\n",
    "\n",
    "Six problem types: RPN, Parentheses, Fibonacci, Binary Search, Edit Distance, Coin Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Any, Optional, Callable\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"A single test case with input and expected output.\"\"\"\n",
    "    input_args: Any\n",
    "    expected_output: Any\n",
    "\n",
    "@dataclass\n",
    "class Problem:\n",
    "    \"\"\"A coding problem with description, signature, and test cases.\"\"\"\n",
    "    problem_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    function_name: str\n",
    "    function_signature: str\n",
    "    test_cases: List[TestCase]\n",
    "    difficulty: int = 5\n",
    "    problem_type: str = \"general\"\n",
    "    examples: List[str] = field(default_factory=list)\n",
    "\n",
    "    def to_prompt(self) -> str:\n",
    "        \"\"\"Convert problem to a prompt for the model - IMPROVED VERSION.\"\"\"\n",
    "        examples_str = \"\\n\".join(f\"  {ex}\" for ex in self.examples[:3])\n",
    "\n",
    "        # IMPROVED: Explicit instruction to avoid class wrappers\n",
    "        prompt = f\"\"\"Write a Python function to solve the following problem.\n",
    "\n",
    "## Problem: {self.title}\n",
    "\n",
    "{self.description}\n",
    "\n",
    "## Function Signature\n",
    "```python\n",
    "{self.function_signature}\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "- Implement the function exactly as specified\n",
    "- Handle edge cases appropriately\n",
    "- Return the correct type\n",
    "\n",
    "## Examples\n",
    "{examples_str}\n",
    "\n",
    "## IMPORTANT\n",
    "Write ONLY a standalone Python function.\n",
    "Do NOT wrap it in a class.\n",
    "Do NOT use 'class Solution'.\n",
    "The function must be directly callable.\n",
    "\"\"\"\n",
    "        return prompt.strip()\n",
    "\n",
    "\n",
    "class ProblemGenerator(ABC):\n",
    "    \"\"\"Base class for procedural problem generators.\"\"\"\n",
    "\n",
    "    def __init__(self, seed: int = None):\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> Problem:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ============== RPN EVALUATOR ==============\n",
    "class RPNEvaluatorGenerator(ProblemGenerator):\n",
    "    \"\"\"Generates RPN (Reverse Polish Notation) evaluation problems.\"\"\"\n",
    "\n",
    "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> Problem:\n",
    "        test_cases = []\n",
    "        for _ in range(num_test_cases):\n",
    "            tokens, result = self._generate_rpn_expression(difficulty)\n",
    "            test_cases.append(TestCase(input_args=[tokens], expected_output=result))\n",
    "\n",
    "        return Problem(\n",
    "            problem_id=f\"rpn_{self.rng.randint(1000, 9999)}\",\n",
    "            title=\"RPN Expression Evaluator\",\n",
    "            description=\"\"\"Evaluate a Reverse Polish Notation (RPN) expression.\n",
    "\n",
    "RPN is a mathematical notation where operators follow their operands.\n",
    "For example: [\"2\", \"3\", \"+\"] = 5, [\"4\", \"2\", \"*\", \"3\", \"+\"] = 11\n",
    "\n",
    "Supported operators: +, -, *\n",
    "All operands are integers (can be negative).\"\"\",\n",
    "            function_name=\"evaluate_rpn\",\n",
    "            function_signature=\"def evaluate_rpn(tokens: List[str]) -> int:\",\n",
    "            test_cases=test_cases,\n",
    "            difficulty=difficulty,\n",
    "            problem_type=\"rpn\",\n",
    "            examples=[\n",
    "                f'evaluate_rpn({test_cases[0].input_args[0]}) -> {test_cases[0].expected_output}',\n",
    "                f'evaluate_rpn({test_cases[1].input_args[0]}) -> {test_cases[1].expected_output}' if len(test_cases) > 1 else \"\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _generate_rpn_expression(self, difficulty: int):\n",
    "        num_ops = min(difficulty, 5)\n",
    "        operators = ['+', '-', '*']\n",
    "        tokens = []\n",
    "        stack = []\n",
    "\n",
    "        for _ in range(num_ops + 1):\n",
    "            # Include some negative numbers for robustness\n",
    "            if self.rng.random() < 0.2:\n",
    "                val = self.rng.randint(-10, -1)\n",
    "            else:\n",
    "                val = self.rng.randint(1, 10)\n",
    "            tokens.append(str(val))\n",
    "            stack.append(val)\n",
    "\n",
    "            if len(stack) >= 2 and self.rng.random() < 0.7:\n",
    "                op = self.rng.choice(operators)\n",
    "                tokens.append(op)\n",
    "                b, a = stack.pop(), stack.pop()\n",
    "                if op == '+': stack.append(a + b)\n",
    "                elif op == '-': stack.append(a - b)\n",
    "                elif op == '*': stack.append(a * b)\n",
    "\n",
    "        while len(stack) > 1:\n",
    "            op = self.rng.choice(operators)\n",
    "            tokens.append(op)\n",
    "            b, a = stack.pop(), stack.pop()\n",
    "            if op == '+': stack.append(a + b)\n",
    "            elif op == '-': stack.append(a - b)\n",
    "            elif op == '*': stack.append(a * b)\n",
    "\n",
    "        return tokens, stack[0]\n",
    "\n",
    "\n",
    "# ============== PARENTHESES VALIDATOR ==============\n",
    "class ParenthesesValidatorGenerator(ProblemGenerator):\n",
    "    \"\"\"Generates balanced parentheses validation problems.\"\"\"\n",
    "\n",
    "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> Problem:\n",
    "        test_cases = []\n",
    "        for i in range(num_test_cases):\n",
    "            if i < num_test_cases // 2:\n",
    "                s = self._generate_valid(difficulty)\n",
    "                test_cases.append(TestCase(input_args=[s], expected_output=True))\n",
    "            else:\n",
    "                s = self._generate_invalid(difficulty)\n",
    "                test_cases.append(TestCase(input_args=[s], expected_output=False))\n",
    "\n",
    "        self.rng.shuffle(test_cases)\n",
    "\n",
    "        return Problem(\n",
    "            problem_id=f\"paren_{self.rng.randint(1000, 9999)}\",\n",
    "            title=\"Valid Parentheses\",\n",
    "            description=\"\"\"Determine if a string of parentheses is valid.\n",
    "\n",
    "A string is valid if:\n",
    "- Open brackets are closed by the same type of brackets\n",
    "- Open brackets are closed in the correct order\n",
    "- Every close bracket has a corresponding open bracket\n",
    "\n",
    "Valid brackets: (), [], {}\"\"\",\n",
    "            function_name=\"is_valid_parentheses\",\n",
    "            function_signature=\"def is_valid_parentheses(s: str) -> bool:\",\n",
    "            test_cases=test_cases,\n",
    "            difficulty=difficulty,\n",
    "            problem_type=\"parentheses\",\n",
    "            examples=[\n",
    "                f'is_valid_parentheses(\"{test_cases[0].input_args[0]}\") -> {test_cases[0].expected_output}',\n",
    "                f'is_valid_parentheses(\"{test_cases[1].input_args[0]}\") -> {test_cases[1].expected_output}' if len(test_cases) > 1 else \"\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _generate_valid(self, difficulty: int) -> str:\n",
    "        pairs = [('(', ')'), ('[', ']'), ('{', '}')]\n",
    "        length = min(difficulty * 2, 12)\n",
    "        result = []\n",
    "\n",
    "        for _ in range(length // 2):\n",
    "            pair = self.rng.choice(pairs)\n",
    "            pos = self.rng.randint(0, len(result))\n",
    "            result.insert(pos, pair[0])\n",
    "            result.insert(pos + 1, pair[1])\n",
    "\n",
    "        return ''.join(result)\n",
    "\n",
    "    def _generate_invalid(self, difficulty: int) -> str:\n",
    "        valid = self._generate_valid(difficulty)\n",
    "        if not valid:\n",
    "            return \"(\"\n",
    "\n",
    "        chars = list(valid)\n",
    "        mutation = self.rng.choice(['swap', 'remove', 'add'])\n",
    "\n",
    "        if mutation == 'swap' and len(chars) >= 2:\n",
    "            i, j = self.rng.sample(range(len(chars)), 2)\n",
    "            chars[i], chars[j] = chars[j], chars[i]\n",
    "        elif mutation == 'remove' and chars:\n",
    "            chars.pop(self.rng.randint(0, len(chars) - 1))\n",
    "        else:\n",
    "            chars.insert(self.rng.randint(0, len(chars)), self.rng.choice('([{'))\n",
    "\n",
    "        return ''.join(chars)\n",
    "\n",
    "\n",
    "# ============== FIBONACCI ==============\n",
    "class FibonacciGenerator(ProblemGenerator):\n",
    "    \"\"\"Generates Fibonacci number computation problems.\"\"\"\n",
    "\n",
    "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> Problem:\n",
    "        max_n = min(difficulty * 5, 30)\n",
    "        test_cases = []\n",
    "\n",
    "        # Always include base cases\n",
    "        test_cases.append(TestCase(input_args=[0], expected_output=0))\n",
    "        test_cases.append(TestCase(input_args=[1], expected_output=1))\n",
    "\n",
    "        # Add random cases\n",
    "        for _ in range(num_test_cases - 2):\n",
    "            n = self.rng.randint(2, max_n)\n",
    "            test_cases.append(TestCase(input_args=[n], expected_output=self._fib(n)))\n",
    "\n",
    "        self.rng.shuffle(test_cases)\n",
    "\n",
    "        return Problem(\n",
    "            problem_id=f\"fib_{self.rng.randint(1000, 9999)}\",\n",
    "            title=\"Fibonacci Number\",\n",
    "            description=\"\"\"Compute the nth Fibonacci number.\n",
    "\n",
    "The Fibonacci sequence is: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\n",
    "- F(0) = 0\n",
    "- F(1) = 1\n",
    "- F(n) = F(n-1) + F(n-2) for n > 1\"\"\",\n",
    "            function_name=\"fibonacci\",\n",
    "            function_signature=\"def fibonacci(n: int) -> int:\",\n",
    "            test_cases=test_cases,\n",
    "            difficulty=difficulty,\n",
    "            problem_type=\"fibonacci\",\n",
    "            examples=[\n",
    "                f'fibonacci({test_cases[0].input_args[0]}) -> {test_cases[0].expected_output}',\n",
    "                f'fibonacci({test_cases[1].input_args[0]}) -> {test_cases[1].expected_output}',\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _fib(self, n: int) -> int:\n",
    "        if n <= 1:\n",
    "            return n\n",
    "        a, b = 0, 1\n",
    "        for _ in range(2, n + 1):\n",
    "            a, b = b, a + b\n",
    "        return b\n",
    "\n",
    "\n",
    "# ============== BINARY SEARCH ==============\n",
    "class BinarySearchGenerator(ProblemGenerator):\n",
    "    \"\"\"Generates binary search problems.\"\"\"\n",
    "\n",
    "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> Problem:\n",
    "        array_size = min(difficulty * 3, 20)\n",
    "        test_cases = []\n",
    "\n",
    "        for i in range(num_test_cases):\n",
    "            arr = sorted(self.rng.sample(range(1, 100), array_size))\n",
    "\n",
    "            if i < num_test_cases // 2:\n",
    "                target = self.rng.choice(arr)\n",
    "                expected = arr.index(target)\n",
    "            else:\n",
    "                target = self.rng.randint(1, 100)\n",
    "                while target in arr:\n",
    "                    target = self.rng.randint(1, 100)\n",
    "                expected = -1\n",
    "\n",
    "            test_cases.append(TestCase(input_args=[arr, target], expected_output=expected))\n",
    "\n",
    "        return Problem(\n",
    "            problem_id=f\"bsearch_{self.rng.randint(1000, 9999)}\",\n",
    "            title=\"Binary Search\",\n",
    "            description=\"\"\"Find the index of a target value in a sorted array.\n",
    "\n",
    "Return the index if found, -1 if not found.\n",
    "The array is sorted in ascending order.\n",
    "Use binary search for O(log n) time complexity.\"\"\",\n",
    "            function_name=\"binary_search\",\n",
    "            function_signature=\"def binary_search(arr: List[int], target: int) -> int:\",\n",
    "            test_cases=test_cases,\n",
    "            difficulty=difficulty,\n",
    "            problem_type=\"binary_search\",\n",
    "            examples=[\n",
    "                f'binary_search({test_cases[0].input_args[0]}, {test_cases[0].input_args[1]}) -> {test_cases[0].expected_output}',\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "# ============== EDIT DISTANCE ==============\n",
    "class EditDistanceGenerator(ProblemGenerator):\n",
    "    \"\"\"Generates edit distance (Levenshtein) problems.\"\"\"\n",
    "\n",
    "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> Problem:\n",
    "        max_len = min(difficulty + 2, 8)\n",
    "        test_cases = []\n",
    "\n",
    "        for _ in range(num_test_cases):\n",
    "            s1 = self._random_word(self.rng.randint(2, max_len))\n",
    "            s2 = self._random_word(self.rng.randint(2, max_len))\n",
    "            dist = self._edit_distance(s1, s2)\n",
    "            test_cases.append(TestCase(input_args=[s1, s2], expected_output=dist))\n",
    "\n",
    "        return Problem(\n",
    "            problem_id=f\"edit_{self.rng.randint(1000, 9999)}\",\n",
    "            title=\"Edit Distance\",\n",
    "            description=\"\"\"Compute the edit distance (Levenshtein distance) between two strings.\n",
    "\n",
    "The edit distance is the minimum number of single-character edits\n",
    "(insertions, deletions, or substitutions) needed to transform one string into another.\n",
    "\n",
    "Example: edit_distance(\"kitten\", \"sitting\") = 3\n",
    "- kitten -> sitten (substitute 'k' with 's')\n",
    "- sitten -> sittin (substitute 'e' with 'i')\n",
    "- sittin -> sitting (insert 'g')\"\"\",\n",
    "            function_name=\"edit_distance\",\n",
    "            function_signature=\"def edit_distance(s1: str, s2: str) -> int:\",\n",
    "            test_cases=test_cases,\n",
    "            difficulty=difficulty,\n",
    "            problem_type=\"edit_distance\",\n",
    "            examples=[\n",
    "                f'edit_distance(\"{test_cases[0].input_args[0]}\", \"{test_cases[0].input_args[1]}\") -> {test_cases[0].expected_output}',\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _random_word(self, length: int) -> str:\n",
    "        return ''.join(self.rng.choice('abcdefghij') for _ in range(length))\n",
    "\n",
    "    def _edit_distance(self, s1: str, s2: str) -> int:\n",
    "        m, n = len(s1), len(s2)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "        for i in range(m + 1):\n",
    "            dp[i][0] = i\n",
    "        for j in range(n + 1):\n",
    "            dp[0][j] = j\n",
    "\n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if s1[i-1] == s2[j-1]:\n",
    "                    dp[i][j] = dp[i-1][j-1]\n",
    "                else:\n",
    "                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "\n",
    "        return dp[m][n]\n",
    "\n",
    "\n",
    "# ============== COIN CHANGE ==============\n",
    "class CoinChangeGenerator(ProblemGenerator):\n",
    "    \"\"\"Generates coin change problems.\"\"\"\n",
    "\n",
    "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> Problem:\n",
    "        coins = sorted(self.rng.sample([1, 2, 5, 10, 20, 25, 50], min(difficulty, 5)))\n",
    "        if 1 not in coins:\n",
    "            coins = [1] + coins\n",
    "\n",
    "        test_cases = []\n",
    "        max_amount = difficulty * 10\n",
    "\n",
    "        for _ in range(num_test_cases):\n",
    "            amount = self.rng.randint(1, max_amount)\n",
    "            result = self._min_coins(coins, amount)\n",
    "            test_cases.append(TestCase(input_args=[coins, amount], expected_output=result))\n",
    "\n",
    "        return Problem(\n",
    "            problem_id=f\"coin_{self.rng.randint(1000, 9999)}\",\n",
    "            title=\"Coin Change\",\n",
    "            description=f\"\"\"Find the minimum number of coins needed to make up a given amount.\n",
    "\n",
    "You have an infinite supply of each coin denomination.\n",
    "Return -1 if the amount cannot be made up by any combination of the coins.\n",
    "\n",
    "Coin denominations for this problem: {coins}\"\"\",\n",
    "            function_name=\"coin_change\",\n",
    "            function_signature=\"def coin_change(coins: List[int], amount: int) -> int:\",\n",
    "            test_cases=test_cases,\n",
    "            difficulty=difficulty,\n",
    "            problem_type=\"coin_change\",\n",
    "            examples=[\n",
    "                f'coin_change({coins}, {test_cases[0].input_args[1]}) -> {test_cases[0].expected_output}',\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _min_coins(self, coins: List[int], amount: int) -> int:\n",
    "        dp = [float('inf')] * (amount + 1)\n",
    "        dp[0] = 0\n",
    "\n",
    "        for coin in coins:\n",
    "            for x in range(coin, amount + 1):\n",
    "                dp[x] = min(dp[x], dp[x - coin] + 1)\n",
    "\n",
    "        return dp[amount] if dp[amount] != float('inf') else -1\n",
    "\n",
    "\n",
    "# Generator registry\n",
    "GENERATORS = {\n",
    "    \"rpn\": RPNEvaluatorGenerator,\n",
    "    \"parentheses\": ParenthesesValidatorGenerator,\n",
    "    \"fibonacci\": FibonacciGenerator,\n",
    "    \"binary_search\": BinarySearchGenerator,\n",
    "    \"edit_distance\": EditDistanceGenerator,\n",
    "    \"coin_change\": CoinChangeGenerator,\n",
    "}\n",
    "\n",
    "print(f\"Loaded {len(GENERATORS)} problem generators: {list(GENERATORS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Code Extraction (FIXED VERSION)\n",
    "\n",
    "Key fix: Handles `class Solution` wrappers that the model tends to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "\n",
    "def extract_method_from_class(code: str, func_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract a method from a class and convert to standalone function.\n",
    "\n",
    "    Handles:\n",
    "    - class Solution:\n",
    "        def func_name(self, ...):\n",
    "            ...\n",
    "\n",
    "    Converts to:\n",
    "    - def func_name(...):\n",
    "        ...\n",
    "    \"\"\"\n",
    "    # Find the method definition\n",
    "    # Pattern: def func_name(self, ...) or def func_name(self)\n",
    "    pattern = rf'def\\s+{func_name}\\s*\\(\\s*self\\s*,?\\s*([^)]*)?\\)\\s*(?:->\\s*[^:]+)?\\s*:'\n",
    "    match = re.search(pattern, code)\n",
    "\n",
    "    if not match:\n",
    "        # Method not found, return original\n",
    "        return code\n",
    "\n",
    "    # Find the start of the method\n",
    "    method_start = match.start()\n",
    "\n",
    "    # Find method body by tracking indentation\n",
    "    lines = code[method_start:].split('\\n')\n",
    "    method_lines = [lines[0]]  # def line\n",
    "\n",
    "    # Get the base indentation of the def line\n",
    "    def_indent = len(lines[0]) - len(lines[0].lstrip())\n",
    "\n",
    "    # Collect body lines\n",
    "    for line in lines[1:]:\n",
    "        stripped = line.lstrip()\n",
    "        if not stripped:  # Empty line\n",
    "            method_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        current_indent = len(line) - len(stripped)\n",
    "\n",
    "        # If we hit a line with same or less indentation (and it's not empty)\n",
    "        # that's the end of the method\n",
    "        if current_indent <= def_indent and stripped and not stripped.startswith('#'):\n",
    "            break\n",
    "\n",
    "        method_lines.append(line)\n",
    "\n",
    "    method_code = '\\n'.join(method_lines)\n",
    "\n",
    "    # Remove 'self' parameter\n",
    "    # Pattern: def func_name(self, params) or def func_name(self)\n",
    "    method_code = re.sub(\n",
    "        rf'(def\\s+{func_name}\\s*\\()self\\s*,?\\s*',\n",
    "        r'\\1',\n",
    "        method_code\n",
    "    )\n",
    "\n",
    "    # Dedent to remove class indentation\n",
    "    method_code = textwrap.dedent(method_code)\n",
    "\n",
    "    return method_code.strip()\n",
    "\n",
    "\n",
    "def extract_code(completion: str, func_name: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Extract Python code from model completion.\n",
    "\n",
    "    IMPROVED VERSION - handles:\n",
    "    1. Raw code (no markdown)\n",
    "    2. ```python ... ``` blocks\n",
    "    3. ``` ... ``` blocks\n",
    "    4. Multiple code blocks (takes the longest one containing 'def')\n",
    "    5. NEW: class Solution wrappers (extracts method)\n",
    "    \"\"\"\n",
    "    code = None\n",
    "\n",
    "    # Try to find ```python ... ``` blocks\n",
    "    python_blocks = re.findall(r'```python\\s*(.*?)```', completion, re.DOTALL)\n",
    "    if python_blocks:\n",
    "        # Return the longest block that contains a function definition\n",
    "        for block in sorted(python_blocks, key=len, reverse=True):\n",
    "            if 'def ' in block:\n",
    "                code = block.strip()\n",
    "                break\n",
    "        if not code:\n",
    "            code = python_blocks[0].strip()\n",
    "\n",
    "    if not code:\n",
    "        # Try to find ``` ... ``` blocks\n",
    "        code_blocks = re.findall(r'```\\s*(.*?)```', completion, re.DOTALL)\n",
    "        if code_blocks:\n",
    "            for block in sorted(code_blocks, key=len, reverse=True):\n",
    "                if 'def ' in block:\n",
    "                    code = block.strip()\n",
    "                    break\n",
    "            if not code:\n",
    "                code = code_blocks[0].strip()\n",
    "\n",
    "    if not code:\n",
    "        # If no code blocks, check if it contains def\n",
    "        if 'def ' in completion:\n",
    "            lines = completion.split('\\n')\n",
    "            code_lines = []\n",
    "            in_function = False\n",
    "            indent_level = None\n",
    "\n",
    "            for line in lines:\n",
    "                stripped = line.lstrip()\n",
    "                if stripped.startswith('def '):\n",
    "                    in_function = True\n",
    "                    indent_level = len(line) - len(stripped)\n",
    "                    code_lines = [line]\n",
    "                elif in_function:\n",
    "                    if stripped and not stripped.startswith('#'):\n",
    "                        current_indent = len(line) - len(stripped)\n",
    "                        if current_indent <= indent_level and stripped:\n",
    "                            break\n",
    "                    code_lines.append(line)\n",
    "\n",
    "            if code_lines:\n",
    "                code = '\\n'.join(code_lines).strip()\n",
    "\n",
    "    if not code:\n",
    "        code = completion.strip()\n",
    "\n",
    "    # NEW: Handle class Solution wrappers\n",
    "    if 'class Solution' in code and func_name:\n",
    "        extracted = extract_method_from_class(code, func_name)\n",
    "        if 'def ' in extracted:\n",
    "            code = extracted\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "# Test the extraction\n",
    "test_completion = '''\n",
    "Here's the solution:\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "\n",
    "class Solution:\n",
    "    def evaluate_rpn(self, tokens: List[str]) -> int:\n",
    "        stack = []\n",
    "        for token in tokens:\n",
    "            if token not in \"+-*\":\n",
    "                stack.append(int(token))\n",
    "            else:\n",
    "                b, a = stack.pop(), stack.pop()\n",
    "                if token == '+': stack.append(a + b)\n",
    "                elif token == '-': stack.append(a - b)\n",
    "                elif token == '*': stack.append(a * b)\n",
    "        return stack[0]\n",
    "```\n",
    "'''\n",
    "\n",
    "extracted = extract_code(test_completion, 'evaluate_rpn')\n",
    "print(\"Extracted code:\")\n",
    "print(extracted)\n",
    "print(\"\\n--- Checking if 'self' removed:\", 'self' not in extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Test Harness (Verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class VerificationResult:\n",
    "    passed: bool\n",
    "    passed_count: int\n",
    "    total_count: int\n",
    "    error: str = None\n",
    "\n",
    "\n",
    "def verify_solution(code: str, problem: Problem, timeout: float = 5.0) -> VerificationResult:\n",
    "    \"\"\"\n",
    "    Verify a solution against test cases by executing in subprocess.\n",
    "    \"\"\"\n",
    "    func_name = problem.function_name\n",
    "\n",
    "    # Build test cases JSON\n",
    "    test_cases_data = [\n",
    "        {\"input\": tc.input_args, \"expected\": tc.expected_output}\n",
    "        for tc in problem.test_cases\n",
    "    ]\n",
    "    test_cases_json = json.dumps(test_cases_data)\n",
    "\n",
    "    # Build test script\n",
    "    test_script = f'''# -*- coding: utf-8 -*-\n",
    "import json\n",
    "from typing import List, Optional, Tuple, Dict, Any, Set\n",
    "\n",
    "# === SOLUTION CODE ===\n",
    "{code}\n",
    "# === END SOLUTION ===\n",
    "\n",
    "def run_tests():\n",
    "    test_cases = json.loads(\\'{test_cases_json}\\')\n",
    "    results = []\n",
    "\n",
    "    for i, tc in enumerate(test_cases):\n",
    "        inp = tc[\"input\"]\n",
    "        expected = tc[\"expected\"]\n",
    "\n",
    "        try:\n",
    "            if isinstance(inp, list):\n",
    "                actual = {func_name}(*inp)\n",
    "            else:\n",
    "                actual = {func_name}(inp)\n",
    "\n",
    "            passed = actual == expected\n",
    "            results.append({{\n",
    "                \"index\": i,\n",
    "                \"passed\": passed,\n",
    "                \"expected\": expected,\n",
    "                \"actual\": actual\n",
    "            }})\n",
    "        except Exception as e:\n",
    "            results.append({{\n",
    "                \"index\": i,\n",
    "                \"passed\": False,\n",
    "                \"error\": str(e)\n",
    "            }})\n",
    "\n",
    "    print(json.dumps({{\"results\": results, \"success\": True}}))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_tests()\n",
    "    except Exception as e:\n",
    "        print(json.dumps({{\"results\": [], \"success\": False, \"error\": str(e)}}))\n",
    "'''\n",
    "\n",
    "    # Write to temp file and execute\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "            f.write(test_script)\n",
    "            temp_path = f.name\n",
    "\n",
    "        result = subprocess.run(\n",
    "            ['python', temp_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "\n",
    "        os.unlink(temp_path)\n",
    "\n",
    "        if result.returncode != 0 and not result.stdout.strip():\n",
    "            return VerificationResult(\n",
    "                passed=False,\n",
    "                passed_count=0,\n",
    "                total_count=len(problem.test_cases),\n",
    "                error=result.stderr or \"Execution error\"\n",
    "            )\n",
    "\n",
    "        data = json.loads(result.stdout)\n",
    "\n",
    "        if not data.get(\"success\", False):\n",
    "            return VerificationResult(\n",
    "                passed=False,\n",
    "                passed_count=0,\n",
    "                total_count=len(problem.test_cases),\n",
    "                error=data.get(\"error\", \"Unknown error\")\n",
    "            )\n",
    "\n",
    "        passed_count = sum(1 for r in data[\"results\"] if r[\"passed\"])\n",
    "        total_count = len(data[\"results\"])\n",
    "\n",
    "        return VerificationResult(\n",
    "            passed=(passed_count == total_count),\n",
    "            passed_count=passed_count,\n",
    "            total_count=total_count\n",
    "        )\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return VerificationResult(\n",
    "            passed=False,\n",
    "            passed_count=0,\n",
    "            total_count=len(problem.test_cases),\n",
    "            error=\"Timeout\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return VerificationResult(\n",
    "            passed=False,\n",
    "            passed_count=0,\n",
    "            total_count=len(problem.test_cases),\n",
    "            error=str(e)\n",
    "        )\n",
    "\n",
    "\n",
    "# Test verification with extracted code\n",
    "print(\"Testing verification with extracted class method...\")\n",
    "gen = RPNEvaluatorGenerator(seed=42)\n",
    "problem = gen.generate(difficulty=5, num_test_cases=5)\n",
    "\n",
    "# This is what the model might output (class wrapper)\n",
    "model_output = '''\n",
    "```python\n",
    "class Solution:\n",
    "    def evaluate_rpn(self, tokens):\n",
    "        stack = []\n",
    "        for token in tokens:\n",
    "            if token not in \"+-*\":\n",
    "                stack.append(int(token))\n",
    "            else:\n",
    "                b, a = stack.pop(), stack.pop()\n",
    "                if token == '+': stack.append(a + b)\n",
    "                elif token == '-': stack.append(a - b)\n",
    "                elif token == '*': stack.append(a * b)\n",
    "        return stack[0]\n",
    "```\n",
    "'''\n",
    "\n",
    "extracted = extract_code(model_output, problem.function_name)\n",
    "result = verify_solution(extracted, problem)\n",
    "print(f\"Result: {result.passed_count}/{result.total_count} tests passed\")\n",
    "print(f\"Error: {result.error}\" if result.error else \"No errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import copy\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "policy_model = get_peft_model(model, lora_config)\n",
    "policy_model.print_trainable_parameters()\n",
    "\n",
    "# Create momentum model (deep copy of LoRA weights)\n",
    "momentum_model = copy.deepcopy(policy_model)\n",
    "for param in momentum_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"\\nModels loaded successfully!\")\n",
    "print(f\"Device: {next(policy_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, tokenizer, prompts: List[str], n_samples: int = 4,\n",
    "                     max_new_tokens: int = 512, temperature: float = 0.7) -> List[List[str]]:\n",
    "    \"\"\"Generate multiple samples for each prompt.\"\"\"\n",
    "    all_samples = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "            samples = []\n",
    "            for _ in range(n_samples):\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "                completion = tokenizer.decode(\n",
    "                    outputs[0, inputs['input_ids'].shape[1]:],\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                samples.append(completion)\n",
    "\n",
    "            all_samples.append(samples)\n",
    "\n",
    "    return all_samples\n",
    "\n",
    "\n",
    "def compute_rewards(prompts: List[str], samples: List[List[str]],\n",
    "                    problems: List[Problem]) -> torch.Tensor:\n",
    "    \"\"\"Compute rewards for all samples using partial credit.\"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for prompt, prompt_samples, problem in zip(prompts, samples, problems):\n",
    "        sample_rewards = []\n",
    "        for sample in prompt_samples:\n",
    "            # Extract code with class wrapper handling\n",
    "            code = extract_code(sample, problem.function_name)\n",
    "\n",
    "            # Verify\n",
    "            result = verify_solution(code, problem)\n",
    "\n",
    "            # Partial reward\n",
    "            reward = result.passed_count / max(result.total_count, 1)\n",
    "            sample_rewards.append(reward)\n",
    "\n",
    "        rewards.append(sample_rewards)\n",
    "\n",
    "    return torch.tensor(rewards)\n",
    "\n",
    "\n",
    "def compute_log_probs(model, tokenizer, prompt: str, completion: str) -> torch.Tensor:\n",
    "    \"\"\"Compute log probabilities for a completion given a prompt.\"\"\"\n",
    "    full_text = prompt + completion\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=1536)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Get log probs for completion tokens only\n",
    "    shift_logits = logits[:, prompt_tokens-1:-1, :]\n",
    "    shift_labels = inputs[\"input_ids\"][:, prompt_tokens:]\n",
    "\n",
    "    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "    token_log_probs = torch.gather(log_probs, 2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return token_log_probs.sum()\n",
    "\n",
    "\n",
    "def update_momentum_model(policy_model, momentum_model, momentum: float = 0.99):\n",
    "    \"\"\"Update momentum model with EMA of policy weights.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for (name_p, param_p), (name_m, param_m) in zip(\n",
    "            policy_model.named_parameters(),\n",
    "            momentum_model.named_parameters()\n",
    "        ):\n",
    "            if param_p.requires_grad:  # Only update LoRA parameters\n",
    "                param_m.data = momentum * param_m.data + (1 - momentum) * param_p.data\n",
    "\n",
    "\n",
    "def compute_entropy(model, tokenizer, prompt: str, completion: str) -> float:\n",
    "    \"\"\"Compute entropy of the completion.\"\"\"\n",
    "    full_text = prompt + completion\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=1536)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[:, prompt_tokens-1:-1, :]\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    entropy = -(probs * log_probs).sum(dim=-1).mean().item()\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "print(\"Training utilities loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Problem Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_problem_sets(seed: int = 42):\n",
    "    \"\"\"Generate train/val/test problem sets.\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    problem_types = [\"rpn\", \"parentheses\", \"fibonacci\", \"binary_search\", \"edit_distance\", \"coin_change\"]\n",
    "    train_per_type = 10\n",
    "    val_per_type = 5\n",
    "    test_per_type = 5\n",
    "    difficulty_range = (4, 7)\n",
    "\n",
    "    train_problems, val_problems, test_problems = [], [], []\n",
    "\n",
    "    for prob_type in problem_types:\n",
    "        gen = GENERATORS[prob_type](seed=rng.randint(0, 1000000))\n",
    "\n",
    "        # Training problems\n",
    "        for _ in range(train_per_type):\n",
    "            diff = rng.randint(*difficulty_range)\n",
    "            train_problems.append(gen.generate(difficulty=diff, num_test_cases=5))\n",
    "\n",
    "        # Validation problems\n",
    "        for _ in range(val_per_type):\n",
    "            diff = rng.randint(*difficulty_range)\n",
    "            val_problems.append(gen.generate(difficulty=diff, num_test_cases=5))\n",
    "\n",
    "        # Test problems\n",
    "        for _ in range(test_per_type):\n",
    "            diff = rng.randint(*difficulty_range)\n",
    "            test_problems.append(gen.generate(difficulty=diff, num_test_cases=5))\n",
    "\n",
    "    rng.shuffle(train_problems)\n",
    "    rng.shuffle(val_problems)\n",
    "    rng.shuffle(test_problems)\n",
    "\n",
    "    return train_problems, val_problems, test_problems\n",
    "\n",
    "\n",
    "# Generate problems\n",
    "train_problems, val_problems, test_problems = generate_problem_sets(seed=42)\n",
    "print(f\"Generated {len(train_problems)} training, {len(val_problems)} validation, {len(test_problems)} test problems\")\n",
    "\n",
    "# Show sample prompt with new format\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PROMPT (with improved instructions):\")\n",
    "print(\"=\"*60)\n",
    "print(train_problems[0].to_prompt()[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: M-GRPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import time\n",
    "\n",
    "# Training configuration\n",
    "NUM_STEPS = 20\n",
    "BATCH_SIZE = 4\n",
    "NUM_POLICY_SAMPLES = 4\n",
    "NUM_MOMENTUM_SAMPLES = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "MOMENTUM = 0.99\n",
    "EVAL_EVERY = 2\n",
    "GREEDY_EVAL_EVERY = 5  # NEW: More frequent greedy evaluation\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(\n",
    "    [p for p in policy_model.parameters() if p.requires_grad],\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Training metrics\n",
    "metrics_history = []\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"M-GRPO TRAINING - EXPERIMENT 16\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Steps: {NUM_STEPS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Policy samples: {NUM_POLICY_SAMPLES}\")\n",
    "print(f\"Momentum samples: {NUM_MOMENTUM_SAMPLES}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Momentum (EMA): {MOMENTUM}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_greedy(model, problems: List[Problem], max_problems: int = 10) -> dict:\n",
    "    \"\"\"Evaluate model with greedy decoding (no sampling).\"\"\"\n",
    "    model.eval()\n",
    "    results_by_type = {}\n",
    "\n",
    "    for problem in problems[:max_problems]:\n",
    "        prompt = problem.to_prompt()\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,  # Still sample but more focused\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "        completion = tokenizer.decode(\n",
    "            outputs[0, inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Extract with class wrapper handling\n",
    "        code = extract_code(completion, problem.function_name)\n",
    "        result = verify_solution(code, problem)\n",
    "\n",
    "        prob_type = problem.problem_type\n",
    "        if prob_type not in results_by_type:\n",
    "            results_by_type[prob_type] = {\"passed\": 0, \"total\": 0}\n",
    "\n",
    "        results_by_type[prob_type][\"total\"] += 1\n",
    "        if result.passed:\n",
    "            results_by_type[prob_type][\"passed\"] += 1\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    total_passed = sum(r[\"passed\"] for r in results_by_type.values())\n",
    "    total_problems = sum(r[\"total\"] for r in results_by_type.values())\n",
    "\n",
    "    return {\n",
    "        \"by_type\": results_by_type,\n",
    "        \"overall\": total_passed / max(total_problems, 1),\n",
    "        \"passed\": total_passed,\n",
    "        \"total\": total_problems\n",
    "    }\n",
    "\n",
    "\n",
    "# Run initial evaluation\n",
    "print(\"\\nInitial greedy evaluation...\")\n",
    "initial_eval = evaluate_greedy(policy_model, val_problems, max_problems=10)\n",
    "print(f\"Initial accuracy: {initial_eval['passed']}/{initial_eval['total']} = {initial_eval['overall']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for step in range(NUM_STEPS):\n",
    "    step_start = time.time()\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Step {step}/{NUM_STEPS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Sample batch of problems\n",
    "    batch_problems = random.sample(train_problems, BATCH_SIZE)\n",
    "    prompts = [p.to_prompt() for p in batch_problems]\n",
    "\n",
    "    # Generate from policy model\n",
    "    print(\"Generating from policy model...\")\n",
    "    policy_samples = generate_samples(\n",
    "        policy_model, tokenizer, prompts,\n",
    "        n_samples=NUM_POLICY_SAMPLES\n",
    "    )\n",
    "\n",
    "    # Generate from momentum model\n",
    "    print(\"Generating from momentum model...\")\n",
    "    momentum_samples = generate_samples(\n",
    "        momentum_model, tokenizer, prompts,\n",
    "        n_samples=NUM_MOMENTUM_SAMPLES\n",
    "    )\n",
    "\n",
    "    # Compute rewards\n",
    "    print(\"Computing rewards...\")\n",
    "    policy_rewards = compute_rewards(prompts, policy_samples, batch_problems)\n",
    "    momentum_rewards = compute_rewards(prompts, momentum_samples, batch_problems)\n",
    "\n",
    "    # Combine for advantage computation\n",
    "    all_rewards = torch.cat([policy_rewards, momentum_rewards], dim=1)\n",
    "\n",
    "    # Log per-problem results\n",
    "    for i, problem in enumerate(batch_problems):\n",
    "        best_reward = all_rewards[i].max().item()\n",
    "        print(f\"  [{i+1}/{BATCH_SIZE}] {problem.title}... reward={best_reward:.2f}\")\n",
    "\n",
    "    # Compute advantages (standardized within each prompt)\n",
    "    advantages = torch.zeros_like(policy_rewards)\n",
    "    for i in range(BATCH_SIZE):\n",
    "        prompt_rewards = all_rewards[i]\n",
    "        mean_r = prompt_rewards.mean()\n",
    "        std_r = prompt_rewards.std() + 1e-8\n",
    "        advantages[i] = (policy_rewards[i] - mean_r) / std_r\n",
    "\n",
    "    # Training step\n",
    "    policy_model.train()\n",
    "    total_loss = 0\n",
    "    num_updates = 0\n",
    "\n",
    "    for i, (prompt, samples, advs, problem) in enumerate(\n",
    "        zip(prompts, policy_samples, advantages, batch_problems)\n",
    "    ):\n",
    "        for sample, adv in zip(samples, advs):\n",
    "            if adv.item() <= 0:\n",
    "                continue\n",
    "\n",
    "            # Compute log prob\n",
    "            log_prob = compute_log_probs(policy_model, tokenizer, prompt, sample)\n",
    "\n",
    "            # Policy gradient loss\n",
    "            loss = -log_prob * adv\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_updates += 1\n",
    "\n",
    "    # Update momentum model\n",
    "    update_momentum_model(policy_model, momentum_model, MOMENTUM)\n",
    "\n",
    "    # Compute entropy\n",
    "    entropy = compute_entropy(\n",
    "        policy_model, tokenizer,\n",
    "        prompts[0], policy_samples[0][0]\n",
    "    )\n",
    "\n",
    "    # Metrics\n",
    "    avg_loss = total_loss / max(num_updates, 1)\n",
    "    avg_reward = policy_rewards.max(dim=1).values.mean().item()\n",
    "    success_rate = (policy_rewards.max(dim=1).values > 0).float().mean().item() * 100\n",
    "\n",
    "    step_time = time.time() - step_start\n",
    "    eta = (NUM_STEPS - step - 1) * step_time / 60\n",
    "\n",
    "    # Validation accuracy\n",
    "    val_correct = 0\n",
    "    val_subset = random.sample(val_problems, min(5, len(val_problems)))\n",
    "    for prob in val_subset:\n",
    "        val_samples = generate_samples(\n",
    "            policy_model, tokenizer, [prob.to_prompt()],\n",
    "            n_samples=1\n",
    "        )\n",
    "        code = extract_code(val_samples[0][0], prob.function_name)\n",
    "        result = verify_solution(code, prob)\n",
    "        if result.passed:\n",
    "            val_correct += 1\n",
    "    val_acc = val_correct / len(val_subset) * 100\n",
    "\n",
    "    print(f\"\\nStep {step} Summary:\")\n",
    "    print(f\"  Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Reward: {avg_reward:.3f}\")\n",
    "    print(f\"  Entropy: {entropy:.3f}\")\n",
    "    print(f\"  Success Rate: {success_rate:.1f}%\")\n",
    "    print(f\"  Val Accuracy: {val_acc:.1f}%\")\n",
    "    print(f\"  Updates: {num_updates}\")\n",
    "    print(f\"  Time: {step_time:.1f}s, ETA: {eta:.1f}m\")\n",
    "\n",
    "    # Check for entropy collapse\n",
    "    if entropy < 0.1:\n",
    "        print(\"\\n WARNING: Entropy below 0.1 - potential collapse!\")\n",
    "\n",
    "    # Greedy evaluation\n",
    "    if step > 0 and step % GREEDY_EVAL_EVERY == 0:\n",
    "        print(\"\\n--- Greedy Evaluation ---\")\n",
    "        greedy_eval = evaluate_greedy(policy_model, val_problems, max_problems=10)\n",
    "        print(f\"Greedy accuracy: {greedy_eval['passed']}/{greedy_eval['total']} = {greedy_eval['overall']*100:.1f}%\")\n",
    "        for ptype, results in greedy_eval['by_type'].items():\n",
    "            print(f\"  {ptype}: {results['passed']}/{results['total']}\")\n",
    "\n",
    "    # Save metrics\n",
    "    metrics_history.append({\n",
    "        \"step\": step,\n",
    "        \"loss\": avg_loss,\n",
    "        \"reward\": avg_reward,\n",
    "        \"entropy\": entropy,\n",
    "        \"success_rate\": success_rate,\n",
    "        \"val_accuracy\": val_acc,\n",
    "        \"num_updates\": num_updates,\n",
    "        \"time\": step_time\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Full validation evaluation\n",
    "print(\"\\nEvaluating on full validation set...\")\n",
    "final_eval = evaluate_greedy(policy_model, val_problems, max_problems=len(val_problems))\n",
    "\n",
    "print(\"\\nResults by Problem Type:\")\n",
    "print(\"-\" * 40)\n",
    "for ptype, results in final_eval['by_type'].items():\n",
    "    acc = results['passed'] / max(results['total'], 1) * 100\n",
    "    print(f\"  {ptype:20s}: {results['passed']}/{results['total']} = {acc:.1f}%\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  {'OVERALL':20s}: {final_eval['passed']}/{final_eval['total']} = {final_eval['overall']*100:.1f}%\")\n",
    "\n",
    "# Show sample generation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE GENERATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_problem = val_problems[0]\n",
    "print(f\"\\nProblem: {sample_problem.title}\")\n",
    "print(f\"\\nPrompt:\\n{sample_problem.to_prompt()[:500]}...\")\n",
    "\n",
    "# Generate solution\n",
    "inputs = tokenizer(sample_problem.to_prompt(), return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "inputs = {k: v.to(policy_model.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = policy_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "completion = tokenizer.decode(\n",
    "    outputs[0, inputs['input_ids'].shape[1]:],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated Solution:\\n{completion[:800]}\")\n",
    "\n",
    "# Verify\n",
    "code = extract_code(completion, sample_problem.function_name)\n",
    "result = verify_solution(code, sample_problem)\n",
    "print(f\"\\nVerification: {result.passed_count}/{result.total_count} tests passed\")\n",
    "if result.error:\n",
    "    print(f\"Error: {result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Visualization & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "steps = [m['step'] for m in metrics_history]\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(steps, [m['loss'] for m in metrics_history], 'b-o')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "\n",
    "# Reward\n",
    "axes[0, 1].plot(steps, [m['reward'] for m in metrics_history], 'g-o')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('Mean Reward')\n",
    "axes[0, 1].set_title('Average Reward')\n",
    "\n",
    "# Entropy\n",
    "axes[0, 2].plot(steps, [m['entropy'] for m in metrics_history], 'r-o')\n",
    "axes[0, 2].axhline(y=0.1, color='orange', linestyle='--', label='Collapse threshold')\n",
    "axes[0, 2].set_xlabel('Step')\n",
    "axes[0, 2].set_ylabel('Mean Entropy')\n",
    "axes[0, 2].set_title('Policy Entropy (should stay above 0.1)')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Success Rate\n",
    "axes[1, 0].plot(steps, [m['success_rate'] for m in metrics_history], 'm-o')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Success Rate (%)')\n",
    "axes[1, 0].set_title('Training Success Rate')\n",
    "axes[1, 0].set_ylim(0, 105)\n",
    "\n",
    "# Val Accuracy\n",
    "axes[1, 1].plot(steps, [m['val_accuracy'] for m in metrics_history], 'c-o')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "axes[1, 1].set_title('Validation Accuracy')\n",
    "axes[1, 1].set_ylim(0, 105)\n",
    "\n",
    "# Updates per step\n",
    "axes[1, 2].bar(steps, [m['num_updates'] for m in metrics_history], color='purple', alpha=0.7)\n",
    "axes[1, 2].set_xlabel('Step')\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "axes[1, 2].set_title('Gradient Updates per Step')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiment_16_metrics.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 16 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_time = sum(m['time'] for m in metrics_history)\n",
    "print(f\"Total training time: {total_time/60:.1f} minutes\")\n",
    "print(f\"\\nFinal metrics:\")\n",
    "print(f\"  Loss: {metrics_history[-1]['loss']:.4f}\")\n",
    "print(f\"  Reward: {metrics_history[-1]['reward']:.3f}\")\n",
    "print(f\"  Entropy: {metrics_history[-1]['entropy']:.3f}\")\n",
    "print(f\"  Training Success: {metrics_history[-1]['success_rate']:.1f}%\")\n",
    "print(f\"  Val Accuracy: {metrics_history[-1]['val_accuracy']:.1f}%\")\n",
    "print(f\"\\nFinal Evaluation: {final_eval['overall']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Save Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "save_path = \"./experiment_16_model\"\n",
    "policy_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# Save metrics\n",
    "with open(f\"{save_path}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics_history, f, indent=2)\n",
    "print(f\"Metrics saved to {save_path}/metrics.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
