{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 15: M-GRPO with Entropy Control (Standalone)\n",
        "\n",
        "**Status:** Ready to Run  \n",
        "**Date:** 2024-12-21  \n",
        "**Runtime:** Google Colab (T4/A100 GPU)  \n",
        "\n",
        "This is a **fully self-contained notebook** - no external dependencies on the axiom-rl package.\n",
        "All code needed for M-GRPO training is embedded directly in this notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## What is M-GRPO?\n",
        "\n",
        "**M-GRPO (Momentum-Anchored GRPO)** is a stabilized reinforcement learning technique for training language models.\n",
        "\n",
        "### The Problem with Standard GRPO\n",
        "Standard GRPO (Group Relative Policy Optimization) often fails due to **policy collapse**:\n",
        "- Model becomes overconfident in one solution pattern\n",
        "- Entropy drops → diversity collapses\n",
        "- Performance crashes\n",
        "\n",
        "### M-GRPO Solution: Two Models\n",
        "1. **Policy Model** - Trainable, learns and improves\n",
        "2. **Momentum Model** - Slow EMA copy, provides stable reference\n",
        "\n",
        "**EMA Update:** `θ_momentum = 0.99 * θ_momentum + 0.01 * θ_policy`\n",
        "\n",
        "Combined sampling from BOTH models prevents collapse.\n",
        "\n",
        "---\n",
        "\n",
        "## What We're Training\n",
        "\n",
        "Teaching a 0.5B model to write Python functions for:\n",
        "- **RPN Evaluator** - Evaluate reverse polish notation expressions\n",
        "- **Parentheses Validator** - Check if brackets are balanced\n",
        "- **Fibonacci** - Compute nth Fibonacci number\n",
        "- **Binary Search** - Find element in sorted array\n",
        "- **Edit Distance** - Levenshtein distance between strings\n",
        "- **Coin Change** - Minimum coins for amount (DP)\n",
        "\n",
        "Each problem has 5 test cases. Model gets **partial reward** for passing some tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## PART 1: SETUP AND INSTALLATION\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch transformers accelerate peft bitsandbytes matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Check and Configuration\n",
        "import torch\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GPU DETECTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU Detected: {gpu_name}\")\n",
        "    print(f\"GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
        "    DEVICE = \"cuda:0\"\n",
        "    DTYPE = torch.float16\n",
        "    \n",
        "    # Select model based on GPU memory\n",
        "    if gpu_memory_gb >= 15:\n",
        "        MODEL_NAME = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
        "        print(f\"Using 1.5B model (enough VRAM)\")\n",
        "    else:\n",
        "        MODEL_NAME = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
        "        print(f\"Using 0.5B model (limited VRAM)\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected! Training will be very slow.\")\n",
        "    DEVICE = \"cpu\"\n",
        "    DTYPE = torch.float32\n",
        "    MODEL_NAME = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
        "\n",
        "print(f\"\\nDevice: {DEVICE}\")\n",
        "print(f\"Model: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    # Model\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"torch_dtype\": \"float16\",\n",
        "    \n",
        "    # M-GRPO Core\n",
        "    \"num_policy_samples\": 4,      # Samples from policy model\n",
        "    \"num_momentum_samples\": 4,    # Samples from momentum model  \n",
        "    \"momentum\": 0.99,             # EMA coefficient\n",
        "    \"beta\": 0.04,                 # KL penalty (if used)\n",
        "    \n",
        "    # Entropy Control\n",
        "    \"use_iqr_filter\": True,       # IQR-based low entropy filtering\n",
        "    \"iqr_k\": 0.75,                # IQR multiplier\n",
        "    \"min_entropy_threshold\": 0.1, # Absolute minimum entropy\n",
        "    \n",
        "    # Training\n",
        "    \"num_steps\": 20,              # Training steps\n",
        "    \"batch_size\": 4,              # Problems per step\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"temperature\": 0.7,\n",
        "    \n",
        "    # LoRA\n",
        "    \"lora_r\": 16,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    \n",
        "    # Problems\n",
        "    \"problem_types\": [\"rpn\", \"parentheses\", \"fibonacci\", \"binary_search\", \"edit_distance\", \"coin_change\"],\n",
        "    \"train_per_type\": 10,\n",
        "    \"val_per_type\": 5,\n",
        "    \"test_cases_per_problem\": 5,\n",
        "    \"difficulty_range\": [4, 7],\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## PART 2: PROBLEM GENERATORS (Self-Contained)\n",
        "\n",
        "These generators create coding problems with test cases.\n",
        "Each problem requires writing a Python function.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Any, Optional\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "@dataclass\n",
        "class TestCase:\n",
        "    \"\"\"A single test case for a problem.\"\"\"\n",
        "    input_args: Any\n",
        "    expected_output: Any\n",
        "\n",
        "@dataclass \n",
        "class AlgorithmicProblem:\n",
        "    \"\"\"A coding problem with test cases.\"\"\"\n",
        "    problem_id: str\n",
        "    title: str\n",
        "    description: str\n",
        "    function_name: str\n",
        "    function_signature: str\n",
        "    test_cases: List[TestCase]\n",
        "    difficulty: int = 5\n",
        "    \n",
        "    def to_prompt(self) -> str:\n",
        "        \"\"\"Convert to LLM prompt.\"\"\"\n",
        "        return f\"\"\"Write a Python function to solve the following problem.\n",
        "\n",
        "## Problem: {self.title}\n",
        "\n",
        "{self.description}\n",
        "\n",
        "## Function Signature\n",
        "```python\n",
        "{self.function_signature}\n",
        "```\n",
        "\n",
        "## Requirements\n",
        "- Implement the function exactly as specified\n",
        "- Handle edge cases appropriately\n",
        "- Return the correct type\n",
        "\n",
        "## Your Solution\n",
        "\"\"\"\n",
        "\n",
        "class AlgorithmicGenerator(ABC):\n",
        "    \"\"\"Base class for problem generators.\"\"\"\n",
        "    \n",
        "    def __init__(self, seed: int = None):\n",
        "        self.rng = random.Random(seed)\n",
        "    \n",
        "    @abstractmethod\n",
        "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> AlgorithmicProblem:\n",
        "        pass\n",
        "\n",
        "print(\"Base classes defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RPN Evaluator Generator\n",
        "class RPNEvaluatorGenerator(AlgorithmicGenerator):\n",
        "    \"\"\"Generate RPN (Reverse Polish Notation) evaluation problems.\"\"\"\n",
        "    \n",
        "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> AlgorithmicProblem:\n",
        "        test_cases = []\n",
        "        for _ in range(num_test_cases):\n",
        "            # Generate RPN expression\n",
        "            num_ops = min(difficulty, 5)\n",
        "            tokens = []\n",
        "            stack_size = 0\n",
        "            \n",
        "            for i in range(num_ops * 2 + 1):\n",
        "                if stack_size < 2 or (self.rng.random() < 0.6 and i < num_ops * 2 - 1):\n",
        "                    tokens.append(str(self.rng.randint(1, 20)))\n",
        "                    stack_size += 1\n",
        "                else:\n",
        "                    op = self.rng.choice(['+', '-', '*'])\n",
        "                    tokens.append(op)\n",
        "                    stack_size -= 1\n",
        "            \n",
        "            # Add remaining operators\n",
        "            while stack_size > 1:\n",
        "                tokens.append(self.rng.choice(['+', '-', '*']))\n",
        "                stack_size -= 1\n",
        "            \n",
        "            # Evaluate\n",
        "            stack = []\n",
        "            for t in tokens:\n",
        "                if t in ['+', '-', '*']:\n",
        "                    b, a = stack.pop(), stack.pop()\n",
        "                    if t == '+': stack.append(a + b)\n",
        "                    elif t == '-': stack.append(a - b)\n",
        "                    else: stack.append(a * b)\n",
        "                else:\n",
        "                    stack.append(int(t))\n",
        "            \n",
        "            test_cases.append(TestCase(input_args=[tokens], expected_output=stack[0]))\n",
        "        \n",
        "        return AlgorithmicProblem(\n",
        "            problem_id=f\"rpn_{self.rng.randint(1000, 9999)}\",\n",
        "            title=\"RPN Expression Evaluator\",\n",
        "            description=\"\"\"Evaluate a Reverse Polish Notation (RPN) expression.\n",
        "\n",
        "RPN is a mathematical notation where operators follow their operands.\n",
        "For example: [\"2\", \"3\", \"+\"] = 5, [\"4\", \"2\", \"*\", \"3\", \"+\"] = 11\n",
        "\n",
        "Supported operators: +, -, *\n",
        "All operands are integers.\"\"\",\n",
        "            function_name=\"evaluate_rpn\",\n",
        "            function_signature=\"def evaluate_rpn(tokens: List[str]) -> int:\",\n",
        "            test_cases=test_cases,\n",
        "            difficulty=difficulty,\n",
        "        )\n",
        "\n",
        "print(\"RPNEvaluatorGenerator defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parentheses Validator Generator\n",
        "class ParenthesesValidatorGenerator(AlgorithmicGenerator):\n",
        "    \"\"\"Generate parentheses validation problems.\"\"\"\n",
        "    \n",
        "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> AlgorithmicProblem:\n",
        "        test_cases = []\n",
        "        brackets = {'(': ')', '[': ']', '{': '}'}\n",
        "        \n",
        "        for i in range(num_test_cases):\n",
        "            length = difficulty * 2\n",
        "            \n",
        "            if i < num_test_cases // 2:  # Valid cases\n",
        "                s = \"\"\n",
        "                stack = []\n",
        "                for _ in range(length // 2):\n",
        "                    if not stack or self.rng.random() < 0.6:\n",
        "                        open_b = self.rng.choice(list(brackets.keys()))\n",
        "                        s += open_b\n",
        "                        stack.append(open_b)\n",
        "                    else:\n",
        "                        s += brackets[stack.pop()]\n",
        "                while stack:\n",
        "                    s += brackets[stack.pop()]\n",
        "                expected = True\n",
        "            else:  # Invalid cases\n",
        "                all_brackets = list(brackets.keys()) + list(brackets.values())\n",
        "                s = ''.join(self.rng.choices(all_brackets, k=length))\n",
        "                # Verify it's actually invalid\n",
        "                stack = []\n",
        "                valid = True\n",
        "                for c in s:\n",
        "                    if c in brackets:\n",
        "                        stack.append(c)\n",
        "                    elif c in brackets.values():\n",
        "                        if not stack:\n",
        "                            valid = False\n",
        "                            break\n",
        "                        if brackets.get(stack.pop()) != c:\n",
        "                            valid = False\n",
        "                            break\n",
        "                if stack:\n",
        "                    valid = False\n",
        "                expected = valid\n",
        "            \n",
        "            test_cases.append(TestCase(input_args=[s], expected_output=expected))\n",
        "        \n",
        "        return AlgorithmicProblem(\n",
        "            problem_id=f\"paren_{self.rng.randint(1000, 9999)}\",\n",
        "            title=\"Valid Parentheses\",\n",
        "            description=\"\"\"Check if a string of brackets is valid.\n",
        "\n",
        "A string is valid if:\n",
        "- Open brackets are closed by the same type of brackets\n",
        "- Open brackets are closed in the correct order\n",
        "- Every close bracket has a corresponding open bracket\n",
        "\n",
        "Bracket types: (), [], {}\"\"\",\n",
        "            function_name=\"is_valid\",\n",
        "            function_signature=\"def is_valid(s: str) -> bool:\",\n",
        "            test_cases=test_cases,\n",
        "            difficulty=difficulty,\n",
        "        )\n",
        "\n",
        "print(\"ParenthesesValidatorGenerator defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fibonacci Generator\n",
        "class FibonacciGenerator(AlgorithmicGenerator):\n",
        "    \"\"\"Generate Fibonacci sequence problems.\"\"\"\n",
        "    \n",
        "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> AlgorithmicProblem:\n",
        "        test_cases = []\n",
        "        max_n = difficulty * 5\n",
        "        \n",
        "        # Precompute fibonacci\n",
        "        fib = [0, 1]\n",
        "        for i in range(2, max_n + 1):\n",
        "            fib.append(fib[-1] + fib[-2])\n",
        "        \n",
        "        # Generate test cases\n",
        "        ns = [0, 1] + [self.rng.randint(2, max_n) for _ in range(num_test_cases - 2)]\n",
        "        self.rng.shuffle(ns)\n",
        "        \n",
        "        for n in ns[:num_test_cases]:\n",
        "            test_cases.append(TestCase(input_args=[n], expected_output=fib[n]))\n",
        "        \n",
        "        return AlgorithmicProblem(\n",
        "            problem_id=f\"fib_{self.rng.randint(1000, 9999)}\",\n",
        "            title=\"Fibonacci Number\",\n",
        "            description=\"\"\"Return the nth Fibonacci number.\n",
        "\n",
        "The Fibonacci sequence is defined as:\n",
        "- F(0) = 0\n",
        "- F(1) = 1  \n",
        "- F(n) = F(n-1) + F(n-2) for n > 1\n",
        "\n",
        "Examples: F(0)=0, F(1)=1, F(2)=1, F(3)=2, F(4)=3, F(5)=5, F(10)=55\"\"\",\n",
        "            function_name=\"fibonacci\",\n",
        "            function_signature=\"def fibonacci(n: int) -> int:\",\n",
        "            test_cases=test_cases,\n",
        "            difficulty=difficulty,\n",
        "        )\n",
        "\n",
        "print(\"FibonacciGenerator defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binary Search Generator\n",
        "class BinarySearchGenerator(AlgorithmicGenerator):\n",
        "    \"\"\"Generate binary search problems.\"\"\"\n",
        "    \n",
        "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> AlgorithmicProblem:\n",
        "        test_cases = []\n",
        "        \n",
        "        for i in range(num_test_cases):\n",
        "            size = difficulty * 3\n",
        "            arr = sorted(self.rng.sample(range(1, size * 3), size))\n",
        "            \n",
        "            if i < num_test_cases // 2:  # Target exists\n",
        "                idx = self.rng.randint(0, len(arr) - 1)\n",
        "                target = arr[idx]\n",
        "                expected = idx\n",
        "            else:  # Target doesn't exist\n",
        "                target = self.rng.choice([0, arr[-1] + 1, arr[0] - 1])\n",
        "                if target in arr:\n",
        "                    target = arr[-1] + self.rng.randint(1, 10)\n",
        "                expected = -1\n",
        "            \n",
        "            test_cases.append(TestCase(input_args=[arr, target], expected_output=expected))\n",
        "        \n",
        "        return AlgorithmicProblem(\n",
        "            problem_id=f\"bsearch_{self.rng.randint(1000, 9999)}\",\n",
        "            title=\"Binary Search\",\n",
        "            description=\"\"\"Find the index of target in a sorted array.\n",
        "\n",
        "Given a sorted array of integers and a target value, return the index\n",
        "of the target if found, otherwise return -1.\n",
        "\n",
        "You must implement binary search with O(log n) time complexity.\"\"\",\n",
        "            function_name=\"binary_search\",\n",
        "            function_signature=\"def binary_search(arr: List[int], target: int) -> int:\",\n",
        "            test_cases=test_cases,\n",
        "            difficulty=difficulty,\n",
        "        )\n",
        "\n",
        "print(\"BinarySearchGenerator defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Edit Distance Generator\n",
        "class EditDistanceGenerator(AlgorithmicGenerator):\n",
        "    \"\"\"Generate edit distance (Levenshtein) problems.\"\"\"\n",
        "    \n",
        "    def _edit_distance(self, s1: str, s2: str) -> int:\n",
        "        m, n = len(s1), len(s2)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "        \n",
        "        for i in range(m + 1):\n",
        "            dp[i][0] = i\n",
        "        for j in range(n + 1):\n",
        "            dp[0][j] = j\n",
        "        \n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if s1[i-1] == s2[j-1]:\n",
        "                    dp[i][j] = dp[i-1][j-1]\n",
        "                else:\n",
        "                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
        "        \n",
        "        return dp[m][n]\n",
        "    \n",
        "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> AlgorithmicProblem:\n",
        "        test_cases = []\n",
        "        chars = 'abcdefghij'\n",
        "        max_len = difficulty * 2\n",
        "        \n",
        "        for _ in range(num_test_cases):\n",
        "            len1 = self.rng.randint(1, max_len)\n",
        "            len2 = self.rng.randint(1, max_len)\n",
        "            s1 = ''.join(self.rng.choices(chars, k=len1))\n",
        "            s2 = ''.join(self.rng.choices(chars, k=len2))\n",
        "            \n",
        "            expected = self._edit_distance(s1, s2)\n",
        "            test_cases.append(TestCase(input_args=[s1, s2], expected_output=expected))\n",
        "        \n",
        "        return AlgorithmicProblem(\n",
        "            problem_id=f\"edit_{self.rng.randint(1000, 9999)}\",\n",
        "            title=\"Edit Distance\",\n",
        "            description=\"\"\"Calculate the minimum edit distance between two strings.\n",
        "\n",
        "The edit distance (Levenshtein distance) is the minimum number of\n",
        "single-character operations needed to transform one string into another.\n",
        "\n",
        "Allowed operations:\n",
        "- Insert a character\n",
        "- Delete a character\n",
        "- Replace a character\n",
        "\n",
        "Example: edit_distance(\"kitten\", \"sitting\") = 3\"\"\",\n",
        "            function_name=\"edit_distance\",\n",
        "            function_signature=\"def edit_distance(s1: str, s2: str) -> int:\",\n",
        "            test_cases=test_cases,\n",
        "            difficulty=difficulty,\n",
        "        )\n",
        "\n",
        "print(\"EditDistanceGenerator defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coin Change Generator\n",
        "class CoinChangeGenerator(AlgorithmicGenerator):\n",
        "    \"\"\"Generate coin change (minimum coins) problems.\"\"\"\n",
        "    \n",
        "    def _coin_change(self, coins: List[int], amount: int) -> int:\n",
        "        if amount == 0:\n",
        "            return 0\n",
        "        dp = [float('inf')] * (amount + 1)\n",
        "        dp[0] = 0\n",
        "        \n",
        "        for i in range(1, amount + 1):\n",
        "            for coin in coins:\n",
        "                if coin <= i and dp[i - coin] != float('inf'):\n",
        "                    dp[i] = min(dp[i], dp[i - coin] + 1)\n",
        "        \n",
        "        return dp[amount] if dp[amount] != float('inf') else -1\n",
        "    \n",
        "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> AlgorithmicProblem:\n",
        "        test_cases = []\n",
        "        \n",
        "        for _ in range(num_test_cases):\n",
        "            # Generate coin denominations\n",
        "            num_coins = self.rng.randint(2, min(difficulty, 5))\n",
        "            coins = sorted(list(set([1] + [self.rng.randint(2, difficulty * 3) for _ in range(num_coins - 1)])))\n",
        "            \n",
        "            # Generate amount\n",
        "            amount = self.rng.randint(1, difficulty * 10)\n",
        "            expected = self._coin_change(coins, amount)\n",
        "            \n",
        "            test_cases.append(TestCase(input_args=[coins, amount], expected_output=expected))\n",
        "        \n",
        "        return AlgorithmicProblem(\n",
        "            problem_id=f\"coins_{self.rng.randint(1000, 9999)}\",\n",
        "            title=\"Coin Change\",\n",
        "            description=\"\"\"Find the minimum number of coins to make up an amount.\n",
        "\n",
        "Given an array of coin denominations and a target amount, return the\n",
        "fewest number of coins needed to make up that amount.\n",
        "\n",
        "If the amount cannot be made up, return -1.\n",
        "\n",
        "You have an infinite supply of each coin denomination.\n",
        "\n",
        "Example: coins=[1,2,5], amount=11 -> 3 (5+5+1)\"\"\",\n",
        "            function_name=\"coin_change\",\n",
        "            function_signature=\"def coin_change(coins: List[int], amount: int) -> int:\",\n",
        "            test_cases=test_cases,\n",
        "            difficulty=difficulty,\n",
        "        )\n",
        "\n",
        "print(\"CoinChangeGenerator defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generator Registry\n",
        "GENERATORS = {\n",
        "    \"rpn\": RPNEvaluatorGenerator,\n",
        "    \"parentheses\": ParenthesesValidatorGenerator,\n",
        "    \"fibonacci\": FibonacciGenerator,\n",
        "    \"binary_search\": BinarySearchGenerator,\n",
        "    \"edit_distance\": EditDistanceGenerator,\n",
        "    \"coin_change\": CoinChangeGenerator,\n",
        "}\n",
        "\n",
        "def generate_problems(config, seed=42):\n",
        "    \"\"\"Generate train and validation problem sets.\"\"\"\n",
        "    rng = random.Random(seed)\n",
        "    train_problems, val_problems = [], []\n",
        "    diff_min, diff_max = config[\"difficulty_range\"]\n",
        "    \n",
        "    for prob_type in config[\"problem_types\"]:\n",
        "        if prob_type not in GENERATORS:\n",
        "            print(f\"Warning: Unknown problem type '{prob_type}'\")\n",
        "            continue\n",
        "        \n",
        "        gen = GENERATORS[prob_type](seed=rng.randint(0, 1000000))\n",
        "        \n",
        "        # Generate training problems\n",
        "        for _ in range(config[\"train_per_type\"]):\n",
        "            diff = rng.randint(diff_min, diff_max)\n",
        "            train_problems.append(gen.generate(\n",
        "                difficulty=diff,\n",
        "                num_test_cases=config[\"test_cases_per_problem\"]\n",
        "            ))\n",
        "        \n",
        "        # Generate validation problems\n",
        "        for _ in range(config[\"val_per_type\"]):\n",
        "            diff = rng.randint(diff_min, diff_max)\n",
        "            val_problems.append(gen.generate(\n",
        "                difficulty=diff,\n",
        "                num_test_cases=config[\"test_cases_per_problem\"]\n",
        "            ))\n",
        "    \n",
        "    rng.shuffle(train_problems)\n",
        "    rng.shuffle(val_problems)\n",
        "    return train_problems, val_problems\n",
        "\n",
        "# Test problem generation\n",
        "print(\"\\nTesting problem generation...\")\n",
        "train_problems, val_problems = generate_problems(CONFIG)\n",
        "print(f\"Generated {len(train_problems)} training problems\")\n",
        "print(f\"Generated {len(val_problems)} validation problems\")\n",
        "print(f\"\\nSample problem:\")\n",
        "print(train_problems[0].to_prompt()[:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## PART 3: CODE VERIFICATION (Self-Contained)\n",
        "\n",
        "The verifier executes generated code against test cases.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import tempfile\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "def extract_code(completion: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract Python code from model completion.\n",
        "    \n",
        "    Handles:\n",
        "    1. ```python ... ``` blocks\n",
        "    2. ``` ... ``` blocks  \n",
        "    3. Raw code with 'def' statements\n",
        "    \"\"\"\n",
        "    # Try to find ```python ... ``` blocks\n",
        "    python_blocks = re.findall(r'```python\\s*(.*?)```', completion, re.DOTALL)\n",
        "    if python_blocks:\n",
        "        for block in sorted(python_blocks, key=len, reverse=True):\n",
        "            if 'def ' in block:\n",
        "                return block.strip()\n",
        "        return python_blocks[0].strip()\n",
        "    \n",
        "    # Try to find ``` ... ``` blocks\n",
        "    code_blocks = re.findall(r'```\\s*(.*?)```', completion, re.DOTALL)\n",
        "    if code_blocks:\n",
        "        for block in sorted(code_blocks, key=len, reverse=True):\n",
        "            if 'def ' in block:\n",
        "                return block.strip()\n",
        "        return code_blocks[0].strip()\n",
        "    \n",
        "    # Extract function definition directly\n",
        "    if 'def ' in completion:\n",
        "        lines = completion.split('\\n')\n",
        "        code_lines = []\n",
        "        in_function = False\n",
        "        indent_level = None\n",
        "        \n",
        "        for line in lines:\n",
        "            stripped = line.lstrip()\n",
        "            if stripped.startswith('def '):\n",
        "                in_function = True\n",
        "                indent_level = len(line) - len(stripped)\n",
        "                code_lines = [line]\n",
        "            elif in_function:\n",
        "                if stripped and not stripped.startswith('#'):\n",
        "                    current_indent = len(line) - len(stripped)\n",
        "                    if current_indent <= indent_level and stripped:\n",
        "                        break\n",
        "                code_lines.append(line)\n",
        "        \n",
        "        if code_lines:\n",
        "            return '\\n'.join(code_lines).strip()\n",
        "    \n",
        "    return completion.strip()\n",
        "\n",
        "\n",
        "def verify_solution(code: str, problem: AlgorithmicProblem, timeout: float = 5.0) -> dict:\n",
        "    \"\"\"\n",
        "    Verify a solution against test cases.\n",
        "    \n",
        "    Returns:\n",
        "        dict with 'passed', 'passed_count', 'total_count', 'error'\n",
        "    \"\"\"\n",
        "    # Build test script\n",
        "    test_cases_json = json.dumps([\n",
        "        {\"input\": tc.input_args, \"expected\": tc.expected_output}\n",
        "        for tc in problem.test_cases\n",
        "    ])\n",
        "    \n",
        "    test_script = f'''# -*- coding: utf-8 -*-\n",
        "import json\n",
        "from typing import List, Optional, Tuple, Dict, Any\n",
        "\n",
        "# === SOLUTION CODE ===\n",
        "{code}\n",
        "# === END SOLUTION ===\n",
        "\n",
        "def run_tests():\n",
        "    test_cases = json.loads(\\'{test_cases_json}\\')\n",
        "    results = []\n",
        "    \n",
        "    for i, tc in enumerate(test_cases):\n",
        "        inp = tc[\"input\"]\n",
        "        expected = tc[\"expected\"]\n",
        "        \n",
        "        try:\n",
        "            if isinstance(inp, list):\n",
        "                actual = {problem.function_name}(*inp)\n",
        "            else:\n",
        "                actual = {problem.function_name}(inp)\n",
        "            passed = actual == expected\n",
        "            results.append({{\"passed\": passed, \"error\": None}})\n",
        "        except Exception as e:\n",
        "            results.append({{\"passed\": False, \"error\": str(e)}})\n",
        "    \n",
        "    print(json.dumps({{\"results\": results, \"success\": True}}))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        run_tests()\n",
        "    except Exception as e:\n",
        "        print(json.dumps({{\"results\": [], \"success\": False, \"error\": str(e)}}))\n",
        "'''\n",
        "    \n",
        "    # Execute in subprocess\n",
        "    try:\n",
        "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
        "            f.write(test_script)\n",
        "            script_path = f.name\n",
        "        \n",
        "        result = subprocess.run(\n",
        "            ['python', script_path],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=timeout,\n",
        "        )\n",
        "        \n",
        "        os.unlink(script_path)\n",
        "        \n",
        "        if result.stdout.strip():\n",
        "            data = json.loads(result.stdout.strip())\n",
        "            if data.get(\"success\"):\n",
        "                passed_count = sum(1 for r in data[\"results\"] if r[\"passed\"])\n",
        "                total_count = len(data[\"results\"])\n",
        "                return {\n",
        "                    \"passed\": passed_count == total_count,\n",
        "                    \"passed_count\": passed_count,\n",
        "                    \"total_count\": total_count,\n",
        "                    \"error\": None,\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    \"passed\": False,\n",
        "                    \"passed_count\": 0,\n",
        "                    \"total_count\": len(problem.test_cases),\n",
        "                    \"error\": data.get(\"error\", \"Unknown error\"),\n",
        "                }\n",
        "        else:\n",
        "            return {\n",
        "                \"passed\": False,\n",
        "                \"passed_count\": 0,\n",
        "                \"total_count\": len(problem.test_cases),\n",
        "                \"error\": result.stderr or \"No output\",\n",
        "            }\n",
        "    \n",
        "    except subprocess.TimeoutExpired:\n",
        "        return {\n",
        "            \"passed\": False,\n",
        "            \"passed_count\": 0,\n",
        "            \"total_count\": len(problem.test_cases),\n",
        "            \"error\": \"Timeout\",\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"passed\": False,\n",
        "            \"passed_count\": 0,\n",
        "            \"total_count\": len(problem.test_cases),\n",
        "            \"error\": str(e),\n",
        "        }\n",
        "\n",
        "\n",
        "# Test the verifier\n",
        "print(\"Testing verifier...\")\n",
        "test_problem = train_problems[0]\n",
        "print(f\"Problem: {test_problem.title}\")\n",
        "\n",
        "# Test with a correct solution\n",
        "if test_problem.function_name == \"fibonacci\":\n",
        "    test_code = \"\"\"def fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    a, b = 0, 1\n",
        "    for _ in range(2, n + 1):\n",
        "        a, b = b, a + b\n",
        "    return b\"\"\"\n",
        "    result = verify_solution(test_code, test_problem)\n",
        "    print(f\"Fibonacci test: {result}\")\n",
        "else:\n",
        "    print(f\"First problem is {test_problem.function_name}, skipping built-in test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## PART 4: LOAD MODEL AND CREATE TRAINER\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"\\nLoading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CONFIG[\"model_name\"],\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "print(f\"Tokenizer loaded: vocab_size={tokenizer.vocab_size}\")\n",
        "\n",
        "# Load policy model with LoRA\n",
        "print(\"\\nLoading policy model...\")\n",
        "policy_model = AutoModelForCausalLM.from_pretrained(\n",
        "    CONFIG[\"model_name\"],\n",
        "    torch_dtype=DTYPE,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=CONFIG[\"lora_r\"],\n",
        "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
        "    target_modules=CONFIG[\"target_modules\"],\n",
        "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "policy_model = get_peft_model(policy_model, lora_config)\n",
        "policy_model.print_trainable_parameters()\n",
        "\n",
        "# Create momentum model (EMA copy)\n",
        "print(\"\\nCreating momentum model (EMA copy)...\")\n",
        "momentum_model = copy.deepcopy(policy_model)\n",
        "momentum_model.eval()\n",
        "for p in momentum_model.parameters():\n",
        "    p.requires_grad = False\n",
        "print(\"Momentum model created (frozen)\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    policy_model.parameters(),\n",
        "    lr=CONFIG[\"learning_rate\"],\n",
        ")\n",
        "\n",
        "print(\"\\nSetup complete!\")\n",
        "print(f\"  Policy model: trainable\")\n",
        "print(f\"  Momentum model: frozen (EMA)\")\n",
        "print(f\"  Optimizer: AdamW, lr={CONFIG['learning_rate']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## PART 5: M-GRPO TRAINING UTILITIES\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "def update_momentum(policy_model, momentum_model, m: float = 0.99):\n",
        "    \"\"\"\n",
        "    EMA update for momentum model.\n",
        "    θ_momentum = m * θ_momentum + (1 - m) * θ_policy\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        for p_m, p_p in zip(momentum_model.parameters(), policy_model.parameters()):\n",
        "            p_m.data.mul_(m).add_(p_p.data, alpha=1 - m)\n",
        "\n",
        "\n",
        "def generate_samples(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt: str,\n",
        "    num_samples: int = 4,\n",
        "    max_new_tokens: int = 512,\n",
        "    temperature: float = 0.7,\n",
        ") -> List[str]:\n",
        "    \"\"\"Generate multiple samples from a model.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    inputs = tokenizer(\n",
        "        [prompt] * num_samples,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "    ).to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    input_len = inputs.input_ids.shape[1]\n",
        "    completions = tokenizer.batch_decode(\n",
        "        outputs[:, input_len:],\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    \n",
        "    return completions\n",
        "\n",
        "\n",
        "def compute_entropy(logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Compute per-token entropy from logits.\"\"\"\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    entropy = -(probs * log_probs).sum(dim=-1)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def iqr_filter(entropies: List[float], k: float = 0.75) -> List[bool]:\n",
        "    \"\"\"\n",
        "    IQR-based filtering for low-entropy samples.\n",
        "    Returns mask where True = keep, False = filter out.\n",
        "    \"\"\"\n",
        "    arr = np.array(entropies)\n",
        "    Q1 = np.percentile(arr, 25)\n",
        "    Q3 = np.percentile(arr, 75)\n",
        "    threshold = Q1 - k * (Q3 - Q1)\n",
        "    threshold = max(threshold, 0.1)  # Minimum threshold\n",
        "    return (arr >= threshold).tolist()\n",
        "\n",
        "\n",
        "def compute_advantages(rewards: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Compute standardized advantages.\"\"\"\n",
        "    mean = rewards.mean()\n",
        "    std = rewards.std() + 1e-8\n",
        "    return (rewards - mean) / std\n",
        "\n",
        "\n",
        "print(\"Training utilities defined:\")\n",
        "print(\"  - update_momentum(): EMA update for momentum model\")\n",
        "print(\"  - generate_samples(): Generate completions from model\")\n",
        "print(\"  - compute_entropy(): Per-token entropy calculation\")\n",
        "print(\"  - iqr_filter(): IQR-based low-entropy filtering\")\n",
        "print(\"  - compute_advantages(): Advantage normalization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test generation\n",
        "print(\"Testing generation...\")\n",
        "test_prompt = train_problems[0].to_prompt()\n",
        "test_completions = generate_samples(\n",
        "    policy_model,\n",
        "    tokenizer,\n",
        "    test_prompt,\n",
        "    num_samples=2,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "print(f\"Generated {len(test_completions)} completions\")\n",
        "print(f\"\\nFirst completion (first 300 chars):\")\n",
        "print(test_completions[0][:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## PART 6: M-GRPO TRAINING LOOP\n",
        "\n",
        "The main training loop implementing:\n",
        "1. Combined rollout (policy + momentum samples)\n",
        "2. Reward computation with partial credit\n",
        "3. Policy gradient updates\n",
        "4. Momentum model EMA update\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Metrics tracking\n",
        "metrics_history = {\n",
        "    \"step\": [],\n",
        "    \"loss\": [],\n",
        "    \"mean_reward\": [],\n",
        "    \"mean_entropy\": [],\n",
        "    \"success_rate\": [],\n",
        "    \"val_accuracy\": [],\n",
        "    \"filtered_count\": [],\n",
        "}\n",
        "\n",
        "# Create prompt -> problem mapping for quick lookup\n",
        "prompt_to_problem = {p.to_prompt(): p for p in train_problems}\n",
        "train_prompts = list(prompt_to_problem.keys())\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"M-GRPO TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Steps: {CONFIG['num_steps']}\")\n",
        "print(f\"Batch size: {CONFIG['batch_size']} problems\")\n",
        "print(f\"Samples per problem: {CONFIG['num_policy_samples']} policy + {CONFIG['num_momentum_samples']} momentum\")\n",
        "print(f\"Momentum coefficient: {CONFIG['momentum']}\")\n",
        "print(f\"IQR filter: {CONFIG['use_iqr_filter']}\")\n",
        "print(f\"\\nTraining problems: {len(train_problems)}\")\n",
        "print(f\"Validation problems: {len(val_problems)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main training loop\n",
        "train_start_time = time.time()\n",
        "\n",
        "for step in range(CONFIG[\"num_steps\"]):\n",
        "    step_start = time.time()\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Step {step}/{CONFIG['num_steps']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Sample batch of prompts\n",
        "    batch_prompts = random.sample(train_prompts, min(CONFIG[\"batch_size\"], len(train_prompts)))\n",
        "    \n",
        "    step_loss = 0.0\n",
        "    step_reward = 0.0\n",
        "    step_entropy = 0.0\n",
        "    step_successes = 0\n",
        "    step_updates = 0\n",
        "    step_filtered = 0\n",
        "    \n",
        "    for prompt_idx, prompt in enumerate(batch_prompts):\n",
        "        problem = prompt_to_problem[prompt]\n",
        "        print(f\"  [{prompt_idx+1}/{len(batch_prompts)}] {problem.title}...\", end=\" \")\n",
        "        \n",
        "        # 1. Combined rollout: generate from both models\n",
        "        policy_gens = generate_samples(\n",
        "            policy_model, tokenizer, prompt,\n",
        "            num_samples=CONFIG[\"num_policy_samples\"],\n",
        "            max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
        "            temperature=CONFIG[\"temperature\"],\n",
        "        )\n",
        "        \n",
        "        momentum_gens = generate_samples(\n",
        "            momentum_model, tokenizer, prompt,\n",
        "            num_samples=CONFIG[\"num_momentum_samples\"],\n",
        "            max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
        "            temperature=CONFIG[\"temperature\"],\n",
        "        )\n",
        "        \n",
        "        # 2. Compute rewards for all generations (partial credit)\n",
        "        all_gens = policy_gens + momentum_gens\n",
        "        all_rewards = []\n",
        "        \n",
        "        for gen in all_gens:\n",
        "            code = extract_code(gen)\n",
        "            result = verify_solution(code, problem)\n",
        "            # Partial reward: proportion of tests passed\n",
        "            reward = result[\"passed_count\"] / max(result[\"total_count\"], 1)\n",
        "            all_rewards.append(reward)\n",
        "        \n",
        "        policy_rewards = torch.tensor(all_rewards[:len(policy_gens)])\n",
        "        max_reward = max(all_rewards)\n",
        "        \n",
        "        if max_reward > 0:\n",
        "            step_successes += 1\n",
        "        \n",
        "        # 3. Train on policy samples with positive advantage\n",
        "        advantages = compute_advantages(policy_rewards)\n",
        "        \n",
        "        for gen_idx, (gen, adv, rew) in enumerate(zip(policy_gens, advantages, policy_rewards)):\n",
        "            # Only update on positive advantage\n",
        "            if adv <= 0:\n",
        "                continue\n",
        "            \n",
        "            # Tokenize\n",
        "            full_text = prompt + gen\n",
        "            inputs = tokenizer(\n",
        "                full_text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=2048,\n",
        "            ).to(policy_model.device)\n",
        "            \n",
        "            prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
        "            prompt_len = len(prompt_tokens)\n",
        "            \n",
        "            # Forward pass\n",
        "            policy_model.train()\n",
        "            outputs = policy_model(**inputs)\n",
        "            \n",
        "            # Get logits for completion tokens only\n",
        "            logits = outputs.logits[:, prompt_len-1:-1, :]\n",
        "            labels = inputs.input_ids[:, prompt_len:]\n",
        "            \n",
        "            if labels.shape[1] == 0:\n",
        "                continue\n",
        "            \n",
        "            # Compute entropy for this generation\n",
        "            entropy = compute_entropy(logits).mean().item()\n",
        "            step_entropy += entropy\n",
        "            \n",
        "            # IQR filter check (optional)\n",
        "            if CONFIG[\"use_iqr_filter\"] and entropy < CONFIG[\"min_entropy_threshold\"]:\n",
        "                step_filtered += 1\n",
        "                continue\n",
        "            \n",
        "            # Policy gradient loss\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            token_log_probs = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n",
        "            pg_loss = -(token_log_probs.mean() * adv)\n",
        "            \n",
        "            # Backward\n",
        "            optimizer.zero_grad()\n",
        "            pg_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            step_loss += pg_loss.item()\n",
        "            step_reward += rew.item()\n",
        "            step_updates += 1\n",
        "        \n",
        "        print(f\"reward={max_reward:.2f}\")\n",
        "    \n",
        "    # 4. Update momentum model via EMA\n",
        "    update_momentum(policy_model, momentum_model, CONFIG[\"momentum\"])\n",
        "    \n",
        "    # 5. Compute step metrics\n",
        "    num_samples = len(batch_prompts) * CONFIG[\"num_policy_samples\"]\n",
        "    avg_loss = step_loss / max(step_updates, 1)\n",
        "    avg_reward = step_reward / max(step_updates, 1)\n",
        "    avg_entropy = step_entropy / max(step_updates, 1)\n",
        "    success_rate = step_successes / len(batch_prompts)\n",
        "    \n",
        "    # 6. Quick validation\n",
        "    val_correct = 0\n",
        "    val_subset = random.sample(val_problems, min(5, len(val_problems)))\n",
        "    for vp in val_subset:\n",
        "        gens = generate_samples(policy_model, tokenizer, vp.to_prompt(), num_samples=1, max_new_tokens=512)\n",
        "        code = extract_code(gens[0])\n",
        "        result = verify_solution(code, vp)\n",
        "        if result[\"passed\"]:\n",
        "            val_correct += 1\n",
        "    val_accuracy = val_correct / len(val_subset)\n",
        "    \n",
        "    # Log metrics\n",
        "    metrics_history[\"step\"].append(step)\n",
        "    metrics_history[\"loss\"].append(avg_loss)\n",
        "    metrics_history[\"mean_reward\"].append(avg_reward)\n",
        "    metrics_history[\"mean_entropy\"].append(avg_entropy)\n",
        "    metrics_history[\"success_rate\"].append(success_rate)\n",
        "    metrics_history[\"val_accuracy\"].append(val_accuracy)\n",
        "    metrics_history[\"filtered_count\"].append(step_filtered)\n",
        "    \n",
        "    # Print summary\n",
        "    step_time = time.time() - step_start\n",
        "    elapsed = time.time() - train_start_time\n",
        "    eta = (elapsed / (step + 1)) * (CONFIG[\"num_steps\"] - step - 1)\n",
        "    \n",
        "    print(f\"\\nStep {step} Summary:\")\n",
        "    print(f\"  Loss: {avg_loss:.4f}\")\n",
        "    print(f\"  Reward: {avg_reward:.3f}\")\n",
        "    print(f\"  Entropy: {avg_entropy:.3f}\")\n",
        "    print(f\"  Success Rate: {success_rate:.1%}\")\n",
        "    print(f\"  Val Accuracy: {val_accuracy:.1%}\")\n",
        "    print(f\"  Updates: {step_updates}, Filtered: {step_filtered}\")\n",
        "    print(f\"  Time: {step_time:.1f}s, ETA: {eta/60:.1f}m\")\n",
        "    \n",
        "    # Check for entropy collapse\n",
        "    if avg_entropy < 0.1:\n",
        "        print(f\"  ⚠️ WARNING: Low entropy ({avg_entropy:.3f}) - possible collapse!\")\n",
        "    \n",
        "    # Clear memory\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "total_time = time.time() - train_start_time\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"TRAINING COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
        "print(f\"Final metrics:\")\n",
        "print(f\"  Loss: {metrics_history['loss'][-1]:.4f}\")\n",
        "print(f\"  Reward: {metrics_history['mean_reward'][-1]:.3f}\")\n",
        "print(f\"  Entropy: {metrics_history['mean_entropy'][-1]:.3f}\")\n",
        "print(f\"  Val Accuracy: {metrics_history['val_accuracy'][-1]:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## PART 7: RESULTS VISUALIZATION\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "\n",
        "# Loss\n",
        "axes[0, 0].plot(metrics_history[\"step\"], metrics_history[\"loss\"], 'b-o', markersize=4)\n",
        "axes[0, 0].set_xlabel(\"Step\")\n",
        "axes[0, 0].set_ylabel(\"Loss\")\n",
        "axes[0, 0].set_title(\"Training Loss\")\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Reward\n",
        "axes[0, 1].plot(metrics_history[\"step\"], metrics_history[\"mean_reward\"], 'g-o', markersize=4)\n",
        "axes[0, 1].set_xlabel(\"Step\")\n",
        "axes[0, 1].set_ylabel(\"Mean Reward\")\n",
        "axes[0, 1].set_title(\"Average Reward\")\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Entropy (critical for M-GRPO)\n",
        "axes[0, 2].plot(metrics_history[\"step\"], metrics_history[\"mean_entropy\"], 'r-o', markersize=4)\n",
        "axes[0, 2].axhline(y=0.1, color='orange', linestyle='--', label='Collapse threshold')\n",
        "axes[0, 2].set_xlabel(\"Step\")\n",
        "axes[0, 2].set_ylabel(\"Mean Entropy\")\n",
        "axes[0, 2].set_title(\"Policy Entropy (should stay above 0.1)\")\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Success Rate\n",
        "axes[1, 0].plot(metrics_history[\"step\"], [s*100 for s in metrics_history[\"success_rate\"]], 'm-o', markersize=4)\n",
        "axes[1, 0].set_xlabel(\"Step\")\n",
        "axes[1, 0].set_ylabel(\"Success Rate (%)\")\n",
        "axes[1, 0].set_title(\"Training Success Rate\")\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Validation Accuracy\n",
        "axes[1, 1].plot(metrics_history[\"step\"], [v*100 for v in metrics_history[\"val_accuracy\"]], 'c-o', markersize=4)\n",
        "axes[1, 1].set_xlabel(\"Step\")\n",
        "axes[1, 1].set_ylabel(\"Accuracy (%)\")\n",
        "axes[1, 1].set_title(\"Validation Accuracy\")\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Filtered samples\n",
        "axes[1, 2].bar(metrics_history[\"step\"], metrics_history[\"filtered_count\"], color='orange', alpha=0.7)\n",
        "axes[1, 2].set_xlabel(\"Step\")\n",
        "axes[1, 2].set_ylabel(\"Count\")\n",
        "axes[1, 2].set_title(\"IQR Filtered Samples (low entropy)\")\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"mgrpo_training_results.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nResults saved to mgrpo_training_results.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## PART 8: SAVE MODEL AND RESULTS\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(\"mgrpo_output\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save policy model\n",
        "print(\"Saving policy model...\")\n",
        "policy_model.save_pretrained(output_dir / \"policy\")\n",
        "tokenizer.save_pretrained(output_dir / \"policy\")\n",
        "\n",
        "# Save momentum model\n",
        "print(\"Saving momentum model...\")\n",
        "momentum_model.save_pretrained(output_dir / \"momentum\")\n",
        "\n",
        "# Save metrics\n",
        "print(\"Saving metrics...\")\n",
        "with open(output_dir / \"metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics_history, f, indent=2)\n",
        "\n",
        "# Save config\n",
        "print(\"Saving config...\")\n",
        "with open(output_dir / \"config.json\", \"w\") as f:\n",
        "    # Convert non-serializable types\n",
        "    config_save = {k: str(v) if not isinstance(v, (int, float, str, list, dict, bool)) else v \n",
        "                   for k, v in CONFIG.items()}\n",
        "    json.dump(config_save, f, indent=2)\n",
        "\n",
        "# Save summary\n",
        "summary = {\n",
        "    \"model\": CONFIG[\"model_name\"],\n",
        "    \"num_steps\": CONFIG[\"num_steps\"],\n",
        "    \"final_loss\": metrics_history[\"loss\"][-1],\n",
        "    \"final_reward\": metrics_history[\"mean_reward\"][-1],\n",
        "    \"final_entropy\": metrics_history[\"mean_entropy\"][-1],\n",
        "    \"final_val_accuracy\": metrics_history[\"val_accuracy\"][-1],\n",
        "    \"total_time_minutes\": total_time / 60,\n",
        "}\n",
        "with open(output_dir / \"summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nAll files saved to {output_dir}/\")\n",
        "print(f\"  - policy/: Trained policy model\")\n",
        "print(f\"  - momentum/: Momentum model (EMA)\")\n",
        "print(f\"  - metrics.json: Training metrics\")\n",
        "print(f\"  - config.json: Configuration\")\n",
        "print(f\"  - summary.json: Final summary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## PART 9: FINAL EVALUATION\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"FINAL EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Evaluate on all validation problems\n",
        "val_results = {}\n",
        "total_correct = 0\n",
        "total_problems = len(val_problems)\n",
        "\n",
        "for prob_type in CONFIG[\"problem_types\"]:\n",
        "    val_results[prob_type] = {\"correct\": 0, \"total\": 0}\n",
        "\n",
        "print(\"\\nEvaluating on validation set...\")\n",
        "for vp in val_problems:\n",
        "    # Generate solution\n",
        "    gens = generate_samples(\n",
        "        policy_model, tokenizer, vp.to_prompt(),\n",
        "        num_samples=1,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.3,  # Lower temp for eval\n",
        "    )\n",
        "    \n",
        "    code = extract_code(gens[0])\n",
        "    result = verify_solution(code, vp)\n",
        "    \n",
        "    # Find problem type\n",
        "    for pt in CONFIG[\"problem_types\"]:\n",
        "        if pt in vp.problem_id:\n",
        "            val_results[pt][\"total\"] += 1\n",
        "            if result[\"passed\"]:\n",
        "                val_results[pt][\"correct\"] += 1\n",
        "                total_correct += 1\n",
        "            break\n",
        "\n",
        "# Print results\n",
        "print(\"\\nResults by Problem Type:\")\n",
        "print(\"-\" * 40)\n",
        "for prob_type, stats in val_results.items():\n",
        "    if stats[\"total\"] > 0:\n",
        "        acc = stats[\"correct\"] / stats[\"total\"] * 100\n",
        "        print(f\"  {prob_type:20s}: {stats['correct']}/{stats['total']} = {acc:.1f}%\")\n",
        "\n",
        "print(\"-\" * 40)\n",
        "overall_acc = total_correct / total_problems * 100\n",
        "print(f\"  {'OVERALL':20s}: {total_correct}/{total_problems} = {overall_acc:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show a sample generation\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAMPLE GENERATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sample_problem = val_problems[0]\n",
        "print(f\"\\nProblem: {sample_problem.title}\")\n",
        "print(f\"\\nPrompt:\")\n",
        "print(sample_problem.to_prompt()[:500] + \"...\")\n",
        "\n",
        "# Generate\n",
        "sample_gen = generate_samples(\n",
        "    policy_model, tokenizer, sample_problem.to_prompt(),\n",
        "    num_samples=1,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.3,\n",
        ")[0]\n",
        "\n",
        "print(f\"\\nGenerated Solution:\")\n",
        "print(sample_gen[:800])\n",
        "\n",
        "# Verify\n",
        "code = extract_code(sample_gen)\n",
        "result = verify_solution(code, sample_problem)\n",
        "print(f\"\\nVerification: {result['passed_count']}/{result['total_count']} tests passed\")\n",
        "if result[\"error\"]:\n",
        "    print(f\"Error: {result['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## EXPERIMENT COMPLETE\n",
        "\n",
        "### Summary\n",
        "\n",
        "This notebook implemented **M-GRPO (Momentum-Anchored GRPO)** training with:\n",
        "\n",
        "1. **Two-model architecture**: Policy (trainable) + Momentum (EMA)\n",
        "2. **Combined rollout**: Samples from both models for diversity\n",
        "3. **Partial rewards**: Proportional credit for passing tests\n",
        "4. **IQR filtering**: Removes low-entropy samples\n",
        "5. **Entropy monitoring**: Detects and warns about collapse\n",
        "\n",
        "### Files Created\n",
        "\n",
        "- `mgrpo_output/policy/` - Trained policy model (can load with HuggingFace)\n",
        "- `mgrpo_output/momentum/` - Momentum model\n",
        "- `mgrpo_output/metrics.json` - Training metrics\n",
        "- `mgrpo_output/config.json` - Configuration\n",
        "- `mgrpo_output/summary.json` - Final summary\n",
        "- `mgrpo_training_results.png` - Training curves\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Download the trained model from `mgrpo_output/policy/`\n",
        "2. Run on harder problems or more iterations\n",
        "3. Compare against vanilla GRPO baseline\n",
        "4. Tune hyperparameters (momentum, learning rate, samples)\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
