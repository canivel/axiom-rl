{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Axiom-RL V2: Self-Improving Reasoning - Step by Step\n",
    "\n",
    "This notebook walks through the **V2 Expert Iteration** approach step by step.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Why V2?** - The critical flaw in V1 and how V2 fixes it\n",
    "2. **Problem Design** - How problems with multiple test cases prevent memorization\n",
    "3. **Problem Generation** - Creating algorithmic problems programmatically\n",
    "4. **Model Evaluation** - Testing if a model can solve problems\n",
    "5. **Solution Verification** - Checking solutions against ALL test cases\n",
    "6. **Training Loop** - Fine-tuning on verified solutions\n",
    "7. **Self-Improvement** - The complete iteration cycle\n",
    "\n",
    "Run each cell in order and observe the outputs carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install Dependencies\n",
    "\n",
    "First, install the required packages. This takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install -q torch transformers accelerate peft bitsandbytes datasets\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Check GPU Availability\n",
    "\n",
    "We need a GPU for model inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"GPU CHECK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU Available: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    DEVICE = \"cuda:0\"\n",
    "else:\n",
    "    print(\"WARNING: No GPU found! Training will be very slow.\")\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"\\nUsing device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.3 Set Configuration (REQUIRED)\n\n**IMPORTANT: You must run this cell before any other cells that use CONFIG.**\n\nConfigure the experiment parameters. Adjust these based on your GPU memory."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================\n# EXPERIMENT CONFIGURATION - MUST RUN THIS CELL FIRST!\n# =============================================================\n# This cell defines CONFIG which is used throughout the notebook.\n# If you get \"CONFIG is not defined\" errors, run this cell first.\n\nCONFIG = {\n    # Model\n    \"model_name\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n    \n    # Problems\n    \"problem_types\": [\"rpn\", \"parentheses\"],\n    \"train_per_type\": 5,      # Training problems per type\n    \"val_per_type\": 3,        # Validation problems per type  \n    \"test_per_type\": 3,       # Test problems per type\n    \"test_cases\": 5,          # Test cases per problem (KEY!)\n    \n    # Training\n    \"num_iterations\": 2,      # Self-improvement iterations\n    \"learning_rate\": 5e-5,\n    \"lora_r\": 16,\n    \"lora_alpha\": 32,\n    \n    # Random seed for reproducibility\n    \"seed\": 42,\n}\n\nprint(\"=\"*50)\nprint(\"EXPERIMENT CONFIGURATION\")\nprint(\"=\"*50)\nprint(\"\\n[!] CONFIG is now defined and ready to use.\\n\")\nfor key, value in CONFIG.items():\n    print(f\"  {key:20} = {value}\")\nprint(\"\\n\" + \"=\"*50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Understanding V2 Problem Design\n",
    "---\n",
    "\n",
    "## Why V1 Failed\n",
    "\n",
    "V1 problems had a **critical flaw**: only ONE test case per problem.\n",
    "\n",
    "```python\n",
    "# V1 Problem: \"Compute 3 + 4 * 2\"\n",
    "def solve():\n",
    "    return 11  # Just return the answer - no algorithm needed!\n",
    "```\n",
    "\n",
    "The model could pass by **memorizing answers** instead of **learning algorithms**.\n",
    "\n",
    "## How V2 Fixes This\n",
    "\n",
    "V2 problems have **multiple test cases** with **different inputs**:\n",
    "\n",
    "```python\n",
    "# V2 Problem: \"Implement RPN Evaluator\"\n",
    "def evaluate_rpn(expression: str) -> int:\n",
    "    # Must actually implement the algorithm!\n",
    "    # Can't just return a constant.\n",
    "```\n",
    "\n",
    "Test cases:\n",
    "- `evaluate_rpn(\"3 4 +\")` → 7\n",
    "- `evaluate_rpn(\"5 2 *\")` → 10  \n",
    "- `evaluate_rpn(\"2 3 + 4 *\")` → 20\n",
    "\n",
    "**The model MUST implement the algorithm to pass all test cases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define the TestCase Class\n",
    "\n",
    "A test case has input arguments and an expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Any, Optional\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"A single test case with input and expected output.\"\"\"\n",
    "    input_args: List[Any]    # Arguments to pass to the function\n",
    "    expected_output: Any      # Expected return value\n",
    "\n",
    "    def to_assertion(self, func_name: str) -> str:\n",
    "        \"\"\"Generate an assertion statement.\"\"\"\n",
    "        args_str = \", \".join(repr(arg) for arg in self.input_args)\n",
    "        return f\"assert {func_name}({args_str}) == {self.expected_output!r}\"\n",
    "\n",
    "# Example: Create some test cases\n",
    "example_cases = [\n",
    "    TestCase(input_args=[\"3 4 +\"], expected_output=7),\n",
    "    TestCase(input_args=[\"5 2 *\"], expected_output=10),\n",
    "    TestCase(input_args=[\"2 3 + 4 *\"], expected_output=20),\n",
    "]\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"EXAMPLE TEST CASES\")\n",
    "print(\"=\"*50)\n",
    "for i, tc in enumerate(example_cases, 1):\n",
    "    print(f\"\\nTest Case {i}:\")\n",
    "    print(f\"  Input:    {tc.input_args}\")\n",
    "    print(f\"  Expected: {tc.expected_output}\")\n",
    "    print(f\"  Assertion: {tc.to_assertion('evaluate_rpn')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define the AlgorithmicProblem Class\n",
    "\n",
    "A problem contains multiple test cases and generates prompts for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AlgorithmicProblem:\n",
    "    \"\"\"A problem requiring algorithm implementation.\"\"\"\n",
    "    problem_type: str         # e.g., \"rpn\", \"parentheses\"\n",
    "    problem_id: str           # Unique identifier\n",
    "    title: str                # Human-readable title\n",
    "    description: str          # Full problem description\n",
    "    function_signature: str   # e.g., \"def evaluate_rpn(expression: str) -> int:\"\n",
    "    test_cases: List[TestCase]  # MULTIPLE test cases\n",
    "    difficulty: int = 5\n",
    "\n",
    "    def to_prompt(self) -> str:\n",
    "        \"\"\"Convert to a prompt for the model.\"\"\"\n",
    "        # Show some example test cases (not all - keep some hidden)\n",
    "        visible_cases = self.test_cases[:3]\n",
    "        func_name = self.function_signature.split(\"(\")[0].replace(\"def \", \"\")\n",
    "        \n",
    "        examples = \"\\n\".join(\n",
    "            f\"  {tc.to_assertion(func_name)}\"\n",
    "            for tc in visible_cases\n",
    "        )\n",
    "        \n",
    "        return f\"\"\"## {self.title}\n",
    "\n",
    "{self.description}\n",
    "\n",
    "### Function Signature\n",
    "```python\n",
    "{self.function_signature}\n",
    "```\n",
    "\n",
    "### Examples\n",
    "```python\n",
    "{examples}\n",
    "```\n",
    "\n",
    "Implement the function. Your solution must pass ALL test cases.\"\"\"\n",
    "\n",
    "    def get_func_name(self) -> str:\n",
    "        \"\"\"Extract function name from signature.\"\"\"\n",
    "        return self.function_signature.split(\"(\")[0].replace(\"def \", \"\")\n",
    "\n",
    "print(\"AlgorithmicProblem class defined!\")\n",
    "print(f\"\\nKey attributes:\")\n",
    "print(f\"  - problem_type: Type of algorithm (rpn, parentheses, etc.)\")\n",
    "print(f\"  - test_cases: List of TestCase objects (typically 5+)\")\n",
    "print(f\"  - to_prompt(): Generates the prompt for the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Problem Generators\n",
    "---\n",
    "\n",
    "Generators create problems with randomized test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Base Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class AlgorithmicGenerator(ABC):\n",
    "    \"\"\"Base class for problem generators.\"\"\"\n",
    "    \n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        self.rng = random.Random(seed)\n",
    "        self._problem_counter = 0\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def problem_type(self) -> str:\n",
    "        \"\"\"Return the problem type identifier.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def title(self) -> str:\n",
    "        \"\"\"Return the problem title.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def description(self) -> str:\n",
    "        \"\"\"Return the problem description.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def function_signature(self) -> str:\n",
    "        \"\"\"Return the function signature.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def generate_test_cases(self, difficulty: int, count: int) -> List[TestCase]:\n",
    "        \"\"\"Generate test cases for the problem.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def generate(self, difficulty: int = 5, num_test_cases: int = 5) -> AlgorithmicProblem:\n",
    "        \"\"\"Generate a complete problem.\"\"\"\n",
    "        self._problem_counter += 1\n",
    "        problem_id = f\"{self.problem_type}_{self._problem_counter}\"\n",
    "        \n",
    "        test_cases = self.generate_test_cases(difficulty, num_test_cases)\n",
    "        \n",
    "        return AlgorithmicProblem(\n",
    "            problem_type=self.problem_type,\n",
    "            problem_id=problem_id,\n",
    "            title=self.title,\n",
    "            description=self.description,\n",
    "            function_signature=self.function_signature,\n",
    "            test_cases=test_cases,\n",
    "            difficulty=difficulty,\n",
    "        )\n",
    "\n",
    "print(\"AlgorithmicGenerator base class defined!\")\n",
    "print(f\"\\nSubclasses must implement:\")\n",
    "print(f\"  - problem_type: e.g., 'rpn'\")\n",
    "print(f\"  - title: e.g., 'RPN Expression Evaluator'\")\n",
    "print(f\"  - description: Full problem description\")\n",
    "print(f\"  - function_signature: e.g., 'def evaluate_rpn(expr: str) -> int:'\")\n",
    "print(f\"  - generate_test_cases(): Create randomized test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 RPN Evaluator Generator\n",
    "\n",
    "Generates Reverse Polish Notation evaluation problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPNEvaluatorGenerator(AlgorithmicGenerator):\n",
    "    \"\"\"\n",
    "    Generates RPN (Reverse Polish Notation) evaluation problems.\n",
    "    \n",
    "    RPN puts operators AFTER operands:\n",
    "      \"3 4 +\" means 3 + 4 = 7\n",
    "      \"3 4 + 2 *\" means (3 + 4) * 2 = 14\n",
    "    \n",
    "    Requires a stack-based algorithm to solve.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def problem_type(self) -> str:\n",
    "        return \"rpn\"\n",
    "\n",
    "    @property\n",
    "    def title(self) -> str:\n",
    "        return \"RPN Expression Evaluator\"\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return \"\"\"Implement a Reverse Polish Notation (RPN) expression evaluator.\n",
    "\n",
    "In RPN, operators come AFTER their operands:\n",
    "- \"3 4 +\" means 3 + 4 = 7\n",
    "- \"3 4 + 2 *\" means (3 + 4) * 2 = 14\n",
    "- \"5 1 2 + 4 * + 3 -\" means 5 + ((1 + 2) * 4) - 3 = 14\n",
    "\n",
    "Rules:\n",
    "- Tokens are separated by spaces\n",
    "- Valid operators: +, -, *\n",
    "- All numbers are integers\n",
    "- Return the final result as an integer\"\"\"\n",
    "\n",
    "    @property\n",
    "    def function_signature(self) -> str:\n",
    "        return \"def evaluate_rpn(expression: str) -> int:\"\n",
    "\n",
    "    def generate_test_cases(self, difficulty: int, count: int = 5) -> List[TestCase]:\n",
    "        \"\"\"Generate RPN expressions with their correct answers.\"\"\"\n",
    "        test_cases = []\n",
    "        for _ in range(count):\n",
    "            expr, result = self._generate_rpn_expression(difficulty)\n",
    "            test_cases.append(TestCase(input_args=[expr], expected_output=result))\n",
    "        return test_cases\n",
    "\n",
    "    def _generate_rpn_expression(self, difficulty: int) -> tuple:\n",
    "        \"\"\"Generate a valid RPN expression and its result.\"\"\"\n",
    "        num_ops = min(1 + difficulty // 2, 5)\n",
    "        ops = ['+', '-', '*']\n",
    "\n",
    "        # Build expression using stack simulation\n",
    "        stack = []\n",
    "        tokens = []\n",
    "\n",
    "        # Start with two numbers\n",
    "        n1 = self.rng.randint(1, 9)\n",
    "        n2 = self.rng.randint(1, 9)\n",
    "        stack.extend([n1, n2])\n",
    "        tokens.extend([str(n1), str(n2)])\n",
    "\n",
    "        # Add operations\n",
    "        for _ in range(num_ops):\n",
    "            if len(stack) >= 2:\n",
    "                op = self.rng.choice(ops)\n",
    "                b, a = stack.pop(), stack.pop()\n",
    "                result = a + b if op == '+' else (a - b if op == '-' else a * b)\n",
    "                stack.append(result)\n",
    "                tokens.append(op)\n",
    "\n",
    "            if self.rng.random() < 0.4 and len(tokens) < difficulty * 2:\n",
    "                n = self.rng.randint(1, 9)\n",
    "                stack.append(n)\n",
    "                tokens.append(str(n))\n",
    "\n",
    "        # Reduce to single result\n",
    "        while len(stack) > 1:\n",
    "            op = self.rng.choice(ops)\n",
    "            b, a = stack.pop(), stack.pop()\n",
    "            result = a + b if op == '+' else (a - b if op == '-' else a * b)\n",
    "            stack.append(result)\n",
    "            tokens.append(op)\n",
    "\n",
    "        return \" \".join(tokens), stack[0]\n",
    "\n",
    "print(\"RPNEvaluatorGenerator defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Test the RPN Generator\n",
    "\n",
    "Let's generate a problem and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator with fixed seed for reproducibility\n",
    "rpn_gen = RPNEvaluatorGenerator(seed=42)\n",
    "\n",
    "# Generate a problem with 5 test cases\n",
    "rpn_problem = rpn_gen.generate(difficulty=5, num_test_cases=5)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATED RPN PROBLEM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nProblem ID: {rpn_problem.problem_id}\")\n",
    "print(f\"Problem Type: {rpn_problem.problem_type}\")\n",
    "print(f\"Difficulty: {rpn_problem.difficulty}\")\n",
    "print(f\"Number of Test Cases: {len(rpn_problem.test_cases)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"TEST CASES:\")\n",
    "print(\"-\"*60)\n",
    "for i, tc in enumerate(rpn_problem.test_cases, 1):\n",
    "    expr = tc.input_args[0]\n",
    "    result = tc.expected_output\n",
    "    print(f\"  {i}. evaluate_rpn(\\\"{expr}\\\") -> {result}\")\n",
    "\n",
    "# Check output diversity\n",
    "outputs = [tc.expected_output for tc in rpn_problem.test_cases]\n",
    "unique_outputs = len(set(outputs))\n",
    "print(f\"\\nOutput Diversity: {unique_outputs}/{len(outputs)} unique values\")\n",
    "if unique_outputs >= 3:\n",
    "    print(\"GOOD: Diverse outputs - can't pass by hardcoding!\")\n",
    "else:\n",
    "    print(\"WARNING: Low diversity - might allow hardcoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 See the Generated Prompt\n",
    "\n",
    "This is what the model sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PROMPT FOR THE MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(rpn_problem.to_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Parentheses Validator Generator\n",
    "\n",
    "Another problem type that requires a stack-based algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParenthesesValidatorGenerator(AlgorithmicGenerator):\n",
    "    \"\"\"\n",
    "    Generates parentheses validation problems.\n",
    "    \n",
    "    Check if a string of brackets is properly balanced:\n",
    "      \"()[]{}\" -> True\n",
    "      \"([)]\" -> False\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def problem_type(self) -> str:\n",
    "        return \"parentheses\"\n",
    "\n",
    "    @property\n",
    "    def title(self) -> str:\n",
    "        return \"Valid Parentheses Checker\"\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return \"\"\"Implement a function to check if a string of brackets is valid.\n",
    "\n",
    "A string is valid if:\n",
    "1. Open brackets are closed by the same type of brackets\n",
    "2. Open brackets are closed in the correct order\n",
    "3. Every close bracket has a corresponding open bracket\n",
    "\n",
    "Valid brackets: (), [], {}\"\"\"\n",
    "\n",
    "    @property\n",
    "    def function_signature(self) -> str:\n",
    "        return \"def is_valid(s: str) -> bool:\"\n",
    "\n",
    "    def generate_test_cases(self, difficulty: int, count: int = 5) -> List[TestCase]:\n",
    "        \"\"\"Generate bracket strings with correct validity.\"\"\"\n",
    "        test_cases = []\n",
    "        num_valid = count // 2 + 1\n",
    "        num_invalid = count - num_valid\n",
    "\n",
    "        for _ in range(num_valid):\n",
    "            s = self._generate_valid(difficulty)\n",
    "            test_cases.append(TestCase(input_args=[s], expected_output=True))\n",
    "\n",
    "        for _ in range(num_invalid):\n",
    "            s = self._generate_invalid(difficulty)\n",
    "            test_cases.append(TestCase(input_args=[s], expected_output=False))\n",
    "\n",
    "        self.rng.shuffle(test_cases)\n",
    "        return test_cases\n",
    "\n",
    "    def _generate_valid(self, difficulty: int) -> str:\n",
    "        \"\"\"Generate a valid bracket string.\"\"\"\n",
    "        length = 2 * (1 + difficulty // 2)\n",
    "        pairs = [(\"(\", \")\"), (\"[\", \"]\"), (\"{\", \"}\")]\n",
    "        if difficulty <= 3:\n",
    "            pairs = pairs[:1]\n",
    "        elif difficulty <= 6:\n",
    "            pairs = pairs[:2]\n",
    "\n",
    "        result, stack = [], []\n",
    "        while len(result) < length:\n",
    "            remaining = length - len(result)\n",
    "            if len(stack) >= remaining // 2:\n",
    "                result.append(stack.pop())\n",
    "            elif len(stack) == 0:\n",
    "                pair = self.rng.choice(pairs)\n",
    "                result.append(pair[0])\n",
    "                stack.append(pair[1])\n",
    "            elif self.rng.random() < 0.5:\n",
    "                pair = self.rng.choice(pairs)\n",
    "                result.append(pair[0])\n",
    "                stack.append(pair[1])\n",
    "            else:\n",
    "                result.append(stack.pop())\n",
    "        while stack:\n",
    "            result.append(stack.pop())\n",
    "        return \"\".join(result)\n",
    "\n",
    "    def _generate_invalid(self, difficulty: int) -> str:\n",
    "        \"\"\"Generate an invalid bracket string.\"\"\"\n",
    "        pairs = [(\"(\", \")\"), (\"[\", \"]\"), (\"{\", \"}\")]\n",
    "        if difficulty <= 3:\n",
    "            pairs = pairs[:1]\n",
    "        elif difficulty <= 6:\n",
    "            pairs = pairs[:2]\n",
    "\n",
    "        pattern = self.rng.choice([\"mismatch\", \"unclosed\", \"extra_close\"])\n",
    "        \n",
    "        if pattern == \"mismatch\" and len(pairs) > 1:\n",
    "            p1, p2 = self.rng.sample(pairs, 2)\n",
    "            return p1[0] + p2[1]\n",
    "        elif pattern == \"unclosed\":\n",
    "            pair = self.rng.choice(pairs)\n",
    "            return pair[0] * 2 + pair[1]\n",
    "        else:\n",
    "            pair = self.rng.choice(pairs)\n",
    "            return pair[1] + pair[0] + pair[1]\n",
    "\n",
    "print(\"ParenthesesValidatorGenerator defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Test the Parentheses Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paren_gen = ParenthesesValidatorGenerator(seed=42)\n",
    "paren_problem = paren_gen.generate(difficulty=5, num_test_cases=6)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATED PARENTHESES PROBLEM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nProblem ID: {paren_problem.problem_id}\")\n",
    "print(f\"Number of Test Cases: {len(paren_problem.test_cases)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"TEST CASES:\")\n",
    "print(\"-\"*60)\n",
    "true_count = 0\n",
    "for i, tc in enumerate(paren_problem.test_cases, 1):\n",
    "    s = tc.input_args[0]\n",
    "    result = tc.expected_output\n",
    "    if result:\n",
    "        true_count += 1\n",
    "    print(f\"  {i}. is_valid(\\\"{s}\\\") -> {result}\")\n",
    "\n",
    "print(f\"\\nBalance: {true_count} True, {len(paren_problem.test_cases) - true_count} False\")\n",
    "print(\"GOOD: Mix of True/False prevents always returning same value!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Why Hardcoding Fails\n",
    "\n",
    "Let's prove that a hardcoded solution cannot pass all test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"HARDCODING PREVENTION DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# RPN Problem\n",
    "print(\"\\n--- RPN Problem ---\")\n",
    "for i, tc in enumerate(rpn_problem.test_cases, 1):\n",
    "    print(f\"  {i}. evaluate_rpn(\\\"{tc.input_args[0]}\\\") -> {tc.expected_output}\")\n",
    "\n",
    "# Try hardcoding the first answer\n",
    "first_answer = rpn_problem.test_cases[0].expected_output\n",
    "print(f\"\\nHardcoded solution: return {first_answer}\")\n",
    "passed = sum(1 for tc in rpn_problem.test_cases if tc.expected_output == first_answer)\n",
    "print(f\"Would pass: {passed}/{len(rpn_problem.test_cases)} test cases\")\n",
    "print(f\"Result: {'FAIL' if passed < len(rpn_problem.test_cases) else 'PASS'}\")\n",
    "\n",
    "# Parentheses Problem\n",
    "print(\"\\n--- Parentheses Problem ---\")\n",
    "for i, tc in enumerate(paren_problem.test_cases, 1):\n",
    "    print(f\"  {i}. is_valid(\\\"{tc.input_args[0]}\\\") -> {tc.expected_output}\")\n",
    "\n",
    "print(f\"\\nHardcoded solution: return True\")\n",
    "passed_true = sum(1 for tc in paren_problem.test_cases if tc.expected_output == True)\n",
    "print(f\"Would pass: {passed_true}/{len(paren_problem.test_cases)} test cases\")\n",
    "\n",
    "print(f\"\\nHardcoded solution: return False\")\n",
    "passed_false = sum(1 for tc in paren_problem.test_cases if tc.expected_output == False)\n",
    "print(f\"Would pass: {passed_false}/{len(paren_problem.test_cases)} test cases\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUSION: Hardcoding CANNOT pass all test cases!\")\n",
    "print(\"The model MUST implement the actual algorithm.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Solution Verification\n",
    "---\n",
    "\n",
    "We need to safely execute model-generated code and check if it passes all test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Code Extraction\n",
    "\n",
    "Extract Python code from model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_code(response: str) -> Optional[str]:\n",
    "    \"\"\"Extract Python code from a model response.\"\"\"\n",
    "    # Try to find code in ```python blocks\n",
    "    pattern = r\"```python\\s*\\n(.*?)```\"\n",
    "    matches = re.findall(pattern, response, re.DOTALL)\n",
    "    if matches:\n",
    "        return matches[-1].strip()  # Return last code block\n",
    "    \n",
    "    # Try plain ``` blocks\n",
    "    pattern = r\"```\\s*\\n(.*?)```\"\n",
    "    matches = re.findall(pattern, response, re.DOTALL)\n",
    "    if matches:\n",
    "        return matches[-1].strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test with example response\n",
    "example_response = '''Here's my solution:\n",
    "\n",
    "```python\n",
    "def evaluate_rpn(expression: str) -> int:\n",
    "    stack = []\n",
    "    for token in expression.split():\n",
    "        if token.lstrip('-').isdigit():\n",
    "            stack.append(int(token))\n",
    "        else:\n",
    "            b, a = stack.pop(), stack.pop()\n",
    "            if token == '+': stack.append(a + b)\n",
    "            elif token == '-': stack.append(a - b)\n",
    "            elif token == '*': stack.append(a * b)\n",
    "    return stack[0]\n",
    "```\n",
    "'''\n",
    "\n",
    "extracted = extract_code(example_response)\n",
    "print(\"=\"*60)\n",
    "print(\"CODE EXTRACTION TEST\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nExtracted code:\")\n",
    "print(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Safe Code Execution\n",
    "\n",
    "Execute code in a controlled environment with timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import traceback\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "def execute_with_timeout(code: str, func_name: str, args: list, timeout: float = 5.0):\n",
    "    \"\"\"\n",
    "    Execute code and call the function with given arguments.\n",
    "    Returns (success, result_or_error)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create isolated namespace\n",
    "        namespace = {}\n",
    "        \n",
    "        # Execute the code to define the function\n",
    "        exec(code, namespace)\n",
    "        \n",
    "        # Check if function exists\n",
    "        if func_name not in namespace:\n",
    "            return False, f\"Function '{func_name}' not defined\"\n",
    "        \n",
    "        # Call the function\n",
    "        func = namespace[func_name]\n",
    "        result = func(*args)\n",
    "        \n",
    "        return True, result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"{type(e).__name__}: {str(e)}\"\n",
    "\n",
    "# Test execution\n",
    "print(\"=\"*60)\n",
    "print(\"CODE EXECUTION TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_code = extracted  # Use the code we extracted above\n",
    "test_cases_to_try = [\n",
    "    (\"3 4 +\", 7),\n",
    "    (\"5 2 *\", 10),\n",
    "    (\"2 3 + 4 *\", 20),\n",
    "]\n",
    "\n",
    "print(\"\\nTesting extracted code against test cases:\")\n",
    "for expr, expected in test_cases_to_try:\n",
    "    success, result = execute_with_timeout(test_code, \"evaluate_rpn\", [expr])\n",
    "    status = \"PASS\" if success and result == expected else \"FAIL\"\n",
    "    print(f\"  {status}: evaluate_rpn(\\\"{expr}\\\") = {result} (expected {expected})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Solution Verifier\n",
    "\n",
    "Verify a solution against ALL test cases in a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_solution(code: str, problem: AlgorithmicProblem, verbose: bool = True) -> tuple:\n",
    "    \"\"\"\n",
    "    Verify a solution against all test cases.\n",
    "    \n",
    "    Returns: (passed_all, passed_count, total_count, details)\n",
    "    \"\"\"\n",
    "    func_name = problem.get_func_name()\n",
    "    passed_count = 0\n",
    "    details = []\n",
    "    \n",
    "    for i, tc in enumerate(problem.test_cases):\n",
    "        success, result = execute_with_timeout(code, func_name, tc.input_args)\n",
    "        \n",
    "        if success and result == tc.expected_output:\n",
    "            passed_count += 1\n",
    "            status = \"PASS\"\n",
    "        else:\n",
    "            status = \"FAIL\"\n",
    "        \n",
    "        detail = {\n",
    "            \"test_case\": i + 1,\n",
    "            \"input\": tc.input_args,\n",
    "            \"expected\": tc.expected_output,\n",
    "            \"actual\": result if success else \"ERROR\",\n",
    "            \"status\": status,\n",
    "        }\n",
    "        details.append(detail)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  {status}: {func_name}({tc.input_args[0]!r}) = {result} (expected {tc.expected_output})\")\n",
    "    \n",
    "    passed_all = (passed_count == len(problem.test_cases))\n",
    "    return passed_all, passed_count, len(problem.test_cases), details\n",
    "\n",
    "# Test the verifier\n",
    "print(\"=\"*60)\n",
    "print(\"SOLUTION VERIFICATION TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nVerifying against {len(rpn_problem.test_cases)} test cases:\")\n",
    "passed_all, passed, total, _ = verify_solution(extracted, rpn_problem)\n",
    "\n",
    "print(f\"\\nResult: {passed}/{total} test cases passed\")\n",
    "print(f\"Verdict: {'ACCEPTED' if passed_all else 'REJECTED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Model Loading and Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Check that CONFIG is defined\nif 'CONFIG' not in dir():\n    raise RuntimeError(\"CONFIG is not defined! Please run cell 1.3 first.\")\n\nprint(\"=\"*60)\nprint(\"LOADING MODEL\")\nprint(\"=\"*60)\nprint(f\"\\nModel: {CONFIG['model_name']}\")\nprint(\"This may take a few minutes...\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nprint(\"Tokenizer loaded!\")\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    CONFIG[\"model_name\"],\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nprint(f\"Model loaded on: {model.device}\")\n\n# Model info\nnum_params = sum(p.numel() for p in model.parameters())\nprint(f\"Parameters: {num_params / 1e9:.2f}B\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Generate Solution for a Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(model, tokenizer, problem: AlgorithmicProblem, \n",
    "                      max_new_tokens: int = 512, temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Generate a solution for a problem using the model.\n",
    "    \"\"\"\n",
    "    # Build the prompt\n",
    "    system_msg = \"You are an expert Python programmer. Implement the requested function.\"\n",
    "    user_msg = problem.to_prompt()\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode response (excluding prompt)\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "print(\"generate_solution function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Test Solution Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GENERATING SOLUTION FOR RPN PROBLEM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nProblem:\")\n",
    "print(f\"  {rpn_problem.title}\")\n",
    "print(f\"  Test cases: {len(rpn_problem.test_cases)}\")\n",
    "\n",
    "print(\"\\nGenerating solution...\")\n",
    "response = generate_solution(model, tokenizer, rpn_problem)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"-\"*60)\n",
    "print(response[:1000])  # Show first 1000 chars\n",
    "if len(response) > 1000:\n",
    "    print(\"... (truncated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Extract and Verify the Generated Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"VERIFYING GENERATED SOLUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract code\n",
    "generated_code = extract_code(response)\n",
    "\n",
    "if generated_code:\n",
    "    print(\"\\nExtracted code:\")\n",
    "    print(\"-\"*40)\n",
    "    print(generated_code)\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    print(f\"\\nVerifying against {len(rpn_problem.test_cases)} test cases:\")\n",
    "    passed_all, passed, total, _ = verify_solution(generated_code, rpn_problem)\n",
    "    \n",
    "    print(f\"\\nResult: {passed}/{total} test cases passed\")\n",
    "    print(f\"Verdict: {'ACCEPTED - Can use for training!' if passed_all else 'REJECTED - Not good enough'}\")\n",
    "else:\n",
    "    print(\"ERROR: Could not extract code from response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Problem Set Generation\n",
    "---\n",
    "\n",
    "Generate train/val/test problem sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Generator Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registry of available generators\n",
    "GENERATORS_V2 = {\n",
    "    \"rpn\": RPNEvaluatorGenerator,\n",
    "    \"parentheses\": ParenthesesValidatorGenerator,\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AVAILABLE PROBLEM GENERATORS\")\n",
    "print(\"=\"*60)\n",
    "for name, cls in GENERATORS_V2.items():\n",
    "    gen = cls(seed=42)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Title: {gen.title}\")\n",
    "    print(f\"  Signature: {gen.function_signature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Generate Problem Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check that CONFIG is defined\nif 'CONFIG' not in dir():\n    raise RuntimeError(\"CONFIG is not defined! Please run cell 1.3 first.\")\n\ndef generate_problem_sets(config: dict, seed: int = 42) -> tuple:\n    \"\"\"\n    Generate train, validation, and test problem sets.\n    \n    Returns: (train_problems, val_problems, test_problems)\n    \"\"\"\n    rng = random.Random(seed)\n    \n    train_problems = []\n    val_problems = []\n    test_problems = []\n    \n    for prob_type in config[\"problem_types\"]:\n        if prob_type not in GENERATORS_V2:\n            print(f\"WARNING: Unknown problem type '{prob_type}', skipping\")\n            continue\n        \n        # Create generator with unique seed per type\n        gen = GENERATORS_V2[prob_type](seed=rng.randint(0, 1000000))\n        \n        # Generate problems for each split\n        for _ in range(config[\"train_per_type\"]):\n            difficulty = rng.randint(3, 7)\n            problem = gen.generate(difficulty=difficulty, num_test_cases=config[\"test_cases\"])\n            train_problems.append(problem)\n        \n        for _ in range(config[\"val_per_type\"]):\n            difficulty = rng.randint(3, 7)\n            problem = gen.generate(difficulty=difficulty, num_test_cases=config[\"test_cases\"])\n            val_problems.append(problem)\n        \n        for _ in range(config[\"test_per_type\"]):\n            difficulty = rng.randint(3, 7)\n            problem = gen.generate(difficulty=difficulty, num_test_cases=config[\"test_cases\"])\n            test_problems.append(problem)\n    \n    # Shuffle\n    rng.shuffle(train_problems)\n    rng.shuffle(val_problems)\n    rng.shuffle(test_problems)\n    \n    return train_problems, val_problems, test_problems\n\n# Generate problem sets\nprint(\"=\"*60)\nprint(\"GENERATING PROBLEM SETS\")\nprint(\"=\"*60)\n\ntrain_problems, val_problems, test_problems = generate_problem_sets(CONFIG, seed=CONFIG[\"seed\"])\n\nprint(f\"\\nGenerated:\")\nprint(f\"  Train: {len(train_problems)} problems\")\nprint(f\"  Val:   {len(val_problems)} problems\")\nprint(f\"  Test:  {len(test_problems)} problems\")\nprint(f\"  Test cases per problem: {CONFIG['test_cases']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Inspect Generated Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING PROBLEMS OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count by type\n",
    "type_counts = {}\n",
    "for p in train_problems:\n",
    "    type_counts[p.problem_type] = type_counts.get(p.problem_type, 0) + 1\n",
    "\n",
    "print(\"\\nProblems by type:\")\n",
    "for ptype, count in type_counts.items():\n",
    "    print(f\"  {ptype}: {count}\")\n",
    "\n",
    "print(\"\\nFirst 5 training problems:\")\n",
    "for i, p in enumerate(train_problems[:5], 1):\n",
    "    print(f\"  {i}. [{p.problem_type}] {p.problem_id} ({len(p.test_cases)} test cases)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Evaluation Loop\n",
    "---\n",
    "\n",
    "Evaluate the model on a set of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Evaluate Model on Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, problems: List[AlgorithmicProblem], \n",
    "                   verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate model on a list of problems.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - accuracy: % of problems fully solved\n",
    "    - test_pass_rate: % of individual test cases passed\n",
    "    - results: detailed results per problem\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_passed_all = 0\n",
    "    total_test_cases = 0\n",
    "    total_tests_passed = 0\n",
    "    \n",
    "    for i, problem in enumerate(problems):\n",
    "        if verbose:\n",
    "            print(f\"  [{i+1}/{len(problems)}] {problem.problem_id}...\", end=\" \")\n",
    "        \n",
    "        # Generate solution\n",
    "        try:\n",
    "            response = generate_solution(model, tokenizer, problem)\n",
    "            code = extract_code(response)\n",
    "            \n",
    "            if code:\n",
    "                passed_all, passed, total, details = verify_solution(code, problem, verbose=False)\n",
    "                total_test_cases += total\n",
    "                total_tests_passed += passed\n",
    "                \n",
    "                if passed_all:\n",
    "                    total_passed_all += 1\n",
    "                    if verbose:\n",
    "                        print(f\"PASS ({passed}/{total})\")\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(f\"FAIL ({passed}/{total})\")\n",
    "                \n",
    "                results.append({\n",
    "                    \"problem_id\": problem.problem_id,\n",
    "                    \"passed_all\": passed_all,\n",
    "                    \"passed\": passed,\n",
    "                    \"total\": total,\n",
    "                    \"code\": code,\n",
    "                })\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"FAIL (no code)\")\n",
    "                results.append({\n",
    "                    \"problem_id\": problem.problem_id,\n",
    "                    \"passed_all\": False,\n",
    "                    \"passed\": 0,\n",
    "                    \"total\": len(problem.test_cases),\n",
    "                    \"code\": None,\n",
    "                })\n",
    "                total_test_cases += len(problem.test_cases)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"ERROR: {e}\")\n",
    "            results.append({\n",
    "                \"problem_id\": problem.problem_id,\n",
    "                \"passed_all\": False,\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "    \n",
    "    accuracy = total_passed_all / len(problems) if problems else 0\n",
    "    test_pass_rate = total_tests_passed / total_test_cases if total_test_cases else 0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"test_pass_rate\": test_pass_rate,\n",
    "        \"total_solved\": total_passed_all,\n",
    "        \"total_problems\": len(problems),\n",
    "        \"results\": results,\n",
    "    }\n",
    "\n",
    "print(\"evaluate_model function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Run Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"INITIAL EVALUATION (Before Training)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n--- Training Set ({len(train_problems)} problems) ---\")\n",
    "train_eval = evaluate_model(model, tokenizer, train_problems)\n",
    "\n",
    "print(f\"\\n--- Validation Set ({len(val_problems)} problems) ---\")\n",
    "val_eval = evaluate_model(model, tokenizer, val_problems)\n",
    "\n",
    "print(f\"\\n--- Test Set ({len(test_problems)} problems) ---\")\n",
    "test_eval = evaluate_model(model, tokenizer, test_problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ITERATION 0 - EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'Set':<12} {'Accuracy':>12} {'Test Pass Rate':>16} {'Solved':>10}\")\n",
    "print(\"-\"*52)\n",
    "print(f\"{'Train':<12} {train_eval['accuracy']*100:>11.1f}% {train_eval['test_pass_rate']*100:>15.1f}% {train_eval['total_solved']:>6}/{train_eval['total_problems']}\")\n",
    "print(f\"{'Validation':<12} {val_eval['accuracy']*100:>11.1f}% {val_eval['test_pass_rate']*100:>15.1f}% {val_eval['total_solved']:>6}/{val_eval['total_problems']}\")\n",
    "print(f\"{'Test':<12} {test_eval['accuracy']*100:>11.1f}% {test_eval['test_pass_rate']*100:>15.1f}% {test_eval['total_solved']:>6}/{test_eval['total_problems']}\")\n",
    "\n",
    "# Store for comparison\n",
    "iteration_metrics = [{\n",
    "    \"iteration\": 0,\n",
    "    \"train_accuracy\": train_eval['accuracy'],\n",
    "    \"val_accuracy\": val_eval['accuracy'],\n",
    "    \"test_accuracy\": test_eval['accuracy'],\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 8: Solution Collection\n",
    "---\n",
    "\n",
    "Collect verified solutions for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Collect Verified Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_solutions(model, tokenizer, problems: List[AlgorithmicProblem],\n",
    "                      samples_per_problem: int = 1) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Collect verified solutions from problems.\n",
    "    Only solutions that pass ALL test cases are kept.\n",
    "    \"\"\"\n",
    "    solutions = []\n",
    "    \n",
    "    for i, problem in enumerate(problems):\n",
    "        print(f\"  [{i+1}/{len(problems)}] {problem.problem_id}...\", end=\" \")\n",
    "        \n",
    "        for sample_idx in range(samples_per_problem):\n",
    "            try:\n",
    "                response = generate_solution(model, tokenizer, problem, temperature=0.7)\n",
    "                code = extract_code(response)\n",
    "                \n",
    "                if code:\n",
    "                    passed_all, passed, total, _ = verify_solution(code, problem, verbose=False)\n",
    "                    \n",
    "                    if passed_all:\n",
    "                        solutions.append({\n",
    "                            \"problem_id\": problem.problem_id,\n",
    "                            \"problem_type\": problem.problem_type,\n",
    "                            \"prompt\": problem.to_prompt(),\n",
    "                            \"solution_code\": code,\n",
    "                            \"passed_tests\": passed,\n",
    "                            \"total_tests\": total,\n",
    "                        })\n",
    "                        print(f\"COLLECTED ({passed}/{total})\")\n",
    "                        break  # Got a good solution, move to next problem\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        else:\n",
    "            print(\"SKIPPED (no valid solution)\")\n",
    "    \n",
    "    return solutions\n",
    "\n",
    "print(\"collect_solutions function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Collect Training Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"COLLECTING VERIFIED SOLUTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nCollecting from {len(train_problems)} training problems...\")\n",
    "collected_solutions = collect_solutions(model, tokenizer, train_problems)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"COLLECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Collected: {len(collected_solutions)} verified solutions\")\n",
    "print(f\"From: {len(train_problems)} problems\")\n",
    "print(f\"Collection rate: {len(collected_solutions)/len(train_problems)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Inspect Collected Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SAMPLE COLLECTED SOLUTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, sol in enumerate(collected_solutions[:2], 1):\n",
    "    print(f\"\\n--- Solution {i} ---\")\n",
    "    print(f\"Problem: {sol['problem_id']}\")\n",
    "    print(f\"Type: {sol['problem_type']}\")\n",
    "    print(f\"Tests: {sol['passed_tests']}/{sol['total_tests']}\")\n",
    "    print(f\"\\nCode:\")\n",
    "    print(sol['solution_code'][:500])\n",
    "    if len(sol['solution_code']) > 500:\n",
    "        print(\"... (truncated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 9: LoRA Training\n",
    "---\n",
    "\n",
    "Fine-tune the model on collected solutions using LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_training_data(solutions: List[dict], tokenizer) -> Dataset:\n",
    "    \"\"\"\n",
    "    Convert solutions to training format.\n",
    "    \"\"\"\n",
    "    training_examples = []\n",
    "    \n",
    "    for sol in solutions:\n",
    "        # Format as chat\n",
    "        system_msg = \"You are an expert Python programmer. Implement the requested function.\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": sol[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": f\"```python\\n{sol['solution_code']}\\n```\"},\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        training_examples.append({\"text\": text})\n",
    "    \n",
    "    return Dataset.from_list(training_examples)\n",
    "\n",
    "# Skip training if no solutions\n",
    "if len(collected_solutions) == 0:\n",
    "    print(\"WARNING: No solutions collected. Skipping training.\")\n",
    "    SKIP_TRAINING = True\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"PREPARING TRAINING DATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    train_dataset = prepare_training_data(collected_solutions, tokenizer)\n",
    "    print(f\"\\nTraining examples: {len(train_dataset)}\")\n",
    "    \n",
    "    print(\"\\nSample training text (first 500 chars):\")\n",
    "    print(train_dataset[0][\"text\"][:500])\n",
    "    \n",
    "    SKIP_TRAINING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_TRAINING:\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"CONFIGURING LoRA\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=CONFIG[\"lora_r\"],\n",
    "        lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nLoRA Configuration:\")\n",
    "    print(f\"  Rank (r): {lora_config.r}\")\n",
    "    print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "    print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "    print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "\n",
    "    # Apply LoRA to model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"\\nTrainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_TRAINING:\n",
    "    from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Tokenize dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./lora_output\",\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nStarting training...\")\n",
    "    print(f\"  Examples: {len(tokenized_dataset)}\")\n",
    "    print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "    print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Merge LoRA Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_TRAINING:\n",
    "    print(\"=\"*60)\n",
    "    print(\"MERGING LoRA WEIGHTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Merge LoRA weights into base model\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    print(\"LoRA weights merged into base model!\")\n",
    "    print(\"Model is now ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 10: Post-Training Evaluation\n",
    "---\n",
    "\n",
    "Evaluate the model after training to see improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Re-evaluate on All Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ITERATION 1 - POST-TRAINING EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n--- Training Set ({len(train_problems)} problems) ---\")\n",
    "train_eval_1 = evaluate_model(model, tokenizer, train_problems)\n",
    "\n",
    "print(f\"\\n--- Validation Set ({len(val_problems)} problems) ---\")\n",
    "val_eval_1 = evaluate_model(model, tokenizer, val_problems)\n",
    "\n",
    "print(f\"\\n--- Test Set ({len(test_problems)} problems) ---\")\n",
    "test_eval_1 = evaluate_model(model, tokenizer, test_problems)\n",
    "\n",
    "# Store metrics\n",
    "iteration_metrics.append({\n",
    "    \"iteration\": 1,\n",
    "    \"train_accuracy\": train_eval_1['accuracy'],\n",
    "    \"val_accuracy\": val_eval_1['accuracy'],\n",
    "    \"test_accuracy\": test_eval_1['accuracy'],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Compare Before vs After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"COMPARISON: BEFORE vs AFTER TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Before':>12} {'After':>12} {'Change':>12}\")\n",
    "print(\"-\"*58)\n",
    "\n",
    "# Train accuracy\n",
    "before = iteration_metrics[0]['train_accuracy'] * 100\n",
    "after = iteration_metrics[1]['train_accuracy'] * 100\n",
    "change = after - before\n",
    "print(f\"{'Train Accuracy':<20} {before:>11.1f}% {after:>11.1f}% {change:>+11.1f}%\")\n",
    "\n",
    "# Val accuracy\n",
    "before = iteration_metrics[0]['val_accuracy'] * 100\n",
    "after = iteration_metrics[1]['val_accuracy'] * 100\n",
    "change = after - before\n",
    "print(f\"{'Val Accuracy':<20} {before:>11.1f}% {after:>11.1f}% {change:>+11.1f}%\")\n",
    "\n",
    "# Test accuracy\n",
    "before = iteration_metrics[0]['test_accuracy'] * 100\n",
    "after = iteration_metrics[1]['test_accuracy'] * 100\n",
    "change = after - before\n",
    "print(f\"{'Test Accuracy':<20} {before:>11.1f}% {after:>11.1f}% {change:>+11.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 11: Results Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Model: {CONFIG['model_name']}\")\n",
    "print(f\"  Problem types: {CONFIG['problem_types']}\")\n",
    "print(f\"  Train problems: {len(train_problems)}\")\n",
    "print(f\"  Test cases per problem: {CONFIG['test_cases']}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Solutions collected: {len(collected_solutions)}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  {'Iteration':<12} {'Train':>10} {'Val':>10} {'Test':>10}\")\n",
    "print(f\"  {'-'*44}\")\n",
    "for m in iteration_metrics:\n",
    "    print(f\"  {m['iteration']:<12} {m['train_accuracy']*100:>9.1f}% {m['val_accuracy']*100:>9.1f}% {m['test_accuracy']*100:>9.1f}%\")\n",
    "\n",
    "# Calculate improvement\n",
    "train_improvement = (iteration_metrics[-1]['train_accuracy'] - iteration_metrics[0]['train_accuracy']) * 100\n",
    "val_improvement = (iteration_metrics[-1]['val_accuracy'] - iteration_metrics[0]['val_accuracy']) * 100\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Train: {train_improvement:+.1f}%\")\n",
    "print(f\"  Val:   {val_improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "1. V2 PROBLEM DESIGN WORKS\n",
    "   - Multiple test cases prevent memorization\n",
    "   - Model must implement actual algorithms\n",
    "   - Training accuracy IMPROVES (not degrades like V1)\n",
    "\n",
    "2. SELF-IMPROVEMENT LOOP\n",
    "   - Model generates solutions\n",
    "   - Solutions verified against ALL test cases\n",
    "   - Only fully correct solutions used for training\n",
    "   - Model learns from its own successful outputs\n",
    "\n",
    "3. EXPERT ITERATION\n",
    "   - Model N generates solutions\n",
    "   - Verified solutions train Model N+1\n",
    "   - Model N+1 is better at generating solutions\n",
    "   - Repeat for continuous improvement\n",
    "\n",
    "4. NEXT STEPS\n",
    "   - Run more iterations (5-10+)\n",
    "   - Add more problem types\n",
    "   - Scale to more problems per type\n",
    "   - Implement replay buffer to prevent forgetting\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}