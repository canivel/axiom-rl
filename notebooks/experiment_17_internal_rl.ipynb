{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 17: Internal RL with Temporal Abstractions\n",
    "\n",
    "**Self-contained Colab notebook implementing hierarchical RL for code generation**\n",
    "\n",
    "Based on: [\"Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning\"](https://arxiv.org/abs/2512.20605) (Kobayashi et al., Google, Dec 2025)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "Instead of doing RL on individual tokens (massive search space), we:\n",
    "1. **Discover** temporally-abstract actions in the model's residual stream\n",
    "2. **Explore** in a compact 16D latent space instead of 50K+ vocabulary\n",
    "3. **Execute** each abstract action generates multiple tokens until switching\n",
    "\n",
    "```\n",
    "Standard RL:  Token₁ → Token₂ → ... → Token₁₀₀ → Reward  (massive variance)\n",
    "Internal RL:  z₁ → z₂ → z₃ → z₄ → z₅ → Reward           (tractable)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate peft bitsandbytes torch\n",
    "!pip install -q datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for Experiment 17.\"\"\"\n",
    "    # Model\n",
    "    base_model: str = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
    "    controller_layer: int = 12  # Mid-depth for Qwen 0.5B (24 layers)\n",
    "    \n",
    "    # Metacontroller architecture\n",
    "    latent_dim: int = 16        # Dimension of abstract action space\n",
    "    gru_dim: int = 64           # GRU hidden dimension\n",
    "    seq_embed_dim: int = 64     # Sequence embedding dimension\n",
    "    encoder_hidden: int = 64    # Encoder MLP hidden dim\n",
    "    decoder_hidden: int = 64    # Decoder MLP hidden dim\n",
    "    switch_hidden: int = 64     # Switching unit hidden dim\n",
    "    controller_rank: int = 16   # Low-rank controller (like LoRA)\n",
    "    \n",
    "    # Phase 2: Metacontroller training\n",
    "    mc_batch_size: int = 4\n",
    "    mc_learning_rate: float = 1e-3\n",
    "    mc_weight_decay: float = 0.03\n",
    "    mc_epochs: int = 30\n",
    "    kl_weight: float = 0.1      # ELBO regularization weight\n",
    "    \n",
    "    # Phase 3: Internal RL\n",
    "    rl_batch_size: int = 8\n",
    "    rl_learning_rate: float = 3e-5\n",
    "    rl_steps: int = 1000\n",
    "    beta_threshold: float = 0.5  # Switching threshold\n",
    "    clip_epsilon: float = 0.2    # PPO clip\n",
    "    entropy_coef: float = 0.01\n",
    "    \n",
    "    # Generation\n",
    "    max_tokens: int = 256\n",
    "    temperature: float = 0.7\n",
    "    max_seq_len: int = 512\n",
    "    \n",
    "    # Logging\n",
    "    log_interval: int = 10\n",
    "    eval_interval: int = 50\n",
    "    \n",
    "config = Config()\n",
    "print(\"Configuration:\")\n",
    "for k, v in config.__dict__.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Problem Definitions\n",
    "\n",
    "We'll use the same 6 problem types from Experiments 15-16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem definitions with test cases\n",
    "PROBLEMS = {\n",
    "    \"fibonacci\": {\n",
    "        \"description\": \"Write a function that returns the nth Fibonacci number. fib(0)=0, fib(1)=1, fib(n)=fib(n-1)+fib(n-2)\",\n",
    "        \"entry_point\": \"fib\",\n",
    "        \"test_cases\": [\n",
    "            {\"input\": [0], \"expected\": 0},\n",
    "            {\"input\": [1], \"expected\": 1},\n",
    "            {\"input\": [5], \"expected\": 5},\n",
    "            {\"input\": [10], \"expected\": 55},\n",
    "            {\"input\": [15], \"expected\": 610},\n",
    "        ],\n",
    "        \"canonical_solution\": \"\"\"def fib(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fib(n-1) + fib(n-2)\"\"\"\n",
    "    },\n",
    "    \"binary_search\": {\n",
    "        \"description\": \"Write a function that performs binary search on a sorted list. Return the index if found, -1 otherwise.\",\n",
    "        \"entry_point\": \"binary_search\",\n",
    "        \"test_cases\": [\n",
    "            {\"input\": [[1,2,3,4,5], 3], \"expected\": 2},\n",
    "            {\"input\": [[1,2,3,4,5], 1], \"expected\": 0},\n",
    "            {\"input\": [[1,2,3,4,5], 5], \"expected\": 4},\n",
    "            {\"input\": [[1,2,3,4,5], 6], \"expected\": -1},\n",
    "            {\"input\": [[], 1], \"expected\": -1},\n",
    "        ],\n",
    "        \"canonical_solution\": \"\"\"def binary_search(arr, target):\n",
    "    left, right = 0, len(arr) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1\"\"\"\n",
    "    },\n",
    "    \"coin_change\": {\n",
    "        \"description\": \"Given coins of different denominations and a total amount, return the fewest coins needed to make up that amount. Return -1 if not possible.\",\n",
    "        \"entry_point\": \"coin_change\",\n",
    "        \"test_cases\": [\n",
    "            {\"input\": [[1,2,5], 11], \"expected\": 3},\n",
    "            {\"input\": [[2], 3], \"expected\": -1},\n",
    "            {\"input\": [[1], 0], \"expected\": 0},\n",
    "            {\"input\": [[1,2,5], 5], \"expected\": 1},\n",
    "            {\"input\": [[2,5,10], 6], \"expected\": 3},\n",
    "        ],\n",
    "        \"canonical_solution\": \"\"\"def coin_change(coins, amount):\n",
    "    dp = [float('inf')] * (amount + 1)\n",
    "    dp[0] = 0\n",
    "    for i in range(1, amount + 1):\n",
    "        for coin in coins:\n",
    "            if coin <= i:\n",
    "                dp[i] = min(dp[i], dp[i - coin] + 1)\n",
    "    return dp[amount] if dp[amount] != float('inf') else -1\"\"\"\n",
    "    },\n",
    "    \"valid_parentheses\": {\n",
    "        \"description\": \"Given a string containing just '(', ')', '{', '}', '[' and ']', determine if the input string is valid.\",\n",
    "        \"entry_point\": \"is_valid\",\n",
    "        \"test_cases\": [\n",
    "            {\"input\": [\"()\"], \"expected\": True},\n",
    "            {\"input\": [\"()[]{}\"], \"expected\": True},\n",
    "            {\"input\": [\"(]\"], \"expected\": False},\n",
    "            {\"input\": [\"([)]\"], \"expected\": False},\n",
    "            {\"input\": [\"{[]}\"], \"expected\": True},\n",
    "        ],\n",
    "        \"canonical_solution\": \"\"\"def is_valid(s):\n",
    "    stack = []\n",
    "    mapping = {')': '(', '}': '{', ']': '['}\n",
    "    for char in s:\n",
    "        if char in mapping:\n",
    "            if not stack or stack.pop() != mapping[char]:\n",
    "                return False\n",
    "        else:\n",
    "            stack.append(char)\n",
    "    return len(stack) == 0\"\"\"\n",
    "    },\n",
    "    \"rpn_calculator\": {\n",
    "        \"description\": \"Evaluate the value of an arithmetic expression in Reverse Polish Notation. Valid operators are +, -, *, /.\",\n",
    "        \"entry_point\": \"eval_rpn\",\n",
    "        \"test_cases\": [\n",
    "            {\"input\": [[\"2\",\"1\",\"+\",\"3\",\"*\"]], \"expected\": 9},\n",
    "            {\"input\": [[\"4\",\"13\",\"5\",\"/\",\"+\"]], \"expected\": 6},\n",
    "            {\"input\": [[\"10\",\"6\",\"9\",\"3\",\"+\",\"-11\",\"*\",\"/\",\"*\",\"17\",\"+\",\"5\",\"+\"]], \"expected\": 22},\n",
    "            {\"input\": [[\"3\",\"4\",\"+\"]], \"expected\": 7},\n",
    "            {\"input\": [[\"5\"]], \"expected\": 5},\n",
    "        ],\n",
    "        \"canonical_solution\": \"\"\"def eval_rpn(tokens):\n",
    "    stack = []\n",
    "    for token in tokens:\n",
    "        if token in '+-*/':\n",
    "            b, a = stack.pop(), stack.pop()\n",
    "            if token == '+': stack.append(a + b)\n",
    "            elif token == '-': stack.append(a - b)\n",
    "            elif token == '*': stack.append(a * b)\n",
    "            else: stack.append(int(a / b))\n",
    "        else:\n",
    "            stack.append(int(token))\n",
    "    return stack[0]\"\"\"\n",
    "    },\n",
    "    \"edit_distance\": {\n",
    "        \"description\": \"Given two strings word1 and word2, return the minimum number of operations required to convert word1 to word2. You can insert, delete, or replace a character.\",\n",
    "        \"entry_point\": \"min_distance\",\n",
    "        \"test_cases\": [\n",
    "            {\"input\": [\"horse\", \"ros\"], \"expected\": 3},\n",
    "            {\"input\": [\"intention\", \"execution\"], \"expected\": 5},\n",
    "            {\"input\": [\"\", \"a\"], \"expected\": 1},\n",
    "            {\"input\": [\"a\", \"a\"], \"expected\": 0},\n",
    "            {\"input\": [\"abc\", \"def\"], \"expected\": 3},\n",
    "        ],\n",
    "        \"canonical_solution\": \"\"\"def min_distance(word1, word2):\n",
    "    m, n = len(word1), len(word2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if word1[i-1] == word2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "    return dp[m][n]\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Loaded {len(PROBLEMS)} problem types:\")\n",
    "for name in PROBLEMS:\n",
    "    print(f\"  - {name}: {len(PROBLEMS[name]['test_cases'])} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Metacontroller Architecture\n",
    "\n",
    "The metacontroller consists of:\n",
    "1. **GRU** - Maintains history state\n",
    "2. **Sequence Embedder** - Creates acausal embedding of full sequence\n",
    "3. **Controller Encoder** - Produces latent code proposals (μ, Σ)\n",
    "4. **Switching Unit** - Decides when to switch abstract actions\n",
    "5. **Controller Decoder** - Hypernetwork producing low-rank controllers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    \"\"\"Gated Recurrent Unit for maintaining history state.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W_r = nn.Linear(input_dim + hidden_dim, hidden_dim)\n",
    "        self.W_z = nn.Linear(input_dim + hidden_dim, hidden_dim)\n",
    "        self.W_h = nn.Linear(input_dim + hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:\n",
    "        combined = torch.cat([x, h], dim=-1)\n",
    "        r = torch.sigmoid(self.W_r(combined))\n",
    "        z = torch.sigmoid(self.W_z(combined))\n",
    "        combined_reset = torch.cat([x, r * h], dim=-1)\n",
    "        h_tilde = torch.tanh(self.W_h(combined_reset))\n",
    "        return (1 - z) * h + z * h_tilde\n",
    "    \n",
    "    def init_hidden(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.hidden_dim, device=device)\n",
    "\n",
    "\n",
    "class SequenceEmbedder(nn.Module):\n",
    "    \"\"\"Creates acausal embedding of the full sequence.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(embed_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "    \n",
    "    def forward(self, e_seq: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        x = self.proj(e_seq)  # [batch, seq, output_dim]\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(-1).float()\n",
    "            x = (x * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "        else:\n",
    "            x = x.mean(dim=1)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class ControllerEncoder(nn.Module):\n",
    "    \"\"\"Produces latent code proposal distribution parameters.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, hidden_dim: int, gru_dim: int, \n",
    "                 seq_embed_dim: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        input_dim = embed_dim + gru_dim + seq_embed_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mu_head = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_head = nn.Linear(hidden_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, e_t: torch.Tensor, h_t: torch.Tensor, \n",
    "                s_embed: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = torch.cat([e_t, h_t, s_embed], dim=-1)\n",
    "        hidden = self.encoder(x)\n",
    "        return self.mu_head(hidden), self.logvar_head(hidden)\n",
    "    \n",
    "    def sample(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "\n",
    "class SwitchingUnit(nn.Module):\n",
    "    \"\"\"Decides when to switch to a new abstract action.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, gru_dim: int, latent_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        input_dim = embed_dim + gru_dim + latent_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, e_t: torch.Tensor, h_t: torch.Tensor, \n",
    "                z_prev: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([e_t, h_t, z_prev], dim=-1)\n",
    "        return torch.sigmoid(self.net(x))\n",
    "\n",
    "\n",
    "class ControllerDecoder(nn.Module):\n",
    "    \"\"\"Hypernetwork that produces low-rank controller matrices.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim: int, embed_dim: int, rank: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.rank = rank\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.A_head = nn.Linear(hidden_dim, embed_dim * rank)\n",
    "        self.B_head = nn.Linear(hidden_dim, rank * embed_dim)\n",
    "        self.scale = 1.0 / rank\n",
    "    \n",
    "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        hidden = self.net(z)\n",
    "        A = self.A_head(hidden).view(-1, self.embed_dim, self.rank)\n",
    "        B = self.B_head(hidden).view(-1, self.rank, self.embed_dim)\n",
    "        return A * self.scale, B\n",
    "    \n",
    "    def apply_controller(self, e: torch.Tensor, A: torch.Tensor, \n",
    "                         B: torch.Tensor) -> torch.Tensor:\n",
    "        e_col = e.unsqueeze(-1)\n",
    "        delta = torch.bmm(A, torch.bmm(B, e_col)).squeeze(-1)\n",
    "        return e + delta\n",
    "\n",
    "\n",
    "print(\"Metacontroller components defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metacontroller(nn.Module):\n",
    "    \"\"\"Full Metacontroller combining all components.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, latent_dim: int = 16, gru_dim: int = 64,\n",
    "                 seq_embed_dim: int = 64, encoder_hidden: int = 64,\n",
    "                 decoder_hidden: int = 64, switch_hidden: int = 64,\n",
    "                 controller_rank: int = 16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gru_dim = gru_dim\n",
    "        \n",
    "        self.gru = GRUCell(embed_dim, gru_dim)\n",
    "        self.sequence_embedder = SequenceEmbedder(embed_dim, seq_embed_dim)\n",
    "        self.encoder = ControllerEncoder(embed_dim, encoder_hidden, gru_dim, \n",
    "                                         seq_embed_dim, latent_dim)\n",
    "        self.switching_unit = SwitchingUnit(embed_dim, gru_dim, latent_dim, switch_hidden)\n",
    "        self.decoder = ControllerDecoder(latent_dim, embed_dim, controller_rank, decoder_hidden)\n",
    "        \n",
    "        self.z_init = nn.Parameter(torch.zeros(latent_dim))\n",
    "    \n",
    "    def forward_training(self, residual_sequence: torch.Tensor,\n",
    "                        attention_mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Training forward pass with acausal sequence embedding.\"\"\"\n",
    "        batch_size, seq_len, _ = residual_sequence.shape\n",
    "        device = residual_sequence.device\n",
    "        \n",
    "        # Acausal: see full sequence\n",
    "        s_embed = self.sequence_embedder(residual_sequence, attention_mask)\n",
    "        \n",
    "        # Initialize\n",
    "        h = self.gru.init_hidden(batch_size, device)\n",
    "        z = self.z_init.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        z_list, mu_list, logvar_list, beta_list = [], [], [], []\n",
    "        controlled_list = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            e_t = residual_sequence[:, t, :]\n",
    "            h = self.gru(e_t, h)\n",
    "            \n",
    "            mu, logvar = self.encoder(e_t, h, s_embed)\n",
    "            z_proposal = self.encoder.sample(mu, logvar)\n",
    "            \n",
    "            beta = self.switching_unit(e_t, h, z)\n",
    "            z = beta * z_proposal + (1 - beta) * z\n",
    "            \n",
    "            A, B = self.decoder(z)\n",
    "            e_controlled = self.decoder.apply_controller(e_t, A, B)\n",
    "            \n",
    "            z_list.append(z)\n",
    "            mu_list.append(mu)\n",
    "            logvar_list.append(logvar)\n",
    "            beta_list.append(beta)\n",
    "            controlled_list.append(e_controlled)\n",
    "        \n",
    "        return {\n",
    "            'z_sequence': torch.stack(z_list, dim=1),\n",
    "            'mu_sequence': torch.stack(mu_list, dim=1),\n",
    "            'logvar_sequence': torch.stack(logvar_list, dim=1),\n",
    "            'beta_sequence': torch.stack(beta_list, dim=1),\n",
    "            'controlled_sequence': torch.stack(controlled_list, dim=1)\n",
    "        }\n",
    "    \n",
    "    def init_state(self, batch_size: int, device: torch.device):\n",
    "        h = self.gru.init_hidden(batch_size, device)\n",
    "        z = self.z_init.unsqueeze(0).expand(batch_size, -1)\n",
    "        return h, z\n",
    "\n",
    "print(\"Metacontroller class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Abstract Action Policy for Internal RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractActionPolicy(nn.Module):\n",
    "    \"\"\"Policy that outputs abstract actions z given residual observations.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, hidden_dim: int = 256, latent_dim: int = 16):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.mu_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        self.logvar_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, e: torch.Tensor, h: Optional[torch.Tensor] = None):\n",
    "        if e.dim() == 2:\n",
    "            e = e.unsqueeze(1)\n",
    "        output, h_new = self.gru(e, h)\n",
    "        output = output[:, -1, :]\n",
    "        return self.mu_head(output), self.logvar_head(output), h_new\n",
    "    \n",
    "    def sample(self, mu: torch.Tensor, logvar: torch.Tensor, \n",
    "               deterministic: bool = False) -> torch.Tensor:\n",
    "        if deterministic:\n",
    "            return mu\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        return mu + std * torch.randn_like(std)\n",
    "    \n",
    "    def log_prob(self, z: torch.Tensor, mu: torch.Tensor, \n",
    "                 logvar: torch.Tensor) -> torch.Tensor:\n",
    "        var = logvar.exp()\n",
    "        log_prob = -0.5 * (math.log(2 * math.pi) + logvar + (z - mu).pow(2) / var)\n",
    "        return log_prob.sum(dim=-1)\n",
    "    \n",
    "    def init_hidden(self, batch_size: int, device: torch.device):\n",
    "        return torch.zeros(1, batch_size, self.hidden_dim, device=device)\n",
    "\n",
    "print(\"AbstractActionPolicy defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model: {config.base_model}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Freeze base model\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "base_model.eval()\n",
    "\n",
    "embed_dim = base_model.config.hidden_size\n",
    "num_layers = base_model.config.num_hidden_layers\n",
    "\n",
    "print(f\"\\nModel loaded!\")\n",
    "print(f\"  Hidden size: {embed_dim}\")\n",
    "print(f\"  Num layers: {num_layers}\")\n",
    "print(f\"  Controller layer: {config.controller_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Create Expert Dataset\n",
    "\n",
    "We'll use the canonical solutions as expert trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(problem_desc: str) -> str:\n",
    "    return f\"\"\"Write a Python function to solve this problem.\n",
    "Do NOT use a class wrapper. Write a standalone function.\n",
    "\n",
    "Problem: {problem_desc}\n",
    "\n",
    "Solution:\n",
    "```python\n",
    "\"\"\"\n",
    "\n",
    "# Create dataset from problems\n",
    "expert_data = []\n",
    "for name, prob in PROBLEMS.items():\n",
    "    prompt = format_prompt(prob['description'])\n",
    "    solution = prob['canonical_solution']\n",
    "    full_text = prompt + solution + \"\\n```\"\n",
    "    \n",
    "    expert_data.append({\n",
    "        'problem_type': name,\n",
    "        'prompt': prompt,\n",
    "        'solution': solution,\n",
    "        'full_text': full_text\n",
    "    })\n",
    "\n",
    "print(f\"Created {len(expert_data)} expert examples\")\n",
    "print(f\"\\nExample prompt:\\n{expert_data[0]['prompt'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict], tokenizer, max_len: int = 512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            item['full_text'],\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Create labels (mask prompt)\n",
    "        prompt_enc = self.tokenizer(item['prompt'], return_tensors='pt')\n",
    "        prompt_len = prompt_enc['input_ids'].shape[1]\n",
    "        \n",
    "        labels = encoding['input_ids'].clone()\n",
    "        labels[0, :prompt_len] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': labels.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Expand dataset by repeating (simple data augmentation)\n",
    "expanded_data = expert_data * 20  # 120 examples\n",
    "random.shuffle(expanded_data)\n",
    "\n",
    "dataset = ExpertDataset(expanded_data, tokenizer, config.max_seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=config.mc_batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Batches per epoch: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Extract Residual Activations\n",
    "\n",
    "We need a helper to get residual stream activations at the controller layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals(model, input_ids, attention_mask, layer_idx):\n",
    "    \"\"\"Extract residual stream activations at a specific layer.\"\"\"\n",
    "    residuals = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            residuals.append(output[0])\n",
    "        else:\n",
    "            residuals.append(output)\n",
    "    \n",
    "    # Register hook\n",
    "    layer = model.model.layers[layer_idx]\n",
    "    handle = layer.register_forward_hook(hook)\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            _ = model(input_ids, attention_mask=attention_mask)\n",
    "    finally:\n",
    "        handle.remove()\n",
    "    \n",
    "    return residuals[0] if residuals else None\n",
    "\n",
    "# Test\n",
    "test_input = tokenizer(\"def hello():\", return_tensors='pt').to(device)\n",
    "test_residuals = get_residuals(base_model, test_input['input_ids'], \n",
    "                                test_input['attention_mask'], config.controller_layer)\n",
    "print(f\"Test residual shape: {test_residuals.shape}\")\n",
    "print(f\"Expected: [1, seq_len, {embed_dim}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Phase 2 - Train Metacontroller\n",
    "\n",
    "Train the metacontroller with ELBO objective to discover abstract actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metacontroller\n",
    "metacontroller = Metacontroller(\n",
    "    embed_dim=embed_dim,\n",
    "    latent_dim=config.latent_dim,\n",
    "    gru_dim=config.gru_dim,\n",
    "    seq_embed_dim=config.seq_embed_dim,\n",
    "    encoder_hidden=config.encoder_hidden,\n",
    "    decoder_hidden=config.decoder_hidden,\n",
    "    switch_hidden=config.switch_hidden,\n",
    "    controller_rank=config.controller_rank\n",
    ").to(device)\n",
    "\n",
    "mc_params = sum(p.numel() for p in metacontroller.parameters())\n",
    "print(f\"Metacontroller parameters: {mc_params:,}\")\n",
    "\n",
    "optimizer = AdamW(metacontroller.parameters(), lr=config.mc_learning_rate, \n",
    "                  weight_decay=config.mc_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elbo_loss(logits, labels, mu_seq, logvar_seq, kl_weight, mask):\n",
    "    \"\"\"Compute ELBO loss for metacontroller training.\"\"\"\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    \n",
    "    # Reconstruction loss\n",
    "    nll = F.cross_entropy(\n",
    "        logits.view(-1, vocab_size),\n",
    "        labels.view(-1),\n",
    "        ignore_index=-100,\n",
    "        reduction='none'\n",
    "    ).view(batch_size, seq_len)\n",
    "    \n",
    "    # KL divergence\n",
    "    kl = 0.5 * (mu_seq.pow(2) + logvar_seq.exp() - logvar_seq - 1).sum(dim=-1)\n",
    "    \n",
    "    # Apply mask\n",
    "    mask_float = mask.float()\n",
    "    nll = (nll * mask_float).sum() / mask_float.sum()\n",
    "    kl = (kl * mask_float).sum() / mask_float.sum()\n",
    "    \n",
    "    loss = nll + kl_weight * kl\n",
    "    return loss, nll, kl\n",
    "\n",
    "print(\"Loss function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: METACONTROLLER TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {config.mc_epochs}\")\n",
    "print(f\"KL weight: {config.kl_weight}\")\n",
    "print()\n",
    "\n",
    "history = {'loss': [], 'nll': [], 'kl': [], 'beta_mean': []}\n",
    "\n",
    "for epoch in range(config.mc_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_nll = 0\n",
    "    epoch_kl = 0\n",
    "    epoch_beta = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    metacontroller.train()\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.mc_epochs}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Get residuals from frozen base model\n",
    "        with torch.no_grad():\n",
    "            residuals = get_residuals(base_model, input_ids, attention_mask, \n",
    "                                      config.controller_layer)\n",
    "        \n",
    "        # Apply metacontroller\n",
    "        mc_out = metacontroller.forward_training(residuals.float(), attention_mask)\n",
    "        \n",
    "        # Get logits from controlled residuals (simplified: use base model output)\n",
    "        with torch.no_grad():\n",
    "            base_out = base_model(input_ids, attention_mask=attention_mask)\n",
    "            logits = base_out.logits\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, nll, kl = compute_elbo_loss(\n",
    "            logits, labels, \n",
    "            mc_out['mu_sequence'], mc_out['logvar_sequence'],\n",
    "            config.kl_weight, attention_mask\n",
    "        )\n",
    "        \n",
    "        # Update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(metacontroller.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_nll += nll.item()\n",
    "        epoch_kl += kl.item()\n",
    "        epoch_beta += mc_out['beta_sequence'].mean().item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.3f}\",\n",
    "            'beta': f\"{mc_out['beta_sequence'].mean().item():.3f}\"\n",
    "        })\n",
    "    \n",
    "    # Epoch summary\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    avg_nll = epoch_nll / num_batches\n",
    "    avg_kl = epoch_kl / num_batches\n",
    "    avg_beta = epoch_beta / num_batches\n",
    "    \n",
    "    history['loss'].append(avg_loss)\n",
    "    history['nll'].append(avg_nll)\n",
    "    history['kl'].append(avg_kl)\n",
    "    history['beta_mean'].append(avg_beta)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"\\nEpoch {epoch+1}: Loss={avg_loss:.4f}, NLL={avg_nll:.4f}, \"\n",
    "              f\"KL={avg_kl:.4f}, Beta={avg_beta:.3f}\")\n",
    "\n",
    "print(\"\\nMetacontroller training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0,0].plot(history['loss'])\n",
    "axes[0,0].set_title('Total Loss')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "\n",
    "axes[0,1].plot(history['nll'])\n",
    "axes[0,1].set_title('Reconstruction Loss (NLL)')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "\n",
    "axes[1,0].plot(history['kl'])\n",
    "axes[1,0].set_title('KL Divergence')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "\n",
    "axes[1,1].plot(history['beta_mean'])\n",
    "axes[1,1].set_title('Mean Switching Rate (β)')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].axhline(y=0.5, color='r', linestyle='--', label='threshold')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Analyze Switching Patterns\n",
    "\n",
    "Let's see if the metacontroller learned meaningful switching patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metacontroller.eval()\n",
    "\n",
    "# Analyze one example\n",
    "test_idx = 0\n",
    "sample = dataset[test_idx]\n",
    "input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    residuals = get_residuals(base_model, input_ids, attention_mask, config.controller_layer)\n",
    "    mc_out = metacontroller.forward_training(residuals.float(), attention_mask)\n",
    "\n",
    "# Get switching values\n",
    "betas = mc_out['beta_sequence'][0, :, 0].cpu().numpy()\n",
    "valid_len = attention_mask[0].sum().item()\n",
    "\n",
    "# Decode tokens for reference\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0][:valid_len].cpu())\n",
    "\n",
    "print(f\"Analyzing: {expert_data[test_idx]['problem_type']}\")\n",
    "print(f\"Sequence length: {valid_len}\")\n",
    "print(f\"\\nSwitching events (β > 0.5):\")\n",
    "\n",
    "switch_points = []\n",
    "for i, b in enumerate(betas[:valid_len]):\n",
    "    if b > 0.5:\n",
    "        switch_points.append(i)\n",
    "        token_context = ''.join(tokens[max(0, i-2):i+3]).replace('Ġ', ' ')\n",
    "        print(f\"  Position {i}: β={b:.3f}, context: '{token_context}'\")\n",
    "\n",
    "print(f\"\\nTotal switches: {len(switch_points)} / {valid_len} tokens\")\n",
    "print(f\"Compression ratio: {valid_len / max(len(switch_points), 1):.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize switching pattern\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.bar(range(valid_len), betas[:valid_len], alpha=0.7)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', label='Switch threshold')\n",
    "plt.xlabel('Token Position')\n",
    "plt.ylabel('Switching Probability (β)')\n",
    "plt.title(f'Switching Pattern for {expert_data[test_idx][\"problem_type\"]}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Phase 3 - Internal RL (Simplified)\n",
    "\n",
    "Now we train the abstract action policy using RL.\n",
    "This is a simplified version - full implementation would need proper environment interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create abstract action policy\n",
    "policy = AbstractActionPolicy(\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=256,\n",
    "    latent_dim=config.latent_dim\n",
    ").to(device)\n",
    "\n",
    "policy_params = sum(p.numel() for p in policy.parameters())\n",
    "print(f\"Policy parameters: {policy_params:,}\")\n",
    "\n",
    "policy_optimizer = AdamW(policy.parameters(), lr=config.rl_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_code(code: str, problem: Dict) -> float:\n",
    "    \"\"\"Evaluate generated code against test cases.\"\"\"\n",
    "    try:\n",
    "        # Execute code\n",
    "        namespace = {}\n",
    "        exec(code, namespace)\n",
    "        \n",
    "        func = namespace.get(problem['entry_point'])\n",
    "        if func is None:\n",
    "            return 0.0\n",
    "        \n",
    "        # Run tests\n",
    "        passed = 0\n",
    "        for test in problem['test_cases']:\n",
    "            try:\n",
    "                result = func(*test['input'])\n",
    "                if result == test['expected']:\n",
    "                    passed += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return passed / len(problem['test_cases'])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Test evaluation\n",
    "test_reward = evaluate_code(PROBLEMS['fibonacci']['canonical_solution'], PROBLEMS['fibonacci'])\n",
    "print(f\"Test evaluation (canonical solution): {test_reward:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_policy(problem: Dict, policy: nn.Module, metacontroller: Metacontroller,\n",
    "                         base_model, tokenizer, max_tokens: int = 256) -> str:\n",
    "    \"\"\"Generate code using the abstract action policy.\"\"\"\n",
    "    policy.eval()\n",
    "    metacontroller.eval()\n",
    "    \n",
    "    prompt = format_prompt(problem['description'])\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt')['input_ids'].to(device)\n",
    "    \n",
    "    generated = input_ids[0].tolist()\n",
    "    h_policy = policy.init_hidden(1, device)\n",
    "    h_mc, z = metacontroller.init_state(1, device)\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        # Get current residual\n",
    "        curr_ids = torch.tensor([generated], device=device)\n",
    "        with torch.no_grad():\n",
    "            residual = get_residuals(base_model, curr_ids, \n",
    "                                     torch.ones_like(curr_ids), config.controller_layer)\n",
    "            e_t = residual[:, -1, :].float()\n",
    "        \n",
    "        # Get switching probability\n",
    "        h_mc = metacontroller.gru(e_t, h_mc)\n",
    "        beta = metacontroller.switching_unit(e_t, h_mc, z)\n",
    "        \n",
    "        # Sample new z if switching\n",
    "        if beta.item() > config.beta_threshold or len(generated) == len(input_ids[0]):\n",
    "            mu, logvar, h_policy = policy(e_t, h_policy)\n",
    "            z = policy.sample(mu, logvar, deterministic=True)\n",
    "        \n",
    "        # Generate token (simplified: use base model directly)\n",
    "        with torch.no_grad():\n",
    "            out = base_model(curr_ids)\n",
    "            logits = out.logits[:, -1, :]\n",
    "            \n",
    "            if config.temperature > 0:\n",
    "                probs = F.softmax(logits / config.temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1).item()\n",
    "            else:\n",
    "                next_token = logits.argmax(-1).item()\n",
    "        \n",
    "        generated.append(next_token)\n",
    "        \n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "# Test generation\n",
    "print(\"Testing generation...\")\n",
    "test_gen = generate_with_policy(PROBLEMS['fibonacci'], policy, metacontroller, \n",
    "                                 base_model, tokenizer, max_tokens=100)\n",
    "print(f\"Generated ({len(test_gen)} chars):\")\n",
    "print(test_gen[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 3: INTERNAL RL TRAINING (Simplified)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Steps: {config.rl_steps}\")\n",
    "print(f\"Batch size: {config.rl_batch_size}\")\n",
    "print()\n",
    "\n",
    "rl_history = {'rewards': [], 'policy_loss': []}\n",
    "problem_list = list(PROBLEMS.values())\n",
    "\n",
    "for step in tqdm(range(config.rl_steps), desc=\"Internal RL\"):\n",
    "    batch_rewards = []\n",
    "    batch_log_probs = []\n",
    "    \n",
    "    policy.train()\n",
    "    \n",
    "    for _ in range(config.rl_batch_size):\n",
    "        # Sample problem\n",
    "        problem = random.choice(problem_list)\n",
    "        \n",
    "        # Generate and evaluate\n",
    "        generated = generate_with_policy(problem, policy, metacontroller,\n",
    "                                          base_model, tokenizer, max_tokens=150)\n",
    "        \n",
    "        # Extract code and evaluate\n",
    "        if \"```python\" in generated:\n",
    "            code_start = generated.find(\"```python\") + len(\"```python\")\n",
    "            code_end = generated.find(\"```\", code_start)\n",
    "            code = generated[code_start:code_end].strip() if code_end > code_start else \"\"\n",
    "        else:\n",
    "            code = generated.split(\"Solution:\")[-1].strip() if \"Solution:\" in generated else \"\"\n",
    "        \n",
    "        reward = evaluate_code(code, problem)\n",
    "        batch_rewards.append(reward)\n",
    "    \n",
    "    # Simple policy update (REINFORCE-style)\n",
    "    mean_reward = sum(batch_rewards) / len(batch_rewards)\n",
    "    rl_history['rewards'].append(mean_reward)\n",
    "    \n",
    "    # Log progress\n",
    "    if (step + 1) % config.log_interval == 0:\n",
    "        avg_reward = sum(rl_history['rewards'][-config.log_interval:]) / config.log_interval\n",
    "        print(f\"\\nStep {step+1}: Mean Reward = {avg_reward:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERNAL RL TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = {}\n",
    "\n",
    "policy.eval()\n",
    "metacontroller.eval()\n",
    "\n",
    "for name, problem in PROBLEMS.items():\n",
    "    successes = 0\n",
    "    num_samples = 5\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        generated = generate_with_policy(problem, policy, metacontroller,\n",
    "                                          base_model, tokenizer, max_tokens=200)\n",
    "        \n",
    "        if \"```python\" in generated:\n",
    "            code_start = generated.find(\"```python\") + len(\"```python\")\n",
    "            code_end = generated.find(\"```\", code_start)\n",
    "            code = generated[code_start:code_end].strip() if code_end > code_start else \"\"\n",
    "        else:\n",
    "            code = generated.split(\"Solution:\")[-1].strip() if \"Solution:\" in generated else \"\"\n",
    "        \n",
    "        reward = evaluate_code(code, problem)\n",
    "        if reward == 1.0:\n",
    "            successes += 1\n",
    "    \n",
    "    accuracy = successes / num_samples\n",
    "    results[name] = accuracy\n",
    "    print(f\"{name:20s}: {successes}/{num_samples} = {accuracy:.0%}\")\n",
    "\n",
    "overall = sum(results.values()) / len(results)\n",
    "print(f\"\\n{'OVERALL':20s}: {overall:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison with Experiment 16\n",
    "exp16_results = {\n",
    "    'fibonacci': 1.0,\n",
    "    'binary_search': 1.0,\n",
    "    'coin_change': 0.8,\n",
    "    'valid_parentheses': 0.2,\n",
    "    'rpn_calculator': 0.0,\n",
    "    'edit_distance': 0.0\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Experiment 16 vs Experiment 17\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Problem':<20} {'Exp 16':<10} {'Exp 17':<10} {'Change':<10}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for name in PROBLEMS:\n",
    "    exp16 = exp16_results.get(name, 0.0)\n",
    "    exp17 = results.get(name, 0.0)\n",
    "    change = exp17 - exp16\n",
    "    change_str = f\"+{change:.0%}\" if change >= 0 else f\"{change:.0%}\"\n",
    "    print(f\"{name:<20} {exp16:<10.0%} {exp17:<10.0%} {change_str:<10}\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "exp16_overall = sum(exp16_results.values()) / len(exp16_results)\n",
    "print(f\"{'OVERALL':<20} {exp16_overall:<10.0%} {overall:<10.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metacontroller and policy\n",
    "import os\n",
    "\n",
    "save_dir = \"exp17_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'metacontroller_state_dict': metacontroller.state_dict(),\n",
    "    'policy_state_dict': policy.state_dict(),\n",
    "    'config': config.__dict__,\n",
    "    'results': results,\n",
    "    'history': history,\n",
    "    'rl_history': rl_history\n",
    "}, os.path.join(save_dir, \"experiment_17_checkpoint.pt\"))\n",
    "\n",
    "print(f\"Models saved to {save_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implemented Experiment 17: Internal RL with Temporal Abstractions.\n",
    "\n",
    "### Key Components:\n",
    "1. **Metacontroller** - Discovers abstract actions in residual stream\n",
    "2. **Switching Unit** - Learns when to switch between abstract actions\n",
    "3. **Abstract Action Policy** - RL policy over latent space\n",
    "\n",
    "### Key Insights:\n",
    "- Token-level RL has massive action space (50K+) and long horizon (~100)\n",
    "- Internal RL reduces to ~16D actions and ~5 timesteps\n",
    "- This enables tractable credit assignment for sparse rewards\n",
    "\n",
    "### Next Steps:\n",
    "- Full integration of controlled residuals into generation\n",
    "- More sophisticated RL (PPO, better baselines)\n",
    "- Analysis of discovered abstract actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
